


More on Dota 2












CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexProductOverviewChatGPTGPT-4DALLÂ·E 2Customer storiesSafety standardsAPI data privacyPricingDevelopersOverviewDocumentationAPI referenceExamplesSafetyCompanyAboutBlogCareersCharterSecuritySearch Navigation quick links Log inSign upMenu Mobile Navigation CloseSite NavigationResearchProductDevelopersSafetyCompany Quick Links Log inSign upSearch Submit More on Dota 2Our Dota 2 result shows that self-play can catapult the performance of machine learning systems from far below human level to superhuman, given sufficient compute. In the span of a month, our system went from barely matching a high-ranked player to beating the top pros and has continued to improve since then. Supervised deep learning systems can only be as good as their training datasets, but in self-play systems, the available data improves automatically as the agent gets better.August 16, 2017Self-play,Â Reinforcement learning,Â Games,Â Dota 2,Â Software engineering,Â OpenAI FiveTrueSkill rating (similar to the ELO rating in chess) of our best bot over time, computed by simulating games between the bots and observing the win ratios. Improvements came from every part of the system, from adding new features to algorithmic improvements to scaling things up. The graph is surprisingly linear, meaning the team improved the bot exponentially over time.The projectâs timeline is the following. For some perspective, 15% of players are below 1.5kÂ MMR; 58% of players are below 3k; 99.99% are belowÂ 7.5k.March 1st:Â had our first classical reinforcement learningÂ resultsÂ in a simple Dota environment, where a Drow Ranger learns to kite a hardcodedÂ Earthshaker.May 8th:Â 1.5k MMR tester says heâs been getting better faster than theÂ bot.Early June:Â beat 1.5k MMRÂ testerJune 30th:Â winning most games against 3k MMRÂ testerJuly 8th:Â barely get firstÂ winÂ against 7.5k MMR semi-proÂ tester.August 7th:Â beatÂ BlitzÂ (6.2k former pro) 3â0,Â PajkattÂ (8.5k pro) 2â1, andÂ CC&CÂ (8.9k pro) 3â0. All agreed that Sumail would figure out how to beatÂ it.August 9th:Â beat Arteezy (10k pro, top player) 10â0. He says Sumail could figure out thisÂ bot.August 10th:Â beat Sumail (8.3k pro, top 1v1 player) 6â0, who says itâs unbeatable. Plays the Aug 9th bot, where he goesÂ 2â1.August 11th:Â beat Dendi (7.3k pro, former world champion, old-school crowd favorite) 2â0. Bot has 60% win rate versus August 10thÂ bot.SumaiL 1v102:40The taskThe full game is 5v5, but 1v1 alsoÂ appearsÂ in someÂ tournaments. Our bot played under standard tournament rulesâwe did not add AI-specific simplifications toÂ 1v1.The bot operated off the followingÂ interfaces:Observations:Â Bot API features, which are designed to be the same set of features that humans can see, related to heroes, creeps, courier, and the terrain near the hero. The game is partiallyÂ observable.Actions:Â Actions accessible by the bot API, chosen at a frequency comparable to humans, including moving to a location, attacking a unit, or using anÂ item.Feedback:Â The bot received incentives for winning and basic metrics like health andÂ lastÂ hits.We whitelisted a few dozen item builds that bots could use, and picked one for evaluation. We also separately trained the initial creep block using traditional RL techniques, as it happens before the opponentÂ appears.Arteezy 1v105:44The InternationalOur approach, combining small amounts of âcoachingâ with self-play, allowed us to massively improve our agent between the Monday and Thursday of The International. On Monday evening, Pajkatt won using an unusual item build (buying an early magic wand). We added this item build to the trainingÂ whitelist.Around 1pm on Wednesday, we tested the latest bot. The bot would lose a bunch of health in the first wave. We thought perhaps we needed to roll back, but noticed further gameplay was amazing, and the first wave behavior was baiting the other bots to be aggressive towards it. Further self-play fixed the issue, as the bot learned to counter the baiting strategy. In the meanwhile, we stitched it together with Mondayâs bot for the first wave only, and completed the process twenty minutes before Arteezy showed up atÂ 4pm.After the Arteezy matches, we updated the creep block model, which increased TrueSkill by one point. Further training before Sumailâs match on Thursday increased TrueSkill by two points. Sumail pointed out that the bot had learned to cast razes out of the enemyâs vision. This was due to a mechanic we hadnât known about:Â abilities cast outside of the enemyâs vision prevent the enemy from gaining a wandÂ charge.Arteezy also played a match against our 7.5k semi-pro tester. Arteezy was winning the whole game, but our tester still managed to surprise him with a strategy heâd learned from the bot. Arteezy remarked afterwards that this was a strategy that Paparazi had used against him once and was not commonlyÂ practiced.Pajkatt 1v107:56Pajkatt beating Mondayâs bot. Note he baits the bot into engaging, and uses regeneration (faerie fires and a magic wand) to heal up. The bot is generally very good at deciding who will win a fight, but itâs never played against someone with early wand before.Bot exploitsThough Sumail called the bot âunbeatableâ, it can still be confused in situations very different from what itâs seen. We set up the bot at a LAN event at The International, where players played over 1,000 games to beat the bot by any meansÂ possible.The successful exploits fell into threeÂ archetypes:Creep pulling:Â itâs possible to repeatedly attract the lane creeps into chasing you right when they spawn (between the botâs tier 2 and tier 3 towers). You end up with dozens of creeps chasing you around the map, and eventually the botâs tower dies viaÂ attrition.Orb of venom + wind lace:Â this gives you a big movement speed advantage over the bot at level 1 and allows for a quick first blood. You need to exploit this head start to kill the bot one moreÂ time.Level 1 raze:Â this requires a lot of skill, but several 6â7k MMR players were able to kill the bot at level 1 by successfully hitting 3â5 razes in a short span ofÂ time.Fixing these issues for 1v1 would be similar to fixing the Pajkatt bug. But for 5v5, such issues arenât exploits at all, and weâll need a system which can handle totally weird and wacky situations itâs neverÂ seen.InfrastructureWeâre not ready to talk about agent internalsâthe team is focused on solving 5v5Â first.The first step in the project was figuring out how to run Dota 2 in the cloud on a physical GPU. The game gave an obscure error message on GPU cloud instances. But when starting it on Gregâs personal GPU desktop (which is the desktop brought onstage during the show), we noticed that Dota booted when the monitor was plugged in, but gave the same error message when unplugged. So we configured our cloud GPU instances to pretend there was a physical monitorÂ attached.Dota didnât support custom dedicated servers at the time, meaning that running scalably and without a GPU was possible only with very slow software rendering. We then created a shim to stub out most OpenGL calls, except the ones needed toÂ boot.At the same time, we wrote a scripted botâwe needed a baseline for comparison (especially because the builtin bots donât work well on 1v1) and to understand all the semantics of theÂ bot API. The scripted bot reaches 70 last hits in ten minutes on an empty lane, but still loses to reasonable humans. Our current best 1v1 bot reaches more like 97 (it destroys the tower before then, so we can only extrapolate), and the theoretical maximum isÂ 101.SirActionSlacks 1v105:51Bot playing versus SirActionSlacks. The strategy of distracting the bot with a courier rush did not work.5v51v1 is complicated, but 5v5 is an ocean of complexity. We know weâll need to further push the limits of AI in order to solveÂ it.One well-established place to start is with behavioral cloning. Dota has about a million public matches a day. The replays for these matches are stored on Valveâs servers for two weeks. Weâve been downloading every expert-level replay since last November, and have amassed a dataset of 5.8M games (each game is about 45 minutes with 10 humans). We useÂ OpenDotaÂ to discover these replays, and are donatingÂ $12k (10 years of their fundraising goal) to support theÂ project.We have many more ideas, and areÂ hiringÂ engineers (must be intrigued by machine learning, but need not be an expert) and researchers to help us make this happen. We thank Microsoft Azure and Valve for their support in thisÂ endeavor.AuthorsOpenAI ResearchOverviewIndexProductOverviewChatGPTGPT-4DALLÂ·E 2Customer storiesSafety standardsAPI data privacyPricingSafetyOverviewCompanyAboutBlogCareersCharterSecurityOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top
