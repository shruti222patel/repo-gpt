


Concrete AI safety problems












CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexProductOverviewChatGPTGPT-4DALLÂ·E 2Customer storiesSafety standardsAPI data privacyPricingDevelopersOverviewDocumentationAPI referenceExamplesSafetyCompanyAboutBlogCareersCharterSecuritySearch Navigation quick links Log inSign upMenu Mobile Navigation CloseSite NavigationResearchProductDevelopersSafetyCompany Quick Links Log inSign upSearch Submit Concrete AI safety problemsWe (along with researchers from Berkeley and Stanford) are co-authors on todayâs paper led by Google Brain researchers,Â Concrete Problems in AI Safety. The paper explores many research problems around ensuring that modern machine learning systems operate as intended.June 21, 2016More resourcesRead paperSafety & Alignment,Â Robustness,Â PublicationWe (along with researchers from Berkeley and Stanford) are co-authors on todayâs paper led by Google Brain researchers,Â Concrete Problems in AI Safety. The paper explores many research problems around ensuring that modern machine learning systems operate as intended. (The problems are very practical, and weâve already seen some being integrated intoÂ OpenAIÂ Gym.)Advancing AI requires making AI systems smarter, but it also requires preventing accidentsâthat is, ensuring that AI systems do what people actually want them to do. Thereâs been an increasing focus onÂ safety researchÂ from the machine learning community, such as a recentÂ paperÂ fromÂ DeepMindÂ andÂ FHI. Still, many machine learning researchers have wondered just how much safety research can be doneÂ today.The authors discuss fiveÂ areas:Safe exploration.Â CanÂ reinforcement learningÂ (RL) agents learn about their environment without executing catastrophic actions?Â For example, can an RL agent learn to navigate an environment without ever falling off aÂ ledge?Robustness to distributional shift.Â Can machine learning systems be robust to changes in the data distribution, or at least fail gracefully?Â For example, can we buildÂ image classifiersÂ that indicate appropriate uncertainty when shown new kinds of images, instead of confidently trying to use itsÂ potentially inapplicableÂ learnedÂ model?Avoiding negative side effects.Â Can we transform an RL agentâsÂ reward functionÂ to avoid undesired effects on the environment?Â For example, can we build a robot that will move an object while avoiding knocking anything over or breaking anything, without manually programming a separate penalty for each possible badÂ behavior?Avoiding âreward hackingâ and âwireheadingâ.Â Can we prevent agents from âgamingâ their reward functions, such as by distorting their observations?Â For example, can we train an RL agent to minimize the number of dirty surfaces in a building, without causing it to avoid looking for dirty surfaces or to create new dirty surfaces to cleanÂ up?Scalable oversight.Â Can RL agents efficiently achieve goals for which feedback is very expensive?Â For example, can we build an agent that tries to clean a room in the way the user would be happiest with, even though feedback from the user is very rare and we have to use cheap approximations (like the presence of visible dirt) during training? The divergence between cheap approximations and what we actually care about is an important source of accidentÂ risk.Many of the problems are not new, but the paper explores them in the context of cutting-edge systems. We hope theyâll inspire more people to work on AI safety research, whetherÂ at OpenAIÂ orÂ elsewhere.Weâre particularly excited to have participated in this paper as a cross-institutional collaboration. We think that broad AI safety collaborations will enable everyone to build better machine learning systems.Â Let us knowÂ if you have a future paper youâd like to collaborateÂ on!AuthorsPaul ChristianoGreg BrockmanResearchOverviewIndexProductOverviewChatGPTGPT-4DALLÂ·E 2Customer storiesSafety standardsAPI data privacyPricingSafetyOverviewCompanyAboutBlogCareersCharterSecurityOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top
