


Generative modeling with sparse transformers












CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexProductOverviewChatGPTGPT-4DALLÂ·E 2Customer storiesSafety standardsAPI data privacyPricingDevelopersOverviewDocumentationAPI referenceExamplesSafetyCompanyAboutBlogCareersCharterSecuritySearch Navigation quick links Log inSign upMenu Mobile Navigation CloseSite NavigationResearchProductDevelopersSafetyCompany Quick Links Log inSign upSearch Submit Generative modeling with sparse transformersWeâve developed the Sparse Transformer, a deep neural network which sets new records at predicting what comes next in a sequenceâwhether text, images, or sound. It uses an algorithmic improvement of theÂ attentionÂ mechanism to extract patterns from sequences 30x longer than possibleÂ previously.April 23, 2019More resourcesRead paperView codeTransformers,Â Generative models,Â Sparsity,Â Open source,Â Publication,Â ReleaseOne existing challenge in AI research is modeling long-range, subtle interdependencies in complex data like images, videos, or sounds. The Sparse Transformer incorporates anÂ O(NN) O(N \sqrt{N}) O(NNâ) reformulation of theÂ O(N2) O(N^2) O(N2) TransformerÂ self-attention mechanism, along with several other improvements, to apply it directly to these rich data types. Previously, models used on these data were specifically crafted for one domain or difficult to scale to sequences more than a few thousand elements long. In contrast, our model can model sequences with tens of thousands of elements using hundreds of layers, achieving state-of-the-art performance across multiple domains. At OpenAI, weâre using it to help us build AI systems that possess a greater ability to understand theÂ world.Deep attentionIn Transformers, every output element is connected to every input element, and the weightings between them are dynamically calculated based upon the circumstances, a process calledÂ attention. While it is believed that this allows Transformers to be more flexible than models with fixed connectivity patterns, in practice it requires the creation of anÂ NÃN N\times N NÃNÂ attention matrixÂ for every layer and attention head, which can consume large amounts of memory when applied to data types with many elements, like images or rawÂ audio.Data typeStoredRecomputed1024 text tokens (several paragraphs)1.0 GB16 MB32x32x3 pixels (CIFAR-10 image)9.6 GB151 MB64x64x3 pixels (Imagenet 64 image)154 GB2.4 GB24,000 samples (~2 seconds of 12 kHz audio)590 GB9.2GBAttention memory usage for a deep Transformer (64 layers and 4 heads) when matrices are stored in memory or recomputed during the backward pass. For reference, standard GPUs used for deep learning typically have memory of 12-32 GB.One way to reduce this is by recomputing the attention matrix fromÂ checkpointsÂ during backpropagation, a well-established technique in deep learning for reducing memory usage at the cost of more computation. When done for the attention matrix in Transformers, it means the largest memory cost becomes independent of the number of layers, letting us train networks with substantially greater depth than possible previously. In practice, we found that Transformers with depth up to 128 layers outperformed shallower networks on benchmark tasks likeÂ CIFAR-10.To train these models with increased depth, we made several adjustments to the ordering of operations in the transformer and modified the initialization scheme. Full details can be seen in ourÂ paper.Sparse attentionEven computing a single attention matrix, however, can become impractical for very large inputs. We instead use sparse attention patterns, where each output position only computes weightings from a subset of input positions. When the subset is small relative to the full set of inputs (say,Â N \sqrt{N} NââÂ elements instead of Â NÂ N Â N elements), the resulting attention computation becomes tractable even for very long sequences, with an algorithmic complexity ofÂ O(NN) O(N \sqrt{N}) O(NNâ)Â instead ofÂ O(N2) O(N^2) O(N2).To assess the feasibility of the approach, we first visualized the learned attention patterns for deep Transformers on images, finding that many showed interpretable and structured sparsity patterns. Each of the below images shows which input pixels (highlighted in white) are attended to by a given attention head in order to predict the next value in the image. When the input portions are focused on small subsets and show a high degree of regularity, the layer is amenable to sparsification. A sampling of them are displayed here for a 128-layer model on CIFAR-10Â images:Layer 19Layer 20Learned attention patterns (white highlight) for several layers of a 128-layer CIFAR-10 network. These layers learned to separate attention across two dimensions. Layer 19 summarizes information for each row, and layer 20 aggregates those summaries by column, leading to an efficient factorization of the full attention operation.Layer 6Layer 36Some layers learned to access a positional memory, often attending to similar locations regardless of the input data or timestep (layer 6). Other layers learned highly data-dependent access patterns (layer 36).While many layers displayed sparse structure, some layers clearly display dynamic attention that stretch over the entirety of the image. In order to preserve the ability of our network to learn such patterns, we implemented a two-dimensional factorization of the attention matrix, where the network can attend to all positions through two steps of sparseÂ attention.Normal transformerStrided attentionFixed attentionThe first version,Â stridedÂ attention, is roughly equivalent to each position attending to its row and its column, and is similar to the attention pattern learned by the network above. (Note that the column attention can be equivalently formulated as attending to the row of the transposed matrix). The second version,Â fixedÂ attention, attends to a fixed column and the elements after the latest column element, a pattern we found useful for when the data didnât fit into a two-dimensional structure (like text). For more details, we refer readers to ourÂ paper.Experimental resultsSparse Transformers set new state-of-the-art scores for density estimation of CIFAR-10, Enwik8, and ImagenetÂ 64.CIFAR10Bits per dimPixelCNN++ (Salimans et al, 2017)2.92Image Transformer (Parmar et. al, 2018)2.90PixelSNAIL (Chen et al., 2017)2.85Sparse Transformer 59M (256W, 128L, 2H)2.80Enwik8Bits per byteDeeper Self-Attention (Al-Rfou et al, 2018)1.06Transformer-XL 88M (Dai et al., 2018)1.03Transformer-XL 277M (Dai et al., 2018)0.99Sparse Transformer 95M (512W, 30L, 8H)0.99ImageNet 64x64Bits per dimGated PixelCNN (van den Oord et al, 2016)3.57Parallel Multiscale (Reed et al, 2017)3.7SPN 150M (Menick & Kalchbrenner, 2018)3.52Sparse Transformer 152M (512W, 48L, 16H)3.44Density modeling performance in bits per byte (or dim) on a variety of benchmark datasets. M denotes millions of parameters used in the network, W the width of the network, L the number of layers, and H the number of heads.We also found that sparse attention achieved lower loss than full attention, in addition to being significantly faster (see our paper for comparisons). This may point to a useful inductive bias from our sparsity patterns, or an underlying optimization issue with denseÂ attention.Generating imagesTransformers that use sparse attention seem to have a notion of global structure, which can be qualitatively evaluated by looking at image completions. Here we visualize a model trained onÂ 64Ã64 64\times 64 64Ã64 ImageNet:We also generated fully unconditional samples with an unadjusted softmax temperature of 1.0. These models are trained using the maximum likelihood objective, which is well-known to cover all modes of the data (including potentially nonexistent ones) instead of increasing fidelity of a smaller portion of the data. Sampling from these models with unadjusted temperature lets us see the full distribution of images that the model believes exists in the world. As a result, some samples can appearÂ strange.Model samplesReal dataGenerating raw audio waveformsSparse Transformers can also be adapted to generate raw audio instead of images by simply changing the position embeddings. As deep learning expands to novel data types, we believe the ease of specifying inductive biases with this class of networks will be a usefulÂ tool.This model was trained on raw classical music clips and uses sparse attention to generate sequences of length 65,000. This corresponds to ~5 seconds of raw audio, and we have concatenated several samples together in each of the clipsÂ below.Code releaseNormally, implementing sparse attention would involve slicing query and key matrices in blocks, so to ease experimentation we implemented a set ofÂ block-sparse kernelsÂ which efficiently perform these operations on the the GPU. We open-source these kernels and provide example sparse attention functions inÂ thisÂ repository.Future work and limitationsThe sparse attention patterns we introduced are only preliminary steps in the direction of efficient modeling of long sequences. We think exploring different patterns and combinations of sparsity is useful, and that learning sparse patterns is a particularly promising avenue of research for the next generation of neural networkÂ architectures.Even with the improvements we described above, autoregressive sequence generation still seems impractical for very high resolution images or video. The optimized attention operations we have introduced, however, may be useful primitives to combine with other approaches to modeling high dimensional data, like multi-scaleÂ approaches.If you are interested in advancing AI capabilities and helping further our mission of ensuring they benefit humanity,Â weâreÂ hiring!AuthorsRewon ChildScott GrayAcknowledgmentsThanks to Ashish Vaswani for helpful discussions, and Johannes Otterbach, Mark Chen, Prafulla Dhariwal, David Luan, and Lukasz Kaiser for comments on the manuscript.ResearchOverviewIndexProductOverviewChatGPTGPT-4DALLÂ·E 2Customer storiesSafety standardsAPI data privacyPricingSafetyOverviewCompanyAboutBlogCareersCharterSecurityOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top
