


Be My Eyes












CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexProductOverviewChatGPTGPT-4DALLÂ·E 2Customer storiesSafety standardsAPI data privacyPricingDevelopersOverviewDocumentationAPI referenceExamplesSafetyCompanyAboutBlogCareersCharterSecuritySearch Navigation quick links Log inSign upMenu Mobile Navigation CloseSite NavigationResearchProductDevelopersSafetyCompany Quick Links Log inSign upSearch Submit Be My EyesBe My Eyes uses GPT-4 to transform visual accessibility.March 14, 2023More resourcesView productLanguage,Â GPT-4Since 2012, Be My Eyes has been creating technology for the community of over 250 million people who are blind or have low vision. The Danish startup connects people who are blind or have low vision with volunteers for help with hundreds of daily life tasks like identifying a product or navigating an airport.With the new visual input capability of GPT-4 (in research preview), Be My Eyes began developing a GPT-4 powered Virtual Volunteerâ¢ within the Be My Eyes app that can generate the same level of context and understanding as a human volunteer.Introducing Be My Eyes00:47âIn the short time weâve had access, we have seen unparalleled performance to any image-to-text object recognition tool out there," says Michael Buckley, CEO of Be My Eyes. âThe implications for global accessibility are profound. In the not so distant future, the blind and low vision community will utilize these tools not only for a host of visual interpretation needs, but also to have a greater degree of independence in their lives.âÂ Â Suddenly, the image someone sends of, say, the contents of their fridge, GPT-4 technology not only recognizes and names whatâs in there, but extrapolates and analyzes what you can make with those ingredients. You could then ask it for a good recipe. The use cases are almost unlimited.âThatâs game changing,â says Buckley. âUltimately, whatever the user wants or needs, they can re-prompt the tool to get more information that is usable, beneficial and helpful, nearly instantly.âBe My Eyes Virtual Volunteer01:14In early February, the company started beta-testing the GPT-backed assistant with a small group of employees; the results have been so positive the feature will be in the hands of users in weeks.âThereâs just an incredible potential for our community,â Buckley says. âOur beta testers, including Lucy Edwards, already love what this does.âThe difference between GPT-4 and other language and machine learning models, explains Jesper Hvirring Henriksen, CTO of Be My Eyes, is both the ability to have a conversation and the greater degree of analytical prowess offered by the technology. âBasic image recognition applications only tell you whatâs in front of youâ, he says. âThey canât have a discussion to understand if the noodles have the right kind of ingredients or if the object on the ground isnât just a ball, but a tripping hazardâand communicate that.âÂ The difference between GPT-4 and other language and machine learning models is both the ability to have a conversation and the greater degree of analytical prowess offered by the technology.Jesper Hvirring Henriksen, CTO of Be My EyesAlready the company has a case where a user was able to navigate the railway systemâarguably an impossible task for the sighted as wellânot only getting details about where they were located on a map, but point-by-point instructions on how to safely reach where they wanted to go.Yet traversing the complicated physical world is only half the story. Understanding whatâs on a screen can be twice as arduous for a person who isnât sighted. Screen readers, embedded in most modern operating systems, read through the pieces of a web page or desktop application line by line, section by section, speaking each word. Images, the heart of communication on the web, can be even worse.Â Yet, Henriksen says now theyâre able to show GPT-4 the webpage and the system knowsâafter countless training hours where deep learning algorithms build relationships to understand the âimportantâ part of a webpageâwhich part to read or summarize. This can not only simplify tasks like reading the news online, but grants people who need visual assistance access to some of the most cluttered pages on the web: shopping and e-commerce sites. GPT-4 is able to summarize the search results the way the sighted naturally scan themânot reading every minuscule detail but bouncing between important data pointsâand help those needing sight support make the right purchase, in real-time.âThis is a fantastic development for humanityâ, Buckley says, âbut it also represents an enormous commercial opportunity.â ResearchOverviewIndexProductOverviewChatGPTGPT-4DALLÂ·E 2Customer storiesSafety standardsAPI data privacyPricingSafetyOverviewCompanyAboutBlogCareersCharterSecurityOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top
