


Roboschool












CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexProductOverviewChatGPTGPT-4DALLÂ·E 2Customer storiesSafety standardsAPI data privacyPricingDevelopersOverviewDocumentationAPI referenceExamplesSafetyCompanyAboutBlogCareersCharterSecuritySearch Navigation quick links Log inSign upMenu Mobile Navigation CloseSite NavigationResearchProductDevelopersSafetyCompany Quick Links Log inSign upSearch Submit RoboschoolWe are releasing Roboschool: open-source software for robot simulation, integrated with OpenAI Gym.May 15, 2017Robotics,Â Environments,Â Software engineering,Â Open source,Â ReleaseThree control policies running on three different robots, racing each other in Roboschool. You can re-enact this scene by running agentzoo/demorace1.py. Each time you run the script, a random set of robots appears.Roboschool provides new OpenAI Gym environments for controlling robots in simulation. Eight of these environments serve as free alternatives to pre-existing MuJoCo implementations, re-tuned to produce more realistic motion. We also include several new, challengingÂ environments.Roboschool also makes it easy to train multiple agents together in the sameÂ environment.After we launchedÂ Gym, one issue we heard from many users was that theÂ MuJoCoÂ component required a paid license (though MuJoCo recently addedÂ freeÂ student licenses for personal and class work). Roboschool removes this constraint, letting everyone conduct research regardless of their budget. Roboschool is based on theÂ Bullet Physics Engine, an open-source, permissivelyÂ licensedÂ physics library that has been used by other simulation software such asÂ GazeboÂ andÂ V-REP.EnvironmentsRoboschool ships with twelve environments, including tasks familiar to Mujoco users as well as new challenges, such as harder versions of the Humanoid walker task, and a multi-player Pong environment. We plan to expand this collection over time and look forward to the community contributing asÂ well.For the existing MuJoCo environments, besides porting them to Bullet, we have modified them to be more realistic. Here are three of the environments we ported, with explanations of how they differ from the existingÂ environments.RoboschoolExistingCommentsWalker2d re-tuned to produce more realistic slower-paced motion.Ant is heavier, encouraging it to typically have two or more legs on the ground.Humanoid benefits from more realistic energy cost (= torque Ã angular velocity) subtracted from reward.You can find trained policies for all of these environments in theÂ agent_zooÂ folder in the GitHub repository. You can also access aÂ demo_raceÂ script to initiate a race between threeÂ robots.Interactive and robust controlIn several of the previous OpenAI Gym environments, the goal was to learn a walking controller. However, these environments involved a very basic version of the problem, where the goal is simply to move forward. In practice, the walking policies would learn a single cyclic trajectory and leave most of the state space unvisited. Furthermore, the final policies tended to be very fragile: a small push would often cause the robot to crash andÂ fall.We have added two more environments with the 3D humanoid, which make the locomotion problem more interesting and challenging. These environments requireÂ interactive controlÂ â the robots must run towards a flag, whose position randomly varies overÂ time.HumanoidFlagrun is designed to teach the robot to slow down and turn. The goal is to run towards the flag, whose position variesÂ randomly.HumanoidFlagrunHarder in addition allows the robot to fall and gives it time to get back on foot. It also starts each episode upright or laying on the ground, and the robot is constantly bombarded by white cubes to push it off itsÂ trajectory.We ship trained policies for bothÂ HumanoidFlagrunÂ andÂ HumanoidFlagrunHarder. The walks arenât as fast and natural-looking as the ones we see from the regular humanoid, but these policies can recover from many situations, and they know how to steer. This policy itself is still a multilayer perceptron, which has no internal state, so we believe that in some cases the agent uses its arms to storeÂ information.MultiplayerRoboschool lets you both run and train multiple agents in the same environment. We start with RoboschoolPong, with more environments toÂ follow.With multiplayer training, you can train the same agent playing for both parties (so it plays with itself), you can train two different agents using the same algorithm, or you can even set two different algorithms against eachÂ other.Two agents learning to play RoboschoolPong against each other.The multi-agent setting presents some interesting challenges. If you train both players simultaneously, youâll likely see a learning curve like the following one, obtained from a policy gradientÂ method:Learning curves for pong, where policies are updated with policy gradient algorithms running simultaneously.Hereâs whatâsÂ happening:Agent1 (green) learns it can sometimes hit a ball at the top, so it moves to theÂ top.Agent2 (purple) discovers that its adversary is at the top, so it sends the ball to the bottom and overfits to other agent beingÂ away.Agent1 eventually discovers it can defend itself by moving to the bottom, but now always stay at the bottom, because Agent2 always sends ball to theÂ bottom.That way, the policies oscillated, and neither agent learned anything useful after hours of training. As in generative adversarial networks, learning in an adversarial setting is tricky, but we think itâs an interesting research problem because this interplay can lead to sophisticated strategies even in simple environments, and it can provide a naturalÂ curriculum.See alsoThereâs been a lot of work by the community to createÂ environments for OpenAI Gym, some of which are based on open-source physics simulators. In one recent project, researchers created aÂ fork of OpenAI GymÂ that replaced MuJoCo by the open-source physics simulatorÂ DART. TheyÂ showedÂ that policies can even be transferred between the two physics simulators, MuJoCo andÂ DART.ResearchOverviewIndexProductOverviewChatGPTGPT-4DALLÂ·E 2Customer storiesSafety standardsAPI data privacyPricingSafetyOverviewCompanyAboutBlogCareersCharterSecurityOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top
