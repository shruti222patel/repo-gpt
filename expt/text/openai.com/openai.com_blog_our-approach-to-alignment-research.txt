


Our approach to alignment research












CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexProductOverviewChatGPTGPT-4DALLÂ·E 2Customer storiesSafety standardsAPI data privacyPricingDevelopersOverviewDocumentationAPI referenceExamplesSafetyCompanyAboutBlogCareersCharterSecuritySearch Navigation quick links Log inSign upMenu Mobile Navigation CloseSite NavigationResearchProductDevelopersSafetyCompany Quick Links Log inSign upSearch Submit Our approach to alignment researchWe are improving our AI systemsâ ability to learn from human feedback and to assist humans at evaluating AI. Our goal is to build a sufficiently aligned AI system that can help us solve all other alignmentÂ problems.Illustration: Justin Jay WangAugust 24, 2022AuthorsJan LeikeJohn SchulmanJeffrey WuSafety & AlignmentOur alignment researchÂ aims to make artificial general intelligenceÂ (AGI) aligned with human values and follow human intent. We take an iterative, empirical approach: by attempting to align highly capable AI systems, we can learn what works and what doesnât, thus refining our ability to make AI systems safer and more aligned. Using scientific experiments, we study how alignment techniques scale and where they willÂ break.We tackle alignment problems both in our most capable AI systems as well as alignment problems that we expect to encounter on our path to AGI. Our main goal is to push current alignment ideas as far as possible, and to understand and document precisely how they can succeed or why they will fail. We believe that even without fundamentally new alignment ideas, we can likely build sufficiently aligned AI systems to substantially advance alignment researchÂ itself.Unaligned AGI could pose substantial risks to humanityÂ and solving the AGI alignment problem could be so difficult that it will require all of humanity to work together. Therefore we are committed to openly sharing our alignment research when itâs safe to do so: We want to be transparent about how well our alignment techniques actually work in practice and we want every AGI developer to use the worldâs best alignmentÂ techniques.At a high-level, our approach to alignment research focuses on engineering a scalable training signal for very smart AI systems that is aligned with human intent. It has three mainÂ pillars:Training AI systems using humanÂ feedbackTraining AI systems to assist humanÂ evaluationTraining AI systems to do alignmentÂ researchAligning AI systems with human values also poses a range of other significant sociotechnical challenges, such as deciding to whom these systems should be aligned. Solving these problems is important to achievingÂ our mission, but we do not discuss them in thisÂ post.We want to be transparent about how well our alignment techniques actually work in practice and we want every AGI developer to use the worldâs best alignment techniques.Training AI systems using human feedbackRL from human feedbackÂ is our main technique for aligning our deployed language models today. We train a class of models calledÂ InstructGPTÂ derived from pretrained language models such as GPT-3. These models are trained to follow human intent: both explicit intent given by an instruction as well as implicit intent such as truthfulness, fairness, andÂ safety.Our results show that there is a lot of low-hanging fruit on alignment-focused fine-tuning right now: InstructGPT is preferred by humans over a 100x larger pretrained model, while its fine-tuning costs <2% of GPT-3âs pretraining compute and about 20,000 hours of human feedback. We hope that our work inspires others in the industry to increase their investment in alignment of large language models and that it raises the bar on usersâ expectations about the safety of deployedÂ models.Our natural language APIÂ is a very useful environment for our alignment research: It provides us with a rich feedback loop about how well our alignment techniques actually workÂ in the real world, grounded in a very diverse set of tasks that our customers are willing to pay money for. On average, our customers already prefer to use InstructGPT over our pretrainedÂ models.Yet todayâs versions of InstructGPT areÂ quite far from fully aligned: they sometimes fail to follow simple instructions, arenât always truthful, donât reliably refuse harmful tasks, and sometimes give biased or toxic responses. Some customers find InstructGPTâs responses significantly less creative than the pretrained modelsâ, something we hadnât realized from running InstructGPT on publicly available benchmarks. We are also working on developing a more detailed scientific understanding of RL from human feedback and how to improve the quality of humanÂ feedback.Aligning our API is much easier than aligning AGI since most tasks on our API arenât very hard for humans to supervise and our deployed language models arenât smarter than humans. We donât expect RL from human feedback to be sufficient to align AGI, but it is a core building block for the scalable alignment proposals that weâre most excited about, and so itâs valuable to perfect thisÂ methodology.Training models to assist human evaluationRL from human feedback has a fundamental limitation: it assumes that humans can accurately evaluate the tasks our AI systems are doing. Today humans are pretty good at this, but as models become more capable, they will be able to do tasks that are much harder for humans to evaluate (e.g., finding all the flaws in a large codebase or a scientific paper). Our models might learn to tell our human evaluators what they want to hear instead of telling them the truth. In order to scale alignment, we want to use techniques likeÂ recursive reward modeling (RRM),Â debate, andÂ iteratedÂ amplification.Currently our main direction is based on RRM: we train models that can assist humans at evaluating our models on tasks that are too difficult for humans to evaluate directly. ForÂ example:We trained a model toÂ summarize books. Evaluating book summaries takes a long time for humans if they are unfamiliar with the book, but our model can assist human evaluation by writing chapterÂ summaries.We trained a model toÂ assist humans at evaluating the factual accuracyÂ by browsing the web and providing quotes and links. On simple questions, this modelâs outputs are already preferred to responses written byÂ humans.We trained a model toÂ write critical comments on its own outputs: On a query-based summarization task, assistance with critical comments increases the flaws humans find in model outputs by 50% on average. This holds even if we ask humans to write plausible looking but incorrectÂ summaries.We are creating a set of coding tasks selected to be very difficult to evaluate reliably for unassisted humans. We hope to release this data setÂ soon.Our alignment techniques need to work even if our AI systems are proposing very creative solutions (likeÂ AlphaGoâs move 37), thus we are especially interested in training models to assist humans to distinguish correct from misleading or deceptive solutions. We believe the best way to learn as much as possible about how to make AI-assisted evaluation work in practice is to build AIÂ assistants.Training AI systems to do alignment researchThere is currently no known indefinitely scalable solution to the alignment problem. As AI progress continues, we expect to encounter a number of new alignment problems that we donât observe yet in current systems. Some of these problems we anticipate now and some of them will be entirelyÂ new.We believe that finding an indefinitely scalable solution is likely very difficult. Instead, we aim for a more pragmatic approach: building and aligning a system that can make faster and better alignment research progress than humansÂ can.As we make progress on this, our AI systems can take over more and more of our alignment work and ultimately conceive, implement, study, and develop better alignment techniques than we have now. They will work together with humans to ensure that their own successors are more aligned withÂ humans.We believe that evaluating alignment research is substantially easier than producing it, especially when provided with evaluation assistance. Therefore human researchers will focus more and more of their effort on reviewing alignment research done by AI systems instead of generating this research by themselves. Our goal is to train models to be so aligned that we can off-load almost all of the cognitive labor required for alignment research.Importantly, we only need ânarrowerâ AI systems that have human-level capabilities in the relevant domains to do as well as humans on alignment research. We expect these AI systems are easier to align than general-purpose systems or systems much smarter thanÂ humans.Language models are particularly well-suited for automating alignment research because they come âpreloadedâ with a lot of knowledge and information about human values from reading the internet. Out of the box, they arenât independent agents and thus donât pursue their own goals in the world. To do alignment research they donât need unrestricted access to the internet. Yet a lot of alignment research tasks can be phrased as natural language or codingÂ tasks.Future versions ofÂ WebGPT,Â InstructGPT, andÂ CodexÂ can provide a foundation as alignment research assistants, but they arenât sufficiently capable yet. While we donât know when our models will be capable enough to meaningfully contribute to alignment research, we think itâs important to get started ahead of time. Once we train a model that could be useful, we plan to make it accessible to the external alignment researchÂ community.LimitationsWeâre very excited about this approach towards aligning AGI, but we expect that it needs to be adapted and improved as we learn more about how AI technology develops. Our approach also has a number of importantÂ limitations:The path laid out here underemphasizes the importance of robustness and interpretability research, two areas OpenAI is currently underinvested in. If this fits your profile, please apply for our research scientistÂ positions!Using AI assistance for evaluation has the potential to scale up or amplify even subtle inconsistencies, biases, or vulnerabilities present in the AIÂ assistant.Aligning AGI likely involves solving very different problems than aligning todayâs AI systems. We expect the transition to be somewhat continuous, but if there are major discontinuities or paradigm shifts, then most lessons learned from aligning models like InstructGPT might not be directlyÂ useful.The hardest parts of the alignment problem might not be related to engineering a scalable and aligned training signal for our AI systems. Even if this is true, such a training signal will beÂ necessary.It might not be fundamentally easier to align models that can meaningfully accelerate alignment research than it is to align AGI. In other words, the least capable models that can help with alignment research might already be too dangerous if not properly aligned. If this is true, we wonât get much help from our own systems for solving alignmentÂ problems.Weâre looking to hire more talented people for this line of research! If this interests you, weâre hiringÂ Research EngineersÂ andÂ ResearchÂ Scientists.AuthorsJan LeikeView all articlesJohn SchulmanView all articlesJeffrey WuView all articlesResearchOverviewIndexProductOverviewChatGPTGPT-4DALLÂ·E 2Customer storiesSafety standardsAPI data privacyPricingSafetyOverviewCompanyAboutBlogCareersCharterSecurityOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top
