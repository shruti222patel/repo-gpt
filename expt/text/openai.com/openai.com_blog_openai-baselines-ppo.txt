


Proximal Policy Optimization












CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexProductOverviewChatGPTGPT-4DALLÂ·E 2Customer storiesSafety standardsAPI data privacyPricingDevelopersOverviewDocumentationAPI referenceExamplesSafetyCompanyAboutBlogCareersCharterSecuritySearch Navigation quick links Log inSign upMenu Mobile Navigation CloseSite NavigationResearchProductDevelopersSafetyCompany Quick Links Log inSign upSearch Submit Illustration: Ben BarryProximal Policy OptimizationWeâre releasing a new class of reinforcement learning algorithms, Proximal Policy Optimization (PPO), which perform comparably or better than state-of-the-art approaches while being much simpler to implement and tune. PPO has become the default reinforcement learning algorithm at OpenAI because of its ease of use and good performance.July 20, 2017More resourcesView codeRead paperReinforcement learning,Â Policy optimization,Â Robotics,Â Games,Â Open source,Â Software engineering,Â Milestone,Â Publication,Â ReleasePPO lets us train AI policies in challenging environments, like the Roboschool one shown above where an agent tries to reach a target (the pink sphere), learning to walk, run, turn, use its momentum to recover from minor hits, and how to stand up from the ground when it is knocked over.Policy gradient methodsÂ are fundamental to recent breakthroughs in using deep neural networks for control, fromÂ video games, toÂ 3D locomotion, toÂ Go. But getting good results via policy gradient methods is challenging because they are sensitive to the choice of stepsize â too small, and progress is hopelessly slow; too large and the signal is overwhelmed by the noise, or one might see catastrophic drops in performance. They also often have very poor sample efficiency, taking millions (or billions) of timesteps to learn simpleÂ tasks.Researchers have sought to eliminate these flaws with approaches likeÂ TRPOÂ andÂ ACER, by constraining or otherwise optimizing the size of a policy update. These methods have their own trade-offsâACER is far more complicated than PPO, requiring the addition of code for off-policy corrections and a replay buffer, while only doing marginally better than PPO on the Atari benchmark; TRPOâthough useful for continuous control tasksâisnât easily compatible with algorithms that share parameters between a policy and value function or auxiliary losses, like those used to solve problems in Atari and other domains where the visual input isÂ significant.PPOWith supervised learning, we can easily implement the cost function, run gradient descent on it, and be very confident that weâll get excellent results with relatively little hyperparameter tuning. The route to success in reinforcement learning isnât as obviousâthe algorithms have many moving parts that are hard to debug, and they require substantial effort in tuning in order to get good results. PPO strikes a balance between ease of implementation, sample complexity, and ease of tuning, trying to compute an update at each step that minimizes the cost function while ensuring the deviation from the previous policy is relativelyÂ small.WeâveÂ previouslyÂ detailed a variant of PPO that uses an adaptiveÂ KLÂ penalty to control the change of the policy at each iteration. The new variant uses a novel objective function not typically found in otherÂ algorithms:LCLIP(Î¸)=E^t[min(rt(Î¸))A^t,clip(rt(Î¸),1âÎµ,1+Îµ)A^t)]L^{CLIP}(\theta) = \hat{E}_{t}[ min(r_t(\theta))\hat{A}_t, clip(r_t(\theta), 1 - \varepsilon, 1 + \varepsilon) \hat{A}_t ) ]LCLIP(Î¸)=E^tâ[min(rtâ(Î¸))A^tâ,clip(rtâ(Î¸),1âÎµ,1+Îµ)A^tâ)]Î¸ \theta Î¸ is the policyÂ parameterE^t \hat{E}_{t} E^tâ denotes the empirical expectation overÂ timestepsrt r_t rtâ is the ratio of the probability under the new and old policies,Â respectivelyA^t \hat{A}_t A^tâ is the estimated advantage at time t t tÎµ \varepsilon Îµ is a hyperparameter, usually 0.1 orÂ 0.2This objective implements a way to do a Trust Region update which is compatible with Stochastic Gradient Descent, and simplifies the algorithm by removing the KL penalty and need to make adaptive updates. In tests, this algorithm has displayed the best performance on continuous control tasks and almost matches ACERâs performance on Atari, despite being far simpler toÂ implement.Controllable, complicated robotsAgents trained with PPO develop flexible movement policies that let them improvise turns and tilts as they head towards a target location.Weâve created interactive agents based on policies trained by PPOâwe canÂ use the keyboardÂ to set new target positions for a robot in an environment within Roboschool; though the input sequences are different from what the agent was trained on, it manages toÂ generalize.Baselines: PPO, PPO2, ACER, and TRPOThis release ofÂ baselinesÂ includes scalable, parallel implementations of PPO and TRPO which both use MPI for data passing. Both use Python3 and TensorFlow. Weâre also adding pre-trained versions of the policies used to train the above robots to theÂ RoboschoolÂ agentÂ zoo.Update: Weâre also releasing a GPU-enabled implementation of PPO, called PPO2. This runs approximately 3x faster than the current PPO baseline on Atari. In addition, weâre releasing an implementation of Actor Critic with Experience Replay (ACER), a sample-efficient policy gradient algorithm. ACER makes use of a replay buffer, enabling it to perform more than one gradient update using each piece of sampled experience, as well as a Q-Function approximate trained with the RetraceÂ algorithm.Weâre looking for people to help build and optimize our reinforcement learning algorithm codebase. If youâre excited about RL, benchmarking, thorough experimentation, and open source, pleaseÂ apply, and mention that you read the baselines PPO post in yourÂ application.AuthorsJohn SchulmanOleg KlimovFilip WolskiPrafulla DhariwalAlec RadfordResearchOverviewIndexProductOverviewChatGPTGPT-4DALLÂ·E 2Customer storiesSafety standardsAPI data privacyPricingSafetyOverviewCompanyAboutBlogCareersCharterSecurityOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top
