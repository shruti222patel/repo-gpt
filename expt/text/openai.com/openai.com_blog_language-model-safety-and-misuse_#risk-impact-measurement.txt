


Lessons learned on language model safety and misuse












CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexProductOverviewChatGPTGPT-4DALLÂ·E 2Customer storiesSafety standardsAPI data privacyPricingDevelopersOverviewDocumentationAPI referenceExamplesSafetyCompanyAboutBlogCareersCharterSecuritySearch Navigation quick links Log inSign upMenu Mobile Navigation CloseSite NavigationResearchProductDevelopersSafetyCompany Quick Links Log inSign upSearch Submit Lessons learned on language model safety and misuseIllustration: Justin Jay WangWe describe our latest thinking in the hope of helping other AI developers address safety and misuse of deployedÂ models.March 3, 2022More resourcesRead research agendaSafety & Alignment,Â Language,Â Responsible AI,Â ConclusionSummaryThe deployment of powerful AI systems has enriched our understanding of safety and misuse far more than would have been possible through research alone. Notably: API-based language model misuse often comes in different forms than we feared most; we have identified limitations in existing language model evaluations that we are addressing with novel benchmarks and classifiers; and basic safety research offers significant benefits for the commercial utility of AI systems.Over the past two years, weâve learned a lot about how language models can be used and abusedâinsights we couldnât have gained without the experience of real-world deployment. In June 2020, we began giving access to developers and researchers to theÂ OpenAI API, an interface for accessing and building applications on top of new AI models developed by OpenAI. Deploying GPT-3, Codex, and other models in a way that reduces risks of harm has posed various technical and policyÂ challenges.Overview of our model deployment approachLarge language models are now capable of performing aÂ very wide range of tasks, often out of the box. Their risk profiles, potential applications, and wider effects on societyÂ remainÂ poorlyÂ understood. As a result, our deployment approach emphasizes continuous iteration, and makes use of the following strategies aimed at maximizing the benefits of deployment while reducing associatedÂ risks:Pre-deployment risk analysis, leveraging a growing set of safety evaluations and red teaming tools (e.g., we checked our InstructGPT for any safety degradations using the evaluationsÂ discussedÂ below)Starting with a small user base (e.g., both GPT-3 and ourÂ InstructGPTÂ series began as privateÂ betas)Studying the results of pilots of novel use cases (e.g., exploring the conditions under which we could safely enable longform content generation, working with a small number ofÂ customers)Implementing processes that help keep a pulse on usage (e.g., review of use cases, token quotas, and rateÂ limits)Conducting detailed retrospective reviews (e.g., of safety incidents and majorÂ deployments)There is no silver bullet for responsible deployment, so we try to learn about and address our modelsâ limitations, and potential avenues for misuse, at every stage of development and deployment. This approach allows us to learn as much as we can about safety and policy issues at small scale and incorporate those insights prior to launching larger-scaleÂ deployments.There is no silver bullet for responsible deployment.While not exhaustive, some areas where weâve invested so far include[^footnote-1]:Pre-trainingÂ dataÂ curation andÂ filteringFine-tuningÂ models to betterÂ followÂ instructionsRisk analysis of potentialÂ deploymentsProviding detailed userÂ documentationBuildingÂ toolsÂ to screen harmful modelÂ outputsReviewing use cases against ourÂ policiesMonitoring for signs ofÂ misuseStudying theÂ impacts of ourÂ modelsSince each stage of intervention has limitations, a holistic approach isÂ necessary.There are areas where we could have done more and where we still have room for improvement. For example, when we first worked on GPT-3, we viewed it as an internal research artifact rather than a production system and were not as aggressive in filtering out toxic training data as we might have otherwise been. We have invested more in researching and removing such material for subsequent models. We have taken longer to address some instances of misuse in cases where we did not have clear policies on the subject, and have gotten better at iterating on those policies. And we continue to iterate towards a package of safety requirements that is maximally effective in addressing risks, while also being clearly communicated to developers and minimizing excessiveÂ friction.Still, we believe that our approach has enabled us to measure and reduce various types of harms from language model use compared to a more hands-off approach, while at the same time enabling a wide range of scholarly, artistic, and commercial applications of our models.[^footnote-2]The many shapes and sizes of language model misuseOpenAI has been active in researching the risks of AI misuse since our early work on theÂ malicious use of AIÂ in 2018 andÂ on GPT-2Â in 2019, and we have paid particular attention to AI systems empowering influence operations. We haveÂ worked withÂ external experts to developÂ proofs of conceptÂ and promotedÂ carefulÂ analysisÂ of such risks by third parties. We remain committed to addressing risks associated with language model-enabled influence operations and recently co-organized a workshop on the subject.[^footnote-3]Yet we have detected and stopped hundreds of actors attempting to misuse GPT-3 for a much wider range of purposes than producing disinformation for influence operations, including in ways that we either didnât anticipate or which we anticipated but didnât expect to be so prevalent.[^footnote-4]Â OurÂ use case guidelines,Â content guidelines, and internal detection and response infrastructure were initially oriented towards risks that we anticipated based on internal and external research, such as generation of misleading political content with GPT-3 or generation of malware with Codex. Our detection and response efforts have evolved over time in response to real cases of misuse encountered âin the wildâ that didnât feature as prominently as influence operations in our initial risk assessments. Examples include spam promotions for dubious medical products and roleplaying of racistÂ fantasies.To support the study of language model misuse and mitigation thereof, we are actively exploring opportunities to share statistics on safety incidents this year, in order to concretize discussions about language modelÂ misuse.The difficulty of risk and impact measurementMany aspects of language modelsâ risks and impacts remain hard to measure and therefore hard to monitor, minimize, and disclose in an accountable way. We have made active use of existing academic benchmarks for language model evaluation and are eager to continue building on external work, but we have also have found that existing benchmark datasets are often not reflective of the safety and misuse risks we see in practice.[^footnote-5]Such limitations reflect the fact that academic datasets are seldom created for the explicit purpose of informing production use of language models, and do not benefit from the experience gained from deploying such models at scale. As a result, weâve been developing new evaluation datasets and frameworks for measuring the safety of our models, which we plan to release soon. Specifically, we have developed new evaluation metrics for measuring toxicity in model outputs and have also developed in-house classifiers for detecting content that violates ourÂ content policy, such as erotic content, hate speech, violence, harassment, and self-harm. Both of these in turn have also been leveraged for improving our pre-training data[^footnote-6]âspecifically, by using the classifiers to filter out content and the evaluation metrics to measure the effects of datasetÂ interventions.Reliably classifying individual model outputs along various dimensions is difficult, and measuring their social impact at the scale of the OpenAI API is even harder. We have conducted several internal studies in order to build an institutional muscle for such measurement, but these have often raised more questions thanÂ answers.We are particularly interested in better understanding the economic impact of our models and the distribution of those impacts. We have good reason to believe that the labor market impacts from the deployment of current models may be significant in absolute terms already, and that they will grow as the capabilities and reach of our models grow. We have learned of a variety of local effects to date, including massive productivity improvements on existing tasks performed by individuals like copywriting and summarization (sometimes contributing to job displacement and creation), as well as cases where the API unlocked new applications that were previously infeasible, such asÂ synthesis of large-scale qualitative feedback. But we lack a good understanding of the netÂ effects.We believe that it is important for those developing and deploying powerful AI technologies to address both the positive and negative effects of their work head-on. We discuss some steps in that direction in the concluding section of thisÂ post.The relationship between the safety and utility of AI systemsIn ourÂ Charter, published in 2018, we say that we âare concerned about late-stage AGI development becoming a competitive race without time for adequate safety precautions.â We thenÂ publishedÂ a detailed analysis of competitive AI development, and we have closely followedÂ subsequentÂ research. At the same time, deploying AI systems via the OpenAI API has also deepened our understanding of the synergies between safety andÂ utility.For example, developers overwhelmingly prefer our InstructGPT modelsâwhich are fine-tuned to follow user intentions [^footnote-7] â over the base GPT-3 models. Notably, however, the InstructGPT models were not originally motivated by commercial considerations, but rather were aimed at making progress on long-termalignment problems. In practical terms, this means that customers, perhaps not surprisingly, much prefer models that stay on task and understand the userâs intent, and models that are less likely to produce outputs that are harmful or incorrect.[^footnote-8] Other fundamental research, such as our work onleveraging informationretrieved from the Internet in order to answer questions more truthfully, also has potential to improve the commercial utility of AI systems.[^footnote-9] These synergies will not always occur. For example, more powerful systems will often take more time to evaluate and align effectively, foreclosing immediate opportunities for profit. And a userâs utility and that of society may not be aligned due to negative externalitiesâconsider fully automated copywriting, which can be beneficial for content creators but bad for the information ecosystem as aÂ whole.It is encouraging to see cases of strong synergy between safety and utility, but we are committed to investing in safety and policy research even when they trade off with commercialÂ utility.We are committed to investing in safety and policy research even when they trade off against commercial utility.Ways to get involvedEach of the lessons above raises new questions of its own. What kinds of safety incidents might we still be failing to detect and anticipate? How can we better measure risks and impacts? How can we continue to improve both the safety and utility of our models, and navigate tradeoffs between these two when they doÂ arise?We are actively discussing many of these issues with other companies deploying language models. But we also know that no organization or set of organizations has all the answers, and we would like to highlight several ways that readers can get more involved in understanding and shaping our deployment of state of the art AIÂ systems.First, gaining first-hand experience interacting with state of the art AI systems is invaluable for understanding their capabilities and implications. We recently ended the API waitlist after building more confidence in our ability to effectively detect and respond to misuse. Individuals inÂ supported countries and territoriesÂ can quickly get access to the OpenAI API by signing upÂ here.Second, researchers working on topics of particular interest to us such as bias and misuse, and who would benefit from financial support, can apply for subsidized API credits usingÂ this form. External research is vital for informing both our understanding of these multifaceted systems, as well as wider publicÂ understanding.Finally, today we are publishing aÂ research agendaÂ exploring the labor market impacts associated with our Codex family of models, and a call for external collaborators on carrying out this research. We are excited to work with independent researchers to study the effects of our technologies in order to inform appropriate policy interventions, and to eventually expand our thinking from code generation to otherÂ modalities.If youâre interested in working to responsibly deploy cutting-edge AI technologies,Â applyÂ to work atÂ OpenAI!AuthorsMiles BrundageKatie MayerTyna EloundouSandhini AgarwalSteven AdlerGretchen KruegerJan LeikePamela MishkinAcknowledgmentsThanks to Lilian Weng, Rosie Campbell, Anna Makanju, Bob McGrew, Hannah Wong, Ryan Lowe, Steve Dowling, Mira Murati, Sam Altman, Greg Brockman, Ilya Sutskever, Percy Liang, Peter Welinder, Ethan Perez, Ellie Evans, Helen Ngo, Helen Toner, Justin Jay Wang, Jack Clark, Rishi Bommasani, Girish Sastry, Sarah Shoker, Matt Knight, Bianca Martin, Bob Rotsted, Lama Ahmad, Toki Sherbakov, and others for providing feedback on this post and related work.ResearchOverviewIndexProductOverviewChatGPTGPT-4DALLÂ·E 2Customer storiesSafety standardsAPI data privacyPricingSafetyOverviewCompanyAboutBlogCareersCharterSecurityOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top
