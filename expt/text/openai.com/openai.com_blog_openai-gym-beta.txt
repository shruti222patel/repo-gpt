


OpenAI Gym Beta












CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexProductOverviewChatGPTGPT-4DALLÂ·E 2Customer storiesSafety standardsAPI data privacyPricingDevelopersOverviewDocumentationAPI referenceExamplesSafetyCompanyAboutBlogCareersCharterSecuritySearch Navigation quick links Log inSign upMenu Mobile Navigation CloseSite NavigationResearchProductDevelopersSafetyCompany Quick Links Log inSign upSearch Submit OpenAI Gym BetaWeâre releasing the public beta of OpenAI Gym, a toolkit for developing and comparing reinforcement learning (RL) algorithms. It consists of a growing suite of environments (from simulated robots to Atari games), and a site for comparing and reproducing results.April 27, 2016More resourcesRead paperEnvironments,Â Games,Â Reinforcement learning,Â Robotics,Â Software engineering,Â Open source,Â Release,Â PublicationOpenAI Gym is compatible with algorithms written in any framework, such asÂ TensorflowÂ andÂ Theano. The environments are written in Python, but weâll soon make them easy to use from any language. We originally built OpenAI Gym as a tool to accelerate our own RL research. We hope it will be just as useful for the broaderÂ community.Getting startedIf youâd like to dive in right away, you can work through ourÂ tutorial. You can also help out while learning byÂ reproducingÂ aÂ result.Why RL?Reinforcement learning (RL) is the subfield of machine learning concerned with decision making and motor control. It studies how an agent can learn how to achieve goals in a complex, uncertain environment. Itâs exciting for twoÂ reasons:RL is very general, encompassing all problems that involve making a sequence of decisions:Â for example, controlling a robotâs motors so that itâs able toÂ runÂ andÂ jump, making business decisions like pricing and inventory management, or playingÂ video gamesÂ andÂ board games. RL can even be applied to supervised learning problems withÂ sequentialÂ orÂ structuredÂ outputs.RL algorithms have started to achieve good results in many difficult environments.Â RL has a long history, but until recent advances in deep learning, it required lots of problem-specific engineering. DeepMindâsÂ Atari results,Â BRETTÂ fromÂ Pieter AbbeelâsÂ group, andÂ AlphaGoÂ all used deep RL algorithms which did not make too many assumptions about their environment, and thus can be applied in otherÂ settings.However, RL research is also slowed down by twoÂ factors:The need for better benchmarks.Â In supervised learning, progress has been driven by large labeled datasets likeÂ ImageNet. In RL, the closest equivalent would be a large and diverse collection of environments. However, the existing open-source collections of RL environments donât have enough variety, and they are often difficult to even set up andÂ use.Lack of standardization of environments used in publications.Â Subtle differences in the problem definition, such as the reward function or the set of actions, can drastically alter a taskâs difficulty. This issue makes it difficult to reproduce published research and compare results from differentÂ papers.OpenAI Gym is an attempt to fix bothÂ problems.The environmentsOpenAI Gym provides a diverse suite of environments that range from easy to difficult and involve many different kinds of data. Weâre starting out with the followingÂ collections:Classic controlÂ andÂ toy text: complete small-scale tasks, mostly from the RL literature. Theyâre here to get youÂ started.Algorithmic: perform computations such as adding multi-digit numbers and reversing sequences. One might object that these tasks are easy for a computer. The challenge is to learn these algorithms purely from examples. These tasks have the nice property that itâs easy to vary the difficulty by varying the sequenceÂ length.Atari: play classic Atari games. Weâve integrated theÂ Arcade Learning EnvironmentÂ (which has had a big impact on reinforcement learning research) in anÂ easy-to-installÂ form.Board games: play Go on 9x9 and 19x19 boards. Two-player games are fundamentally different than the other settings weâve included, because there is an adversary playing against you. In our initial release, there is a fixed opponent provided byÂ Pachi, and we may add other opponents later (patches welcome!). Weâll also likely expand OpenAI Gym to have first-class support for multi-playerÂ games.2D and 3D robots: control a robot in simulation. These tasks use theÂ MuJoCoÂ physics engine, which was designed for fast and accurate robot simulation. Included are some environments from a recentÂ benchmarkÂ by UC Berkeley researchers (who incidentally will beÂ joining usÂ this summer). MuJoCo is proprietary software, but offersÂ free trialÂ licenses.Over time, we plan to greatly expand this collection of environments. Contributions from the community are more thanÂ welcome.Each environment has a version number (such asÂ Hopper-v0). If we need to change an environment, weâll bump the version number, defining an entirely new task. This ensures that results on a particular environment are alwaysÂ comparable.EvaluationsWeâve made it easy toÂ upload resultsÂ to OpenAI Gym. However, weâve opted not to create traditional leaderboards. What matters for research isnât your score (itâs possible to overfit or hand-craft solutions to particular tasks), but instead the generality of yourÂ technique.Weâre starting out by maintaining aÂ curated listÂ of contributions that say something interesting about algorithmic capabilities. Long-term, we want this curation to be a community effort rather than something owned by us. Weâll necessarily have to figure out the details over time, and weâd would love yourÂ helpÂ in doingÂ so.We want OpenAI Gym to be a community effort from the beginning. Weâve starting working with partners to put together resources around OpenAIÂ Gym:NVIDIA: technicalÂ Q&AÂ withÂ John.Nervana: implementation of aÂ DQN OpenAI GymÂ agent.Amazon Web Services (AWS):Â $250 credit vouchers for select OpenAI Gym users. If you have an evaluation demonstrating the promise of your algorithm and are resource-constrained from scaling it up,Â ping usÂ for a voucher. (While suppliesÂ last!)During the public beta, weâre looking for feedback on how to make this into an even better tool for research. If youâd like to help, you can try your hand at improving the state-of-the-art on each environment, reproducing other peopleâs results, or even implementing your own environments. Also please join us in theÂ communityÂ chat!AuthorsAuthorsGreg BrockmanResearchOverviewIndexProductOverviewChatGPTGPT-4DALLÂ·E 2Customer storiesSafety standardsAPI data privacyPricingSafetyOverviewCompanyAboutBlogCareersCharterSecurityOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top
