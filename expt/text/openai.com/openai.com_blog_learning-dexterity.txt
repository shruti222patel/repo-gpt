


Learning dexterity












CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexProductOverviewChatGPTGPT-4DALLÂ·E 2Customer storiesSafety standardsAPI data privacyPricingDevelopersOverviewDocumentationAPI referenceExamplesSafetyCompanyAboutBlogCareersCharterSecuritySearch Navigation quick links Log inSign upMenu Mobile Navigation CloseSite NavigationResearchProductDevelopersSafetyCompany Quick Links Log inSign upSearch Submit Illustration: Ben Barry & Eric HainesLearning dexterityWeâve trained a human-like robot hand to manipulate physical objects with unprecedentedÂ dexterity.July 30, 2018More resourcesRead paperRobotics,Â Domain randomization,Â Meta-learning,Â Sim-to-real,Â Transfer learning,Â Dactyl,Â MilestoneLearning dexterity3:29Our system, called Dactyl, is trained entirely in simulation and transfers its knowledge to reality, adapting to real-world physics using techniques weâve been working on for theÂ pastÂ year. Dactyl learns from scratch using the same general-purpose reinforcement learning algorithm and code asÂ OpenAI Five. OurÂ resultsÂ show that itâs possible to train agents in simulation and have them solve real-world tasks, without physically-accurate modeling of theÂ world.Examples of dexterous manipulation behaviors autonomously learned by Dactyl.The taskDactyl is a system for manipulating objects using aÂ Shadow Dexterous Hand. We place an object such as a block or a prism in the palm of the hand and ask Dactyl to reposition it into a different orientation; for example, rotating the block to put a new face on top. The network observes only the coordinates of the fingertips and the images from three regular RGBÂ cameras.Although the first humanoid hands were developed decades ago, using them to manipulate objects effectively has been a long-standing challenge in robotic control. Unlike other problems such asÂ locomotion, progress on dextrous manipulation using traditional robotics approaches has been slow, andÂ current techniquesÂ remain limited in their ability to manipulate objects in the realÂ world.Reorienting an object in the hand requires the following problems to beÂ solved:Working in the real world.Â Reinforcement learning has shown many successes in simulations and video games, but has had comparatively limited results in the real world. We test Dactyl on a physicalÂ robot.High-dimensional control.Â The Shadow Dexterous Hand has 24 degrees of freedom compared to 7 for a typical robotÂ arm.Noisy and partial observations.Â Dactyl works in the physical world and therefore must handle noisy and delayed sensor readings. When a fingertip sensor is occluded by other fingers or by the object, Dactyl has to work with partial information. Many aspects of the physical system like friction and slippage are not directly observable and must beÂ inferred.Manipulating more than one object.Â Dactyl is designed to be flexible enough to reorient multiple kinds of objects. This means that our approach cannot use strategies that are only applicable to a specific objectÂ geometry.Our approachDactyl learns to solve the object reorientation task entirely in simulation without any human input. After this training phase, the learned policy works on the real robot without anyÂ fine-tuning.Learning dexterity: uncut7:15Learning methods for robotic manipulation face a dilemma. Simulated robots can easily provide enough data to train complex policies, but most manipulation problems canât be modeled accurately enough for those policies to transfer to real robots. Even modeling what happens when two objects touchâthe most basic problem in manipulationâis anÂ active area of researchÂ with no widely accepted solution. Training directly on physical robots allows the policy to learn from real-world physics, but todayâs algorithms would require years of experience to solve a problem like objectÂ reorientation.Our approach,Â domain randomization, learns in a simulation which is designed to provide a variety of experiences rather than maximizing realism. This gives us the best of both approaches: by learning in simulation, we can gather more experience quickly by scaling up, and by de-emphasizing realism, we can tackle problems that simulators can only modelÂ approximately.Itâs been shown (byÂ OpenAIÂ andÂ others) that domain randomization can work on increasingly complex problemsâdomain randomizations were evenÂ used to train OpenAI Five. Here, we wanted to see if scaling up domain randomization could solve a task well beyond the reach of current methods inÂ robotics.We built aÂ simulated versionÂ of our robotics setup using theÂ MuJoCoÂ physics engine. This simulation is only a coarse approximation of the realÂ robot:Measuring physical attributes like friction, damping, and rolling resistance is cumbersome and difficult. They also change over time as the robot experiences wear andÂ tear.MuJoCo is aÂ rigid bodyÂ simulator, which means that it cannot simulate the deformable rubber found at the fingertips of the hand or the stretching ofÂ tendons.Our robot can only manipulate the object by repeatedly making contact with it. However, contact forces are notoriously difficult to reproduce accurately inÂ simulation.The simulation can be made more realistic by calibrating its parameters to match robot behavior, but many of these effects simply cannot be modeled accurately in currentÂ simulators.Instead, we train the policy on a distribution of simulated environments where the physical and visual attributes are chosen randomly. Randomized values are a natural way to represent the uncertainties that we have about the physical system and also prevent overfitting to a single simulated environment. If a policy can accomplish the task across all of the simulated environments, it will more likely be able to accomplish it in the realÂ world.Learning to controlBy building simulations that support transfer, we have reduced the problem of controlling a robot in the real world to accomplishing a task in simulation, which is a problem well-suited for reinforcement learning. While the task of manipulating an object in a simulated hand is alreadyÂ somewhat difficult, learning to do so across all combinations of randomized physical parameters is substantially moreÂ difficult.To generalize across environments, it is helpful for the policy to be able to take different actions in environments with different dynamics. Because most dynamics parameters cannot be inferred from a single observation, we used anÂ LSTMâa type of neural network with memoryâto make it possible for the network to learn about the dynamics of the environment. The LSTM achieved about twice as many rotations in simulation as a policy withoutÂ memory.Dactyl learns usingÂ Rapid, the massively scaled implementation of Proximal Policy Optimization developed to allow OpenAI Five to solve Dota 2. We use a different model architecture, environment, and hyperparameters than OpenAI Five does, but we use the same algorithms and training code. Rapid used 6144 CPU cores and 8 GPUs to train our policy, collecting about one hundred years of experience in 50Â hours.For development and testing, we validated our control policy against objects with embedded motion tracking sensors to isolate the performance of our control and visionÂ networks.Learning to seeDactyl was designed to be able to manipulate arbitrary objects, not just those that have been specially modified to support tracking. Therefore, Dactyl uses regular RGB camera images to estimate the position and orientation of theÂ object.We train a pose estimator using a convolutional neural network. The neural network takes the video streams from three cameras positioned around the robot hand and outputs the estimated position and orientation of the object. We use multiple cameras to resolve ambiguities and occlusion. We again use domain randomization to train this network only in simulation using theÂ UnityÂ game development platform, which can model a wider variety of visual phenomena thanÂ Mujoco.By combining these two independent networks, the control network that reorients the object given its pose and the vision network that maps images from cameras to the objectâs pose, Dactyl can manipulate an object by seeingÂ it.Example training images used for learning to estimate the pose of the block.ResultsWhen deploying our system, we noticed that Dactyl uses a rich set ofÂ in-hand dexterous manipulation strategiesÂ to solve the task. These strategies are commonly used by humans as well. However, we do not teach them to our system explicitly; all behaviors are discoveredÂ autonomously.Examples of dexterous manipulation behaviors autonomously learned by Dactyl.Dactyl grasp types according to the GRASP taxonomy. Top left to bottom right: Tip Pinch, Palmar Pinch, Tripod, Quadpod, Power grasp, and 5-Finger Precision grasp.We observed that for precision grasps, such as the Tip Pinch grasp, Dactyl uses the thumb and little finger. Humans tend to use the thumb and either the index or middle finger instead. However, the robot handâs little finger is more flexible due to anÂ extra degree of freedom, which may explain why Dactyl prefers it. This means that Dactyl can rediscover grasps found in humans, but adapt them to better fit the limitations and abilities of its ownÂ body.Transfer performanceWe tested how many rotations Dactyl could achieve before it dropped the object, timed out, or reached 50 successes. Our policies trained purely in simulation were able to successfully manipulate objects in the realÂ world.Dactyl lab setup with Shadow Dexterous Hand, PhaseSpace motion tracking cameras, and Basler RGB cameras.For the task of block manipulation, policies trained with randomization could achieve many more rotations than those trained without randomization, as can be seen in the results below. Also, using the control network with pose estimated from vision performs nearly as well as reading the pose directly from motion trackingÂ sensors.RandomizationsObject trackingMax number of successesMedian number of successesAll randomizationsVision4611.5All randomizationsMotion tracking5013No randomizationsMotion tracking60Learning progressThe vast majority of training time is spent making the policy robust to different physical dynamics. Learning to rotate an object in simulation without randomizations requires about 3 years of simulated experience, while achieving similar performance in a fully randomized simulation requires about 100 years ofÂ experience.Learning progress with and without randomizations over years of simulated experience.What surprised usTactile sensing is not necessary to manipulate real-world objects.Â Our robot receives only the locations of the five fingertips along with the position and orientation of the cube. Although the robot hand has touch sensors on its fingertips, we didnât need to use them. Generally, we found better performance from using a limited set of sensors that could be modeled effectively in the simulator instead of a rich sensor set with values that were hard toÂ model.Randomizations developed for one object generalize to others with similar properties.Â After developing our system for the problem of manipulating a block, we printed an octagonal prism, trained a new policy using its shape, and attempted to manipulate it. Somewhat to our surprise, it achieved high performance using only the randomizations we had designed for the block. By contrast, a policy that manipulated a sphere could only achieve a few successes in a row, perhaps because we had not randomized any simulation parameters that model rollingÂ behavior.With physical robots, good systems engineering is as important as good algorithms.Â At one point, we noticed that one engineer consistently achieved much better performance than others when running the exact same policy. We later discovered that he had a faster laptop, which hid a timing bug that reduced performance. After the bug was fixed, performance improved for the rest of theÂ team.What didnât pan outWe also found to our surprise that a number of commonly employed techniques did not improve ourÂ results.Decreasing reaction time did not improve performance.Â Conventional wisdom states that reducing the time between actions should improve performance because the changes between states are smaller and therefore easier to predict. Our current time between actions is 80ms, which is smaller than human reaction time of 150-250ms, but significantly larger than neural network computation time of roughly 25ms. Surprisingly, decreasing time between actions to 40ms required additional training time but did not noticeably improve performance in the real world. Itâs possible that this rule of thumb is less applicable to neural network models than to the linear models that are in common useÂ today.Using real data to train our vision policies didnât make a difference.Â In early experiments, we used a combination of simulated and real data to improve our models. The real data was gathered from trials of our policy against an object with embedded tracking markers. However, real data has significant disadvantages compared to simulated data. Position information from tracking markers has latency and measurement error. Worse, real data is easily invalidated by common configuration changes, making it a hassle to collect enough to be useful. As our methods developed, our simulator-only error improved until it matched our error from using a mixture of simulated and real data. Our final vision models were trained without realÂ data.This project completes a full cycle of AI development that OpenAI has been pursuing for the past two years: weâve developedÂ a new learning algorithm, scaled it massively to solveÂ hard simulated tasks, and then applied the resulting system to the real world. Repeating this cycle atÂ increasing scaleÂ is the primary route we are pursuing to increase the capabilities of todayâs AI systems towards safe artificial general intelligence. If youâd like to be part of what comes next,Â weâreÂ hiring!AuthorsAuthorsJosh TobinBob McGrewWojciech ZarembaMaciek ChociejSzymon SidorGlenn PowellJakub PachockiAlex RayMarcin AndrychowiczBowen BakerArthur PetronMatthias PlappertRafaÅ JÃ³zefowiczJonas SchneiderPeter WelinderLilian WengContributorsLarissa SchiavoJack ClarkGreg BrockmanIlya SutskeverDiane YoonBen BarryAcknowledgmentsFeedbackThanks to the following for feedback on drafts of this post: Pieter Abbeel, Tamim Asfour, Marek Cygan, Ken Goldberg, Anna Goldie, Edward Mehr, Azalia Mirhoseini, Lerrel Pinto, Aditya Ramesh & Ian Rust.Cover ArtBen Barry & Eric HainesResearchOverviewIndexProductOverviewChatGPTGPT-4DALLÂ·E 2Customer storiesSafety standardsAPI data privacyPricingSafetyOverviewCompanyAboutBlogCareersCharterSecurityOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top
