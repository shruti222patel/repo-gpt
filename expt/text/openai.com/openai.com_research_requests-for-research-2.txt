


Requests for Research 2.0












CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexProductOverviewChatGPTGPT-4DALLÂ·E 2Customer storiesSafety standardsAPI data privacyPricingDevelopersOverviewDocumentationAPI referenceExamplesSafetyCompanyAboutBlogCareersCharterSecuritySearch Navigation quick links Log inSign upMenu Mobile Navigation CloseSite NavigationResearchProductDevelopersSafetyCompany Quick Links Log inSign upSearch Submit Requests for Research 2.0Weâre releasing a new batch ofÂ seven unsolved problemsÂ which have come up in the course of our research at OpenAI.January 31, 2018CommunityLike our originalÂ Requests for Research (whichÂ resultedÂ inÂ severalÂ papers), we expect these problems to be a fun and meaningful way for new people to enter the field, as well as for practitioners to hone their skills (itâs also a great way to get aÂ jobÂ at OpenAI). Many will require inventing new ideas. PleaseÂ emailÂ us with questions or solutions youâd like us toÂ publicize!(Also, if you donât have deep learning background but want to learn to solve problems like these, please apply for ourÂ FellowshipÂ program!)WarmupsIf youâre not sure where to begin, here are some solved starterÂ problems.â­ Train an LSTM to solve theÂ XORÂ problem: that is, given a sequence of bits, determine its parity. TheÂ LSTMÂ should consume the sequence, one bit at a time, and then output the correct answer at the sequenceâs end. Test the two approachesÂ below:Generate a dataset of random 100,000 binary strings of length 50. Train the LSTM; what performance do youÂ get?Generate a dataset of random 100,000 binary strings, where the length of each string is independently and randomly chosen between 1 and 50. Train the LSTM. Does it succeed? What explains theÂ difference?â­ Implement a clone of the classicÂ SnakeÂ game as aÂ GymÂ environment, and solve it with aÂ reinforcement learningÂ algorithm of your choice.Â TweetÂ us videos of the agent playing. Were you able to train a policy that wins theÂ game?Requests for Researchâ­â­Â Slitherinâ.Â Implement and solve a multiplayer clone of the classicÂ SnakeÂ game (seeÂ slither.ioÂ for inspiration) as aÂ GymÂ environment.Environment: have a reasonably large field with multiple snakes; snakes grow when eating randomly-appearing fruit; a snake dies when colliding with another snake, itself, or the wall; and the game ends when all snakes die. Start with two snakes, and scale fromÂ there.Agent: solve the environment using self-play with an RL algorithm ofÂ yourÂ choice. Youâll need to experiment with various approaches to overcome self-play instability (which resembles the instability people see with GANs). For example, try training your current policy against a distribution of past policies. Which approach worksÂ best?Inspect the learned behavior: does the agent learn to competently pursue food and avoid other snakes? Does the agent learn to attack, trap, or gang up against the competing snakes? Tweet us videos of the learnedÂ policies!â­â­â­Â Parameter Averaging in Distributed RL.Â Explore the effect of parameter averaging schemes onÂ sample complexityÂ and amount of communication in RL algorithms. While the simplest solution is to average the gradients from every worker on every update, you canÂ saveÂ on communication bandwidth by independently updating workers and then infrequently averaging parameters. In RL, this may have another benefit: at any given time weâll have agents with different parameters, which could lead to better exploration behavior. Another possibility is use algorithms likeÂ EASGDÂ that bring parameters partly together eachÂ update.â­â­â­Â Transfer Learning Between Different Games via Generative Models.Â Proceed asÂ follows:Train 11 good policies for 11Â AtariÂ games. Generate 10,000 trajectories of 1,000 steps each from the policy for eachÂ game.Fit a generative model (such as theÂ Transformer) to the trajectories produced by 10 of theÂ games.Then fine-tune that model on the 11thÂ game.Your goal is to quantify the benefit from pre-training on the 10 games. How large does the model need to be for the pre-training to be useful? How does the size of the effect change when the amount of data from the 11th game is reduced by 10x? ByÂ 100x?â­â­â­Â Transformers with Linear Attention.Â TheÂ TransformerÂ model uses soft attention with softmax. If we could instead use linear attention (which can be converted into an RNN that usesÂ fast weights), we could use the resulting model for RL. Specifically, an RL rollout with a transformer over a huge context would be impractical, but running an RNN with fast weights would be very feasible. Your goal: take any language modeling task; train a transformer; then find a way to get the same bits per character/word using a linear-attention transformer with different hyperparameters, without increasing the total number of parameters by much. Only one caveat: this may turn out to be impossible. But one potentially helpful hint: it is likely that transformers with linear attention require much higher dimensional key/value vectors compared to attention that uses the softmax, which can be done without significantly increasing the number ofÂ parameters.â­â­â­Â Learned Data Augmentation.Â You could use a learnedÂ VAEÂ of data, to perform âlearned data augmentationâ. One would first train a VAE on input data, then each training point would be transformed by encoding to a latent space, then applying a simple (e.g. Gaussian) perturbation in latent space, then decoding back to observed space. Could we use such an approach to obtain improved generalization? A potential benefit of such data augmentation is that it could include many nonlinear transformations like viewpoint changes and changes in scene lightning. Can we approximate the set of transformations to which the label is invariant? Check out theÂ existingÂ workÂ onÂ thisÂ topicÂ ifÂ youÂ wantÂ a place to getÂ started.â­â­â­â­Â Regularization in Reinforcement Learning.Â Experimentally investigate (and qualitatively explain) the effect of different regularization methods on an RL algorithm of choice. In supervised deep learning, regularization is extremely important forÂ improving optimizationÂ and for preventing overfitting, with very successful methods likeÂ dropout,Â batch normalization, andÂ L2 regularization. However, people havenât benefited from regularization with reinforcement learning algorithms such asÂ policy gradientsÂ andÂ Q-learning. Incidentally, people generally use much smaller models in RL than in supervised learning, as large models perform worse â perhaps because they overfit to recent experience. To get started,Â hereÂ is a relevant but older theoreticalÂ study.â­â­â­â­â­Â Automated Solutions of Olympiad Inequality Problems.Â Olympiad inequality problems are simple to express, butÂ solvingÂ them often requires clever manipulations. Build a dataset of olympiad inequality problems and write a program that can solve a large fraction of them. Itâs not clear whether machine learning will be useful here, but you could potentially use a learned policy to reduce the branchingÂ factor.Want to work on problems like these professionally?Â ApplyÂ toÂ OpenAI!AuthorsIlya SutskeverTim SalimansDurk KingmaResearchOverviewIndexProductOverviewChatGPTGPT-4DALLÂ·E 2Customer storiesSafety standardsAPI data privacyPricingSafetyOverviewCompanyAboutBlogCareersCharterSecurityOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top
