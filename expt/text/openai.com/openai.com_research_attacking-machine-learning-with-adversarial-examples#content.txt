


Attacking machine learning with adversarial examples












CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexProductOverviewChatGPTGPT-4DALLÂ·E 2Customer storiesSafety standardsAPI data privacyPricingDevelopersOverviewDocumentationAPI referenceExamplesSafetyCompanyAboutBlogCareersCharterSecuritySearch Navigation quick links Log inSign upMenu Mobile Navigation CloseSite NavigationResearchProductDevelopersSafetyCompany Quick Links Log inSign upSearch Submit Attacking machine learning with adversarial examplesAdversarial examples are inputs to machine learning models that an attacker has intentionally designed to cause the model to make a mistake; theyâre like optical illusions for machines. In this post weâll show how adversarial examples work across different mediums, and will discuss why securing systems against them can be difficult.February 24, 2017Adversarial examples,Â Safety & Alignment,Â Computer vision,Â Robustness,Â ConclusionAt OpenAI, we think adversarial examples are a good aspect of security to work on because they represent a concrete problem in AI safety that can be addressed in the short term, and because fixing them is difficult enough that it requires a serious research effort. (Though weâll need to explore many aspects of machine learning security to achieve ourÂ goal of building safe, widely distributedÂ AI.)To get an idea of what adversarial examples look like, consider this demonstration fromÂ Explaining and Harnessing Adversarial Examples: starting with an image of a panda, the attacker adds a small perturbation that has been calculated to make the image be recognized as a gibbon with highÂ confidence.The approach is quite robust;Â recent researchÂ has shown adversarial examples can be printed out on standard paper then photographed with a standard smartphone, and still foolÂ systems.Adversarial examples can be printed out on normal paper and photographed with a standard resolution smartphone and still cause a classifier to, in this case, label a âwasherâ as a âsafeâ.Adversarial examples have the potential to be dangerous. For example, attackers could target autonomous vehicles by using stickers or paint to create an adversarial stop sign that the vehicle would interpret as a âyieldâ or other sign, as discussed inÂ Practical Black-Box Attacks against Deep Learning Systems using AdversarialÂ Examples.Reinforcement learning agents can also be manipulated by adversarial examples, according to new research from UC Berkeley, OpenAI, and Pennsylvania State University,Â Adversarial Attacks on Neural Network Policies, and research from the University of Nevada at Reno,Â Vulnerability of Deep Reinforcement Learning to Policy Induction Attacks. The research shows that widely-used RL algorithms, such asÂ DQN,Â TRPO, andÂ A3C, are vulnerable to adversarial inputs. These can lead to degraded performance even in the presence of pertubations too subtle to be percieved by a human, causing an agent to moveÂ a pong paddle down when it should go up, or interfering with its ability to spot enemies inÂ Seaquest.Adversarial Attacks: Seaquest, A3C, L2-Norm05:40If you want to experiment with breaking your own models, you can useÂ cleverhans, an open source library developed jointly byÂ Ian GoodfellowÂ andÂ Nicolas PapernotÂ to test your AIâs vulnerabilities to adversarialÂ examples.Adversarial examples give us some traction on AI safetyWhen we think about the study of AI safety, we usually think about some of the most difficult problems in that field â how can we ensure that sophisticated reinforcement learning agents that are significantly more intelligent than human beings behave in ways that their designersÂ intended?Adversarial examples show us that even simple modern algorithms, for both supervised and reinforcement learning, can already behave in surprising ways that we do notÂ intend.Attempted defenses against adversarial examplesTraditional techniques for making machine learning models more robust, such as weight decay and dropout, generally do not provide a practical defense against adversarial examples. So far, only two methods have provided a significantÂ defense.Adversarial training: This is a brute force solution where we simply generate a lot of adversarial examples and explicitly train the model not to be fooled by each of them. An open-source implementation of adversarial training is available in theÂ cleverhansÂ library and its use illustrated in the followingÂ tutorial.Defensive distillation: This is a strategy where we train the model to output probabilities of different classes, rather than hard decisions about which class to output. The probabilities are supplied by an earlier model, trained on the same task using hard class labels. This creates a model whose surface is smoothed in the directions an adversary will typically try to exploit, making it difficult for them to discover adversarial input tweaks that lead to incorrect categorization. (Distillation was originally introduced inÂ Distilling the Knowledge in a Neural NetworkÂ as a technique for model compression, where a small model is trained to imitate a large one, in order to obtain computationalÂ savings.)Yet even these specialized algorithms can easily be broken by giving more computational firepower to theÂ attacker.A failed defense: âgradient maskingâTo give an example of how a simple defense can fail, letâs consider why a technique called âgradient maskingâ does notÂ work.Gradient maskingâ is a term introduced inÂ Practical Black-Box Attacks against Deep Learning Systems using Adversarial Examples. to describe an entire category of failed defense methods that work by trying to deny the attacker access to a usefulÂ gradient.Most adversarial example construction techniques use the gradient of the model to make an attack. In other words, they look at a picture of an airplane, they test which direction in picture space makes the probability of the âcatâ class increase, and then they give a little push (in other words, they perturb the input) in that direction. The new, modified image is mis-recognized as aÂ cat.But what if there were no gradient â what if an infinitesimal modification to the image caused no change in the output of the model? This seems to provide some defense because the attacker does not know which way to âpushâ theÂ image.We can easily imagine some very trivial ways to get rid of the gradient. For example, most image classification models can be run in two modes: one mode where they output just the identity of the most likely class, and one mode where they output probabilities. If the modelâs output is â99.9% airplane, 0.1% catâ, then a little tiny change to the input gives a little tiny change to the output, and the gradient tells us which changes will increase the probability of the âcatâ class. If we run the model in a mode where the output is just âairplaneâ, then a little tiny change to the input will not change the output at all, and the gradient does not tell usÂ anything.Letâs run a thought experiment to see how well we could defend our model against adversarial examples by running it in âmost likely classâ mode instead of âprobability mode.â The attacker no longer knows where to go to find inputs that will be classified as cats, so we might have some defense. Unfortunately, every image that was classified as a cat before is still classified as a cat now. If the attacker can guess which points are adversarial examples, those points will still be misclassified. We havenât made the model more robust; we have just given the attacker fewer clues to figure out where the holes in the models defenseÂ are.Even more unfortunately, it turns out that the attacker has a very good strategy for guessing where the holes in the defense are. The attacker can train their own model, a smooth model that has a gradient, make adversarial examples for their model, and then deploy those adversarial examples against our non-smooth model. Very often, our model will misclassify these examples too. In the end, our thought experiment reveals that hiding the gradient didnât get usÂ anywhere.The defense strategies that perform gradient masking typically result in a model that is very smooth in specific directions and neighborhoods of training points, which makes it harder for the adversary to find gradients indicating good candidate directions to perturb the input in a damaging way for the model. However, the adversary can train aÂ substituteÂ model: a copy that imitates the defended model by observing the labels that the defended model assigns to inputs chosen carefully by theÂ adversary.A procedure for performing such a model extraction attack was introduced in the black-box attacks paper. The adversary can then use the substitute modelâs gradients to find adversarial examples that are misclassified by the defended model as well. In the figure above, reproduced from the discussion of gradient masking found inÂ Towards the Science of Security and Privacy in Machine Learning, we illustrate this attack strategy with a one-dimensional ML problem. The gradient masking phenomenon would be exacerbated for higher dimensionality problems, but harder toÂ depict.We find that both adversarial training and defensive distillation accidentally perform a kind of gradient masking. Neither algorithm was explicitly designed to perform gradient masking, but gradient masking is apparently a defense that machine learning algorithms can invent relatively easily when they are trained to defend themselves and not given specific instructions about how to do so. If we transfer adversarial examples from one model to a second model that was trained with either adversarial training or defensive distillation, the attack often succeeds, even when a direct attack on the second model would fail. This suggests that both training techniques do more to flatten out the model and remove the gradient than to make sure it classifies more pointsÂ correctly.Why is it hard to defend against adversarial examples?Adversarial examples are hard to defend against because it is difficult to construct a theoretical model of the adversarial example crafting process. Adversarial examples are solutions to an optimization problem that is non-linear and non-convex for many ML models, including neural networks. Because we donât have good theoretical tools for describing the solutions to these complicated optimization problems, it is very hard to make any kind of theoretical argument that a defense will rule out a set of adversarialÂ examples.Adversarial examples are also hard to defend against because they require machine learning models to produce good outputsÂ for every possible input. Most of the time, machine learning models work very well but only work on a very small amount of all the many possible inputs they mightÂ encounter.Every strategy we have tested so far fails because it is notÂ adaptive: it may block one kind of attack, but it leaves another vulnerability open to an attacker who knows about the defense being used. Designing a defense that can protect against a powerful, adaptive attacker is an important researchÂ area.ConclusionAdversarial examples show that many modern machine learning algorithms can be broken in surprising ways. These failures of machine learning demonstrate that even simple algorithms can behave very differently from what their designers intend. We encourage machine learning researchers to get involved and design methods for preventing adversarial examples, in order to close this gap between what designers intend and how algorithms behave. If youâre interested in working on adversarial examples, considerÂ joiningÂ OpenAI.For more informationTo learn more about machine learning security, follow Ian and Nicolasâs machine learning security blogÂ cleverhans.io.AuthorsIan GoodfellowNicolas PapernotSandy HuangYan DuanPieter AbbeelJack ClarkResearchOverviewIndexProductOverviewChatGPTGPT-4DALLÂ·E 2Customer storiesSafety standardsAPI data privacyPricingSafetyOverviewCompanyAboutBlogCareersCharterSecurityOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top
