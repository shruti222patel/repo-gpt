


OpenAI Baselines: DQN












CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexProductOverviewChatGPTGPT-4DALLÂ·E 2Customer storiesSafety standardsAPI data privacyPricingDevelopersOverviewDocumentationAPI referenceExamplesSafetyCompanyAboutBlogCareersCharterSecuritySearch Navigation quick links Log inSign upMenu Mobile Navigation CloseSite NavigationResearchProductDevelopersSafetyCompany Quick Links Log inSign upSearch Submit Illustration: Ben BarryOpenAI Baselines: DQNWeâre open-sourcing OpenAI Baselines, our internal effort to reproduce reinforcement learning algorithms with performance on par with published results. Weâll release the algorithms over upcoming months; todayâs release includes DQN and three of its variants.May 24, 2017More resourcesView codeReinforcement learning,Â Games,Â Software engineering,Â Open source,Â Milestone,Â ReleaseReinforcement learning results are tricky to reproduce: performance is very noisy, algorithms have many moving parts which allow for subtle bugs, and many papers donât report all the required tricks. By releasing known-good implementations (and best practices for creating them), weâd like to ensure that apparent RL advances never are due to comparison with buggy or untuned versions of existing algorithms.This post contains some best practices we use for correct RL algorithm implementations, as well as the details of our first release:Â DQNÂ and three of its variants, algorithms developed byÂ DeepMind.Best practicesCompare to a random baseline:Â in the video below, an agent is taking random actions in the game H.E.R.O. If you saw this behavior in early stages of training, itâd be really easy to trick yourself into believing that the agent is learning. So you should always verify your agent outperforms a randomÂ one.Be wary of non-breaking bugs: when we looked through a sample of ten popular reinforcement learning algorithm reimplementations we noticed that six had subtle bugs found by a community member and confirmed by the author. These ranged from mild bugs thatÂ ignored gradients on some examplesÂ or implementedÂ causal convolutions incorrectlyÂ to serious ones that reportedÂ scores higher than the trueÂ result.See the world as your agent does:Â like most deep learning approaches, for DQN we tend to convert images of our environments to grayscale to reduce the computation required during training. This can create its own bugs: when we ran our DQN algorithm on Seaquest we noticed that our implementation was performing poorly. When we inspected the environment we discovered this was because our post-processed images contained no fish, as this pictureÂ shows.When transforming the screen images into greyscale we had incorrectly calibrated our coefficients for the green color values, which led to the fish disappearing. After we noticed the bug we tweaked the color values and our algorithm was able to see the fishÂ again.To debug issues like this in the future, Gym now contains aÂ playÂ function, which lets a researcher easily see the same observations as the AI agentÂ would.Fix bugs, then hyperparameters: After debugging, we started to calibrate our hyperparameters. We ultimately found that setting the annealing schedule for epsilon, a hyperparameter which controlled the exploration rate, had a huge impact on performance. Our final implementation decreases epsilon to 0.1 over the first million steps and then down to 0.01 over the next 24 million steps. If our implementation contained bugs, then itâs likely we would come up with different hyperparameter settings to try to deal with faults we hadnât yetÂ diagnosed.Double check your interpretations of papers: In the DQNÂ NatureÂ paper the authors write: âWe also found it helpful to clip the error term from the update [...] to be between -1 and 1.â. There are two ways to interpret this statement â clip the objective, or clip the multiplicative term when computing gradient. The former seems more natural, but it causes the gradient to be zero on transitions with high error, which leads to suboptimal performance, as found in oneÂ DQN implementation. The latter is correct and has a simple mathematical interpretation âÂ Huber Loss. You can spot bugs like these by checking that the gradients appear as you expect â this can be easily done within TensorFlow by usingÂ compute_gradients.The majority of bugs in this post were spotted by going over the code multiple times and thinking through what could go wrong with each line. Each bug seems obvious in hindsight, but even experienced researchers tend to underestimate how many passes over the code it can take to find all the bugs in anÂ implementation.Deep Q-LearningDQN: A reinforcement learning algorithm that combines Q-Learning with deep neural networks to let RL work for complex, high-dimensional environments, like video games, orÂ robotics.Double Q Learning: Corrects the stock DQN algorithmâs tendency to sometimes overestimate the values tied to specificÂ actions.Prioritized Replay: Extends DQNâs experience replay function by learning to replay memories where the real reward significantly diverges from the expected reward, letting the agent adjust itself in response to developing incorrectÂ assumptions.Dueling DQN: Splits the neural network into two â one learns to provide an estimate of the value at every timestep, and the other calculates potential advantages of each action, and the two are combined for a single action-advantage QÂ function.To get started, run theÂ following:pip install baselines
# Train model and save the results to cartpole_model.pkl
python -m baselines.deepq.experiments.train_cartpole
# Load the model saved in cartpole_model.pkl and visualize the learned policy
python -m baselines.deepq.experiments.enjoy_cartpolenullWeâve also provided trained agents, which you can obtain byÂ running:python -m baselines.deepq.experiments.atari.download_model --blob model-atari-prior-duel-breakout-1 --model-dir /tmp/models
python -m baselines.deepq.experiments.atari.enjoy --model-dir /tmp/models/model-atari-prior-duel-breakout-1 --env Breakout --duelingnullBenchmarksWeâve included anÂ iPython notebookÂ showing the performance of our DQN implementations on Atari games. You can compare the performance of our various algorithms such as Dueling Double Q learning with Prioritized Replay (yellow), Double Q learning with Prioritized Replay (blue), Dueling Double Q learning (green) and Double Q learningÂ (red).AI is an empirical science, where the ability to do more experiments directly correlates with progress. With Baselines, researchers can spend less time implementing pre-existing algorithms and more time designing new ones. If youâd like to help us refine, extend, and develop AI algorithms thenÂ join usÂ atÂ OpenAI.AuthorsSzymon SidorJohn SchulmanResearchOverviewIndexProductOverviewChatGPTGPT-4DALLÂ·E 2Customer storiesSafety standardsAPI data privacyPricingSafetyOverviewCompanyAboutBlogCareersCharterSecurityOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top
