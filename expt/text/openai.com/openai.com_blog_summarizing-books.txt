


Summarizing books with human feedback












CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexProductOverviewChatGPTGPT-4DALLÂ·E 2Customer storiesSafety standardsAPI data privacyPricingDevelopersOverviewDocumentationAPI referenceExamplesSafetyCompanyAboutBlogCareersCharterSecuritySearch Navigation quick links Log inSign upMenu Mobile Navigation CloseSite NavigationResearchProductDevelopersSafetyCompany Quick Links Log inSign upSearch Submit Summarizing books with human feedbackScaling human oversight of AI systems for tasks that are difficult toÂ evaluate.September 23, 2021Language,Â Human feedback,Â Safety & Alignment,Â Summarization,Â GPT-3,Â Milestone,Â PublicationTo safelyÂ deploy powerful, general-purpose artificial intelligence in the future, we need to ensure that machine learning models act in accordance with human intentions. This challenge has become known as theÂ alignmentÂ problem.A scalable solution to the alignment problem needs to work on tasks where model outputs are difficult or time-consuming for humans to evaluate. To test scalable alignment techniques, we trained a model to summarize entire books, as shown in the following samples.[^footnote-1] Our model works by first summarizing small sections of a book, then summarizing those summaries into a higher-level summary, and soÂ on.Loading samplesâ¦Our best model is fine-tuned from GPT-3 and generates sensible summaries of entire books, sometimes even matching the average quality of human-written summaries: it achieves a 6/7 ratingÂ (similar to the average human-written summary) from humans who have read the book 5% of the time and a 5/7 rating 15% of the time. Our model also achieves state-of-the-art results on theÂ BookSum datasetÂ for book-length summarization. A zero-shot question-answering model can use our modelâs summaries to obtain competitive results on theÂ NarrativeQA datasetÂ for book-length question answering.[^footnote-2]Our approach: combining reinforcement learning from human feedback and recursive task decompositionConsider the task of summarizing a piece of text. LargeÂ pretrained models arenât very good at summarization. In the past we found that training a model withÂ reinforcement learning from human feedbackÂ helped align model summaries with human preferences on short posts and articles. But judging summaries of entire books takes a lot of effort to do directly since a human would need to read the entire book, which takes manyÂ hours.To address this problem, we additionally make use ofÂ recursive task decomposition: we procedurally break up a difficult task into easier ones. In this case we break up summarizing a long piece of text into summarizing several shorter pieces. Compared to an end-to-end training procedure, recursive task decomposition has the followingÂ advantages:Decomposition allows humans to evaluate model summaries more quickly by using summaries of smaller parts of the book rather than reading the sourceÂ text.It is easier to trace the summary-writing process. For example, you can trace to find where in the original text certain events from the summary happen. See for yourself onÂ our summaryÂ explorer!Our method can be used to summarize books of unbounded length, unrestricted by the context length of the transformer models weÂ use.Why we are working on thisThisÂ work is part of ourÂ ongoingÂ researchÂ into aligning advanced AI systems, which is key toÂ our mission.Â As we train our models to do increasingly complex tasks, making informed evaluations of the modelsâ outputs will become increasingly difficult for humans. This makes it harder to detect subtle problems in model outputs that could lead to negative consequences when these models are deployed. Therefore we want our ability to evaluate our models to increase as their capabilitiesÂ increase.Our current approach to this problem is toÂ empower humans to evaluate machine learning model outputs using assistance from other models. In this case, to evaluate book summaries we empower humans with individual chapter summaries written by our model, which saves them time when evaluating these summaries relative to reading the source text. Our progress on book summarization is the first large-scale empirical work on scaling alignmentÂ techniques.Going forward, we are researching better ways to assist humans in evaluating model behavior, with the goal of finding techniques that scale to aligning artificial generalÂ intelligence.Weâre always looking for more talented people to join us; so if this work interests you, pleaseÂ apply to join ourÂ team!AuthorsJeffrey WuRyan LoweJan LeikeAcknowledgmentsWeâd like to acknowledge our paper co-authors: Long Ouyang, Daniel Ziegler, Nisan Stiennon, and Paul Christiano.Thanks to the following for feedback on this release: Steve Dowling, Hannah Wong, Miles Brundage, Gretchen Krueger, Ilya Sutskever, and Sam Altman.Design: Justin Jay WangBook Cover Artwork: DALLÂ·EResearchOverviewIndexProductOverviewChatGPTGPT-4DALLÂ·E 2Customer storiesSafety standardsAPI data privacyPricingSafetyOverviewCompanyAboutBlogCareersCharterSecurityOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top
