


Measuring Goodhart’s law












CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexProductOverviewChatGPTGPT-4DALLÂ·E 2Customer storiesSafety standardsAPI data privacyPricingDevelopersOverviewDocumentationAPI referenceExamplesSafetyCompanyAboutBlogCareersCharterSecuritySearch Navigation quick links Log inSign upMenu Mobile Navigation CloseSite NavigationResearchProductDevelopersSafetyCompany Quick Links Log inSign upSearch Submit Measuring Goodhartâs lawGoodhartâs lawÂ famously says: âWhen a measure becomes a target, it ceases to be a good measure.â Although originally from economics, itâs something we have to grapple with at OpenAI when figuring out how to optimize objectives that are difficult or costly to measure.April 13, 2022Safety & Alignment,Â ConclusionGoodhartâs lawÂ famously says: âWhen a measure becomes a target, it ceases to be a good measure.â Although originally from economics, itâs something we have to grapple with at OpenAI when figuring out how to optimize objectives that are difficult or costly to measure. Itâs often necessary to introduce someÂ proxy objectiveÂ thatâs easier or cheaper to measure, but when we do this, we need to be careful not to optimize it tooÂ much.For example, as part of our work toÂ alignÂ models like GPT-3 with human intent and values, we would like to optimize things like âHowÂ helpfulÂ is this response?â, or âHowÂ factually accurateÂ is this claim?â. These are complex objectives that require humans to carefully check things over. For this reason, we train a model to predict these human preferences, known as aÂ reward model, and use the reward modelâs predictions as a proxy objective. But itâs important to keep track of how well the true objective is beingÂ optimized.In this post weâll look at some of the mathematics behind how we do this. Weâll focus on a setting that is particularly clean to analyze, in which we have access to the true objective. In practice, even human preferences can fail to measure what we really care about, but weâre setting that issue aside in thisÂ post.Best-of-n samplingThere are many ways in which one could optimize the proxy objective, but perhaps the simplest isÂ best-of-n n n sampling, also known asÂ rejection samplingÂ orÂ reranking. We simply sampleÂ n times and take the one that scores the highest according to the proxyÂ objective.Although this method is very simple, it can actually be competitive with more advanced techniques such as reinforcement learning, albeit at the cost of more inference-time compute. For example, inÂ WebGPT, our best-of-64 model outperformed our reinforcement learning model, perhaps in part because the best-of-64 model got to browse many more websites. Even applying best-of-4 provided a significant boost to humanÂ preferences.In addition, best-of-n n n sampling has reliable performance and is straightforward to analyze mathematically, making it well-suited to empirical studies of Goodhartâs law and relatedÂ phenomena.The mathematics of best-of-n samplingLetâs study best-of-n n n sampling more formally. Suppose we have some sample spaceÂ S S SÂ (such as the set of possible question-answer pairs), some probability distributionÂ P P P overÂ S S S,Â a true objective (or ârewardâ)Â Rtrue:SâR  R_{\text{true}}:S\to\mathbb R Rtrueâ:SâR, and a proxy objectiveÂ Rproxy:SâR R_{\text{proxy}}:S\to\mathbb RRproxyâ:SâR. Letâs say that we somehow optimizeÂ Rproxy R_{\text{proxy}} Rproxyâ and thereby obtain some new distributionÂ Pâ² P^\prime Pâ². Then:The expectationÂ Exâ²â¼Pâ²[Rtrue(xâ²)]â \mathbb E_{x^\prime\sim P^\prime}\left[R_{\text{true}}\left(x^\prime\right)\right]â Exâ²â¼Pâ²â[Rtrueâ(xâ²)]âÂ measures how well we have optimized the trueÂ objective.TheÂ KL divergenceÂ DKL(Pâ²â¥P) D_{\text{KL}}\left(P^\prime\parallel P\right) DKLâ(Pâ²â¥P) measures how much optimization we have done. For example, ifÂ Pâ² P^\prime Pâ² is obtained by taking the first sample fromÂ P P P that lies in some subsetÂ Sâ²âS S^\prime\subseteq S Sâ²âS,Â then this KL divergence is just the negative log probability that a sample fromÂ P P PÂ lies inÂ Sâ² S^\prime Sâ².It turns out that in the case of best-of- n n n sampling, both of these quantities can be estimated efficiently using samples fromÂ P P P.Letâs look at the expectation first. The naive approach is to use a Monte Carlo estimator: run best-of- n n n sampling many times, measure the true objective on those samples, and average the results. However, there is a better estimator. If we haveÂ Nâ¥n N\geq n Nâ¥n samples fromÂ P P P overall, then we can simultaneously considerÂ every possible subsetÂ of these samples of sizeÂ n n n(kâ1nâ1) \binom{k-1}{n-1} (nâ1kâ1â),Â whereÂ k k k is the rank of the sample under the proxy objective, fromÂ 1 1 1 (worst) up toÂ N N N (best).[^footnote-1]The sum of these weights isÂ (Nn) \binom{N}{n} (nNâ), giving a proof of theÂ Hockey-stick identity. For a formal derivation of the estimator described here, see Appendix I of theÂ WebGPT paper.As well as using samples more efficiently, this also allows us to reuse samples for different values ofÂ n n nP P P (i.e., as long asÂ P P P has no point masses). One might naively guess that the answer isÂ logâ¡n \log n logn,Â since best-of-n n n is doing something like taking the topÂ 1n \frac 1n n1â of the distribution, and this is roughly correct: the exact answer isÂ logâ¡nânâ1n \log n-\frac{n-1}n lognânnâ1â. [^footnote-2]Together, these estimators allow us to easily analyze how the true objective varies with the amount of optimization applied to the proxyÂ objective.Hereâs a real-life example fromÂ WebGPT: Best-of-nnn performance for WebGPT 175B  Best-of-nnn performance for WebGPT, with shaded regions representing \pm 1Â±1 standard error, and the KL axis following a square root scale. Here, the original distribution (PPP) is given by the 175B model trained using behavior cloning, the proxy objective used to compute best-of-nnn (RproxyR_{\text{proxy}}Rproxyâ) is given by the training reward model, and we consider three putatively âtrueâ objectives (RtrueR_{\text{true}}Rtrueâ): the training reward model itself, a validation reward model trained on held-out data, and actual human preferences. There isnât much over-optimization of the proxy objective, but we would expect there to be at higher KLs. Going beyond best-of-n samplingThe main limitation of best-of-n n n sampling is that the KL divergence grows logarithmically withÂ n n n, so it is only suitable for applying a small amount ofÂ optimization.To apply more optimization, we typically use reinforcement learning. In the settings weâve studied so far, such asÂ summarization, weâve typically been able to reach a KL of around 10Â natsÂ using reinforcement learning before the true objective starts to decrease due to Goodhartâs law. Weâd have to takeÂ n to be around 60,000 to reach this KL usingÂ best-of-n n n,Â and we hope to be able to reach much larger KLs than this with improvements to our reward modeling and reinforcement learningÂ practices.However, not all nats are equal. Empirically, for small KL budgets, best-of-n n nn n n is the âbrute forceâ approach, making it more information-theoretically efficient than reinforcement learning, but less computationally efficient at large KLs.[^footnote-3]Weâre actively studying the scaling properties of proxy objectives as part of our work toÂ alignÂ our models with human intent and values. If youâd like to help us with this research, weâreÂ hiring!AuthorsJacob HiltonLeo GaoAcknowledgmentsThanks to Suchir Balaji, Paul Christiano, William Guss, Vineet Kosaraju, John Schulman, Nisan Stiennon, Jeff Wu, and Daniel Ziegler for discussions related to the ideas in this post. Thanks to Greg Brockman, Jan Leike, Holly Mandel, John Schulman, and Jeff Wu for feedback on drafts. Thanks to Bianca Martin, Steve Dowling, Natalie Summers and Justin Jay Wang for communications and design.ResearchOverviewIndexProductOverviewChatGPTGPT-4DALLÂ·E 2Customer storiesSafety standardsAPI data privacyPricingSafetyOverviewCompanyAboutBlogCareersCharterSecurityOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top
