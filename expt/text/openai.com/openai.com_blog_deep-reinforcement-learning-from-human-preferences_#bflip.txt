


Learning from human preferences












CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexProductOverviewChatGPTGPT-4DALLÂ·E 2Customer storiesSafety standardsAPI data privacyPricingDevelopersOverviewDocumentationAPI referenceExamplesSafetyCompanyAboutBlogCareersCharterSecuritySearch Navigation quick links Log inSign upMenu Mobile Navigation CloseSite NavigationResearchProductDevelopersSafetyCompany Quick Links Log inSign upSearch Submit Learning from human preferencesOne step towards building safe AI systems is to remove the need for humans to write goal functions, since using a simple proxy for a complex goal, or getting the complex goal a bit wrong, can lead to undesirable and even dangerous behavior. In collaboration with DeepMindâs safety team, weâve developed an algorithm which can infer what humans want by being told which of two proposed behaviors is better.June 13, 2017More resourcesRead paperHuman feedback,Â Reinforcement learning,Â Safety & Alignment,Â ReleaseWe present a learning algorithm that uses small amounts of human feedback to solve modern RL environments. Machine learning systems with human feedbackÂ haveÂ beenÂ exploredÂ before, but weâve scaled up the approach to be able to work on much more complicated tasks. Our algorithm needed 900 bits of feedback from a human evaluator to learn to backflipâa seemingly simple task which is simple to judge butÂ challengingÂ toÂ specify.Our algorithm learned to backflip using around 900 individual bits of feedback from the human evaluator.The overall training process is a 3-step feedback cycle between the human, the agentâs understanding of the goal, and the RLÂ training.Our AI agent starts by acting randomly in the environment. Periodically, two video clips of its behavior are given to a human, and the human decides which of the two clips is closest to fulfilling its goalâin this case, a backflip. The AI gradually builds a model of the goal of the task by finding the reward function that best explains the humanâs judgments. It then uses RL to learn how to achieve that goal. As its behavior improves, it continues to ask for human feedback on trajectory pairs where itâs most uncertain about which is better, and further refines its understanding of theÂ goal.Our approach demonstrates promising sample efficiencyâas stated previously, the backflip video required under 1000 bits of human feedback. It took less than an hour of a human evaluatorâs time, while in the background the policy accumulated about 70 hours of overall experience (simulated at a much faster rate than real-time.) We will continue to work on reducing the amount of feedback a human needs to supply. You can see a sped-up version of the training process in the followingÂ video.Human Feedback training process00:39Weâve tested our method on a number of tasks in the simulated robotics and Atari domains (without being given access to the reward function: so in Atari, without having access to the game score). Our agents can learn from human feedback to achieve strong and sometimes superhuman performance in many of the environments we tested. In the following animation you can see agents trained with our technique playing a variety of Atari games. The horizontal bar on the right hand side of each frame representâs each agents prediction about how much a human evaluator would approve of their current behavior. These visualizations indicate that agents trained with human feedback learn to value oxygen in Seaquest (left), anticipate rewards in Breakout and Pong (center), or work out how to recover from crashes in EnduroÂ (right).Note thereâs no need for the feedback to align with the environmentâs normal reward function: we can, for example, train our agents to precisely keep even with other cars in Enduro rather than maximizing game score by passing them. We also sometimes find that learning from feedback does better than reinforcement learning with the normal reward function, because the human shapes the reward better than whoever wrote the environmentâsÂ reward.ChallengesOur algorithmâs performance is only as good as the human evaluatorâs intuition about what behaviorsÂ lookÂ correct, so if the human doesnât have a good grasp of the task they may not offer as much helpful feedback. Relatedly, in some domains our system can result in agents adopting policies that trick the evaluators. For example, a robot which was supposed to grasp items instead positioned its manipulator in between the camera and the object so that it onlyÂ appearedÂ to be grasping it, as shownÂ below.We addressed this particular problem by adding in visual cues (the thick white lines in the above animation) to make it easy for the human evaluators to estimateÂ depth.The research described in this post was done in collaboration with Jan Leike, Miljan Martic, and Shane Legg at DeepMind. Our two organizations plan to continue to collaborate on topics that touch on long-term AI safety. We think that techniques like this are a step towards safe AI systems capable of learning human-centric goals, and can complement and extend existing approaches like reinforcement and imitation learning. This post is representative of the work done by OpenAIâs safety team; if youâre interested in working on problems like this, pleaseÂ joinÂ us!FootnoteBy comparison, we took two hours to write our own reward function (the animation in the above right) to get a robot to backflip, and though it succeeds itâs a lot less elegant than the one trained simply through human feedback (top left). We think there are many cases where human feedback could let us specify a specific goal more intuitively and quickly than is possible by manually hand-crafting theÂ objective.You can replicate this backflip inÂ gymÂ with the following reward function forÂ Hopper:def reward_fn(a, ob):
    backroll = -ob[7]
    height = ob[0]
    vel_act = a[0] * ob[8] + a[1] * ob[9] + a[2] * ob[10]
    backslide = -ob[5]
    return backroll * (1.0 + .3 * height + .1 * vel_act + .05 * backslide)nullAuthorsDario AmodeiPaul ChristianoAlex RayResearchOverviewIndexProductOverviewChatGPTGPT-4DALLÂ·E 2Customer storiesSafety standardsAPI data privacyPricingSafetyOverviewCompanyAboutBlogCareersCharterSecurityOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top
