


CLIP: Connecting text and images












CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexProductOverviewChatGPTGPT-4DALLÂ·E 2Customer storiesSafety standardsAPI data privacyPricingDevelopersOverviewDocumentationAPI referenceExamplesSafetyCompanyAboutBlogCareersCharterSecuritySearch Navigation quick links Log inSign upMenu Mobile Navigation CloseSite NavigationResearchProductDevelopersSafetyCompany Quick Links Log inSign upSearch Submit Illustration: Justin Jay WangCLIP: Connecting text and imagesWeâre introducing a neural network called CLIP which efficiently learns visual concepts from natural language supervision. CLIP can be applied to any visual classification benchmark by simply providing the names of the visual categories to be recognized, similar to the âzero-shotâ capabilities of GPT-2 andÂ GPT-3.January 5, 2021More resourcesRead paperView codeComputer vision,Â Representation learning,Â Transfer learning,Â Contrastive learning,Â Supervised learning,Â CLIP,Â Milestone,Â Publication,Â ReleaseAlthough deep learning has revolutionized computer vision, current approaches have several major problems: typical vision datasets are labor intensive and costly to create while teaching only a narrow set of visual concepts; standard vision models are good at one task and one task only, and require significant effort to adapt to a new task; and models that perform well on benchmarks have disappointingly poor performance on stress tests,[^reference-1][^reference-2][^reference-3][^reference-4]Â casting doubt on the entire deep learning approach to computerÂ vision.We present a neural network that aims to address these problems: it is trained on a wide variety of images with a wide variety of natural language supervision thatâs abundantly available on the internet. By design, the network can be instructed in natural language to perform a great variety of classification benchmarks, without directly optimizing for the benchmarkâs performance, similar to the âzero-shotâ capabilities of GPT-2[^reference-5]Â and GPT-3.[^reference-6]Â This is a key change: by not directly optimizing for the benchmark, we show that it becomes much more representative: our system closes this ârobustness gapâ by up to 75% while matching the performance of the original ResNet-50[^reference-7]Â onÂ ImageNetÂ zero-shot without using any of the original 1.28M labeledÂ examples.Although both models have the same accuracy on the ImageNet test set, CLIPâs performance is much more representative of how it will fare on datasets that measure accuracy in different, non-ImageNet settings. For instance, ObjectNet checks a modelâs ability to recognize objects in many different poses and with many different backgrounds inside homes while ImageNet Rendition and ImageNet Sketch check a modelâs ability to recognize more abstract depictions of objects.Background and related workCLIP (Contrastive LanguageâImage Pre-training) builds on a large body of work on zero-shot transfer, natural language supervision, and multimodal learning. The idea of zero-data learning dates back over a decade[^reference-8]Â but until recently was mostly studied in computer vision as a way of generalizing to unseen object categories.[^reference-9][^reference-10]Â A critical insight was to leverage natural language as a flexible prediction space to enable generalization and transfer. In 2013, Richer Socher and co-authors at Stanford[^reference-11]Â developed a proof of concept by training a model on CIFAR-10 to make predictions in a word vector embedding space and showed this model could predict two unseen classes. The same year DeVISE[^reference-12]Â scaled this approach and demonstrated that it was possible to fine-tune an ImageNet model so that it could generalize to correctly predicting objects outside the original 1000 trainingÂ set.Most inspirational for CLIP is the work of Ang Li and his co-authors at FAIR[^reference-13]Â who in 2016 demonstrated using natural language supervision to enable zero-shot transfer to several existing computer vision classification datasets, such as the canonical ImageNet dataset. They achieved this by fine-tuning an ImageNet CNN to predict a much wider set of visual concepts (visual n-grams) from the text of titles, descriptions, and tags of 30 million Flickr photos and were able to reach 11.5% accuracy on ImageNetÂ zero-shot.Finally, CLIP is part of a group of papers revisiting learning visual representations from natural language supervision in the past year. This line of work uses more modern architectures like the Transformer[^reference-32]Â and includes VirTex,[^reference-33]Â which explored autoregressive language modeling, ICMLM,[^reference-34]Â which investigated masked language modeling, and ConVIRT,[^reference-35]Â which studied the same contrastive objective we use for CLIP but in the field of medicalÂ imaging.ApproachWe show that scaling a simple pre-training task is sufficient to achieve competitive zero-shot performance on a great variety of image classification datasets. Our method uses an abundantly available source of supervision: the text paired with images found across the internet. This data is used to create the following proxy training task for CLIP: given an image, predict which out of a set of 32,768 randomly sampled text snippets, was actually paired with it in ourÂ dataset.In order to solve this task, our intuition is that CLIP models will need to learn to recognize a wide variety of visual concepts in images and associate them with their names. As a result, CLIP models can then be applied to nearly arbitrary visual classification tasks. For instance, if the task of a dataset is classifying photos of dogs vs cats we check for each image whether a CLIP model predicts the text description âa photo of aÂ dogâ or âa photo of aÂ catâ is more likely to be paired withÂ it.CLIP pre-trains an image encoder and a text encoder to predict which images were paired with which texts in our dataset. We then use this behavior to turn CLIP into a zero-shot classifier. We convert all of a datasetâs classes into captions such as âa photo of a dogâ and predict the class of the caption CLIP estimates best pairs with a given image.CLIP was designed to mitigate a number of major problems in the standard deep learning approach to computerÂ vision:Costly datasets: Deep learning needs a lot of data, and vision models have traditionally been trained on manually labeled datasets that are expensive to construct and only provide supervision for a limited number of predetermined visual concepts. The ImageNet dataset, one of the largest efforts in this space, required over 25,000 workers to annotate 14 million images for 22,000 object categories. In contrast, CLIP learns from textâimage pairs that are already publicly available on the internet. Reducing the need for expensive large labeled datasets has been extensively studied by prior work, notably self-supervised learning,[^reference-14][^reference-15][^reference-16]Â contrastive methods,[^reference-17][^reference-18][^reference-19][^reference-20][^reference-21]Â self-training approaches,[^reference-22][^reference-23]Â and generative modeling.[^reference-24][^reference-25][^reference-26][^reference-27]Narrow: An ImageNet model is good at predicting the 1000 ImageNet categories, but thatâs all it can do âout of the box.â If we wish to perform any other task, an ML practitioner needs to build a new dataset, add an output head, and fine-tune the model. In contrast, CLIP can be adapted to perform a wide variety of visual classification tasks without needing additional training examples. To apply CLIP to a new task, all we need to do is âtellâ CLIPâs text-encoder the names of the taskâs visual concepts, and it will output a linear classifier of CLIPâs visual representations. The accuracy of this classifier is often competitive with fully supervisedÂ models.We show random, non-cherry picked, predictions of zero-shot CLIP classifiers on examples from various datasetsÂ below.Loading dataâ¦Poor real-world performance: Deep learning systems are often reported to achieve human or even superhuman performance[^reference-28][^footnote-1] on vision benchmarks, yet when deployed in the wild, their performance can be far below the expectation set by the benchmark. In other words, there is a gap between âbenchmark performanceâ and âreal performance.â We conjecture that this gap occurs because the models âcheatâ by only optimizing for performance on the benchmark, much like a student who passed an exam by studying only the questions on past yearsâ exams. In contrast, the CLIP model can be evaluated on benchmarks without having to train on their data, so it canât âcheatâ in this manner. This results in its benchmark performance being much more representative of its performance in the wild. To verify the âcheating hypothesisâ, we also measure how CLIPâs performance changes when it is able to âstudyâ for ImageNet. When a linear classifier is fitted on top of CLIPâs features, it improves CLIPâs accuracy on the ImageNet test set by almost 10%. However, this classifier doesÂ no betterÂ on average across an evaluation suite of 7 other datasets measuring ârobustâ performance.[^reference-30]Key takeaways1. CLIP is highly efficientCLIP learns from unfiltered, highly varied, and highly noisy data, and is intended to be used in a zero-shot manner. We know from GPT-2 and 3 that models trained on such data can achieve compelling zero shot performance; however, such models require significant training compute. To reduce the needed compute, we focused on algorithmic ways to improve the training efficiency of ourÂ approach.We report two algorithmic choices that led to significant compute savings. The first choice is the adoption of a contrastive objective for connecting text with images.[^reference-31][^reference-17][^reference-35]Â We originally explored an image-to-text approach, similar to VirTex,[^reference-33]Â but encountered difficulties scaling this to achieve state-of-the-art performance. In small to medium scale experiments, we found that the contrastive objective used by CLIP is 4x to 10x more efficient at zero-shot ImageNet classification. The second choice was the adoption of the Vision Transformer,[^reference-36]Â which gave us a further 3x gain in compute efficiency over a standard ResNet. In the end, our best performing CLIP model trains on 256 GPUs for 2 weeks which is similar to existing large scale image models.[^reference-37][^reference-23][^reference-38][^reference-36]We originally explored training image-to-caption language models but found this approach struggled at zero-shot transfer. In this 16 GPU day experiment, a language model only achieves 16% accuracy on ImageNet after training for 400 million images. CLIP is much more efficient and achieves the same accuracy roughly 10x faster.2. CLIP is flexible and generalBecause they learn a wide range of visual concepts directly from natural language, CLIP models are significantly more flexible and general than existing ImageNet models. We find they are able to zero-shot perform many different tasks. To validate this we have measured CLIPâs zero-shot performance on over 30 different datasets including tasks such as fine-grained object classification, geo-localization, action recognition in videos, and OCR.[^footnote-2] In particular, learning OCR is an example of an exciting behavior that does not occur in standard ImageNet models. Above, we visualize a random non-cherry picked prediction from each zero-shotÂ classifier.This finding is also reflected on a standard representation learning evaluation using linear probes. The best CLIP model outperforms the best publicly available ImageNet model, the Noisy Student EfficientNet-L2,[^reference-23]Â on 20 out of 26 different transfer datasets weÂ tested.CLIP-ViTInstagramViT (ImageNet-21k)CLIP-ResNetSimCLRv2BiT-MEfficientNet-NoisyStudentBYOLBiT-SEfficientNetMoCoResNetAcross a suite of 27 datasets measuring tasks such as fine-grained object classification, OCR, activity recognition in videos, and geo-localization, we find that CLIP models learn more widely useful image representations. CLIP models are also more compute efficient than the models from 10 prior approaches that we compare with. LimitationsWhile CLIP usually performs well on recognizing common objects, it struggles on more abstract or systematic tasks such as counting the number of objects in an image and on more complex tasks such as predicting how close the nearest car is in a photo. On these two datasets, zero-shot CLIP is only slightly better than random guessing. Zero-shot CLIP also struggles compared to task specific models on very fine-grained classification, such as telling the difference between car models, variants of aircraft, or flowerÂ species.CLIP also still has poor generalization to images not covered in its pre-training dataset. For instance, although CLIP learns a capable OCR system, when evaluated on handwritten digits from the MNIST dataset, zero-shot CLIP only achieves 88% accuracy, well below the 99.75% of humans on the dataset. Finally, weâve observed that CLIPâs zero-shot classifiers can be sensitive to wording or phrasing and sometimes require trial and error âprompt engineeringâ to performÂ well.Broader impactsCLIP allows people to design their own classifiers and removes the need for task-specific training data. The manner in which these classes are designed can heavily influence both model performance and model biases. For example, we find that when given a set of labels including Fairface[^reference-39]Â race labels[^footnote-3] and a handful of egregious terms such as âcriminalâ, âanimal,â etc., the model tends to classify images of people aged 0â20 in the egregious category at a rate of ~32.3%. However, when we add the class âchildâ to the list of possible classes, this behaviour drops toÂ ~8.7%.Additionally, given that CLIP does not need task-specific training data it can unlock certain niche tasks with greater ease. Some of these tasks may raise privacy or surveillance related risks and we explore this concern by studying the performance of CLIP on celebrity identification. CLIP has a top-1 accuracy of 59.2% for âin the wildâ celebrity image classification when choosing from 100 candidates and a top-1 accuracy of 43.3% when choosing from 1000 possible choices. Although itâs noteworthy to achieve these results with task agnostic pre-training, this performance is not competitive when compared to widely available production level models. We further explore challenges that CLIP poses in ourÂ paperÂ and we hope that this work motivates future research on the characterization of the capabilities, shortcomings, and biases of such models. We are excited to engage with the research community on suchÂ questions.ConclusionWith CLIP, weâve tested whether task agnostic pre-training on internet scale natural language, which has powered a recent breakthrough in NLP, can also be leveraged to improve the performance of deep learning for other fields. We are excited by the results weâve seen so far applying this approach to computer vision. Like the GPT family, CLIP learns a wide variety of tasks during pre-training which we demonstrate via zero-shot transfer. We are also encouraged by our findings on ImageNet that suggest zero-shot evaluation is a more representative measure of a modelâsÂ capability.AuthorsAlec RadfordIlya SutskeverJong Wook KimGretchen KruegerSandhini AgarwalAcknowledgmentsWeâd like to thank the millions of people involved in creating the data CLIP is trained on. We also are grateful to all our co-authors for their contributions to the project. Finally, weâd like to thank Jeff Clune, Miles Brundage, Ryan Lowe, Jakub Pachocki, and Vedant Misra for feedback on drafts of this blog and Matthew Knight for reviewing the code release.Design & Cover ArtworkJustin Jay WangResearchOverviewIndexProductOverviewChatGPTGPT-4DALLÂ·E 2Customer storiesSafety standardsAPI data privacyPricingSafetyOverviewCompanyAboutBlogCareersCharterSecurityOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top
