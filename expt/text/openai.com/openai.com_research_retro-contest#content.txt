


Retro Contest












CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexProductOverviewChatGPTGPT-4DALLÂ·E 2Customer storiesSafety standardsAPI data privacyPricingDevelopersOverviewDocumentationAPI referenceExamplesSafetyCompanyAboutBlogCareersCharterSecuritySearch Navigation quick links Log inSign upMenu Mobile Navigation CloseSite NavigationResearchProductDevelopersSafetyCompany Quick Links Log inSign upSearch Submit Illustration: Timothy J. ReynoldsRetro ContestWeâre launching a transfer learning contest that measures a reinforcement learning algorithmâs ability to generalize from previous experience.April 5, 2018More resourcesContestRead paperGym RetroReinforcement learning,Â Transfer learning,Â Games,Â Environments,Â Open source,Â Community,Â Milestone,Â PublicationWhy it mattersIn typical RL research, algorithms are tested in the same environment where they were trained, which favors algorithms which are good at memorization and have many hyperparameters. Instead, our contest tests an algorithm on previously unseen video game levels. This contest uses Gym Retro, a new platform integrating classic games into Gym, starting with 30 SEGA Genesis games.Update: TheÂ resultsÂ areÂ in!TheÂ OpenAI Retro ContestÂ gives you a training set of levels from the Sonic The Hedgehogâ¢ series of games, and we evaluate your algorithm on a test set of custom levels that we have created for this contest. The contest will run from April 5 to June 5. To get people started weâre releasingÂ retro-baselines, which shows how to run several RL algorithms on the contestÂ tasks.Baseline results on the Retro Contest (test set) show that RL algorithms fall far below human performance, even when using transfer learning. Human performance is shown as a dashed horizontal line. The humans only played for one hour, versus eighteen for the algorithms.You can use any environments or datasets you want at training time, but at test time you only get about 18 hours (1 million timesteps) on each never-before-seen level. 18 hours may sound like a long time to play a single game level, but existing RL algorithms perform far worse than humans given this trainingÂ budget.Sonic BenchmarkTo describe the benchmark in detail, as well as provide some baseline results, we are releasing a technical report:Â Gotta Learn Fast: A New Benchmark for Generalization in RL. This report contains details about the benchmark as well as results from runningÂ Rainbow DQN,Â PPO, and a simple random guessing algorithm called JERK. JERK samples random action sequences in a way that is optimized for Sonic, and as training progresses it replays the top-scoring sequence of actions moreÂ frequently.We found that we could significantly boost PPOâs performance on the test levels by leveraging experience from the training levels. When the network was pre-trained on the training levels and fine-tuned on the test levels, its performance nearly doubled, making it better than the strongest alternative baselines. While this is not the first reported instance of successful transfer learning in RL, it is exciting because it shows that transfer learning can have a large and reliableÂ effect.But we have a long way to go before our algorithms can rival human performance. As shown above, after two hours of practice on the training levels and one hour of play on each test level, humans are able to attain scores that are significantly higher than those attained by RL algorithms, including ones that perform transferÂ learning.Sonic RecordingsWeâve created aÂ dataset of recordings of humansÂ beating the Sonic levels used in the Retro Contest. These recordings can be used to have the agent start playing from random points sampled from the course of each level, exposing the agent to a lot of areas it may not have seen if it only started from the beginning of the level. Researchers can also use these recordings to try to train agents that learn fromÂ demonstrations.Gym Retro BetaWe are releasing Gym Retro, a system for wrapping classic video games as RL environments. This preliminary release includes 30 SEGA Genesis games from theÂ SEGA Mega Drive and Genesis Classics Steam BundleÂ as well as 62 of the Atari 2600 games from the Arcade LearningÂ Environment.The Arcade Learning Environment, a collection of Atari 2600 games with interfaces for reinforcement learning, has been a major driver of RL research for the last five years. These Atari games were more varied and complex than previous RL benchmarks, having been designed to challenge the motor skills and problem solving abilities of humanÂ players.TheÂ Gym Retro BetaÂ utilizes a more modern console than AtariâSEGA Genesisâexpanding the quantity and complexity of games that are available for RL research. Games made on the Genesis tend to have lots of levels that are similar in some dimensions (physics, object appearances) and different in others (layout, items), which makes them good testbeds for transfer learning. They also tend to be more complex than Atari games since they exploit the better hardware of the Genesis (for example, it has more than 500 times as much RAM as the Atari, a greater range of possible control inputs, and support for betterÂ graphics).Gym Retro was inspired by theÂ Retro Learning EnvironmentÂ but written to be more flexible than RLE; for instance, in Gym Retro you can specify the environment definition through JSON files rather than C++ code, making it easier to integrate newÂ games.Altered Beast for SEGA GenesisSpace Harrier II for SEGA GenesisGym Retro is our second generation attempt to build a large dataset of reinforcement learning environments. It builds on some of the same ideas as Universe from late 2016, but we werenât able to get good results from that implementation because Universe environments ran asynchronously, could only run in real time, and were often unreliable due to screen-based detection of game state. Gym Retro extends the model of the Arcade Learning Environment to a much larger set of potential games.To get started with Gym Retro check out the Getting Started section on GitHub.Sometimes, algorithms can find exploits within the game. Here, a PPO-trained policy discovers it can slip through the walls of a level to move right and attain a higher scoreâanother example of how particular reward functions can lead to AI agents manifestingÂ oddÂ behaviors.AuthorsChristopher HesseJohn SchulmanVicki PfauAlex NicholOleg KlimovLarissa SchiavoAcknowledgmentsOther contributionsThanks to Jonathan Gray and Tom Brown for their contributions to early versions of gym-retro.Thanks to Philipp Moritz, Robert Nishihara, Adam Stelmaszczyk, Aravind Srinivas, Qin Yongliang, Julian Togelius, and Emilio Parisotto for helpful feedback.Cover artworkTimothy J. ReynoldsResearchOverviewIndexProductOverviewChatGPTGPT-4DALLÂ·E 2Customer storiesSafety standardsAPI data privacyPricingSafetyOverviewCompanyAboutBlogCareersCharterSecurityOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top
