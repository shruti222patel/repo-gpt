


Robots that learn












CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexProductOverviewChatGPTGPT-4DALLÂ·E 2Customer storiesSafety standardsAPI data privacyPricingDevelopersOverviewDocumentationAPI referenceExamplesSafetyCompanyAboutBlogCareersCharterSecuritySearch Navigation quick links Log inSign upMenu Mobile Navigation CloseSite NavigationResearchProductDevelopersSafetyCompany Quick Links Log inSign upSearch Submit Illustration: Ben BarryRobots that learnWeâve created a robotics system, trained entirely in simulation and deployed on a physical robot, which can learn a new task after seeing it done once.May 16, 2017More resourcesRead paper (domain randomization)Read paper (one-shot imitation learning)Robotics,Â Domain randomization,Â Computer vision,Â Supervised learning,Â Meta-learning,Â Milestone,Â PublicationRobots that learn02:52AlgorithmsLast month, we showed anÂ earlier versionÂ of this robot where weâd trained its vision system usingÂ domain randomization, that is, by showing it simulated objects with a variety of color, backgrounds, and textures, without the use of any realÂ images.Now, weâve developed and deployed a new algorithm,Â one-shot imitation learning, allowing a human to communicate how to do a new task by performing it inÂ VR. Given a single demonstration, the robot is able to solve the same task from an arbitrary startingÂ configuration.General procedureOur system can learn a behavior from a single demonstration delivered within a simulator, then reproduce that behavior in different setups in reality.The system is powered by two neural networks: a vision network and an imitationÂ network.The vision network ingests an image from the robotâs camera and outputs state representing the positions of the objects. AsÂ before, the vision network is trained with hundreds of thousands of simulated images with different perturbations of lighting, textures, and objects. (The vision system is never trained on a realÂ image.)The imitation network observes a demonstration, processes it to infer the intent of the task, and then accomplishes the intent starting from another starting configuration. Thus, the imitation network must generalize the demonstration to a new setting. But how does the imitation network know how toÂ generalize?The network learns this from the distribution of training examples. It is trained on dozens of different tasks with thousands of demonstrations for each task. Each training example is a pair of demonstrations that perform the same task. The network is given the entirety of the first demonstration and a single observation from the second demonstration. We then use supervised learning to predict what action the demonstrator took at that observation. In order to predict the action effectively, the robot must learn how to infer the relevant portion of the task from the firstÂ demonstration.Applied to block stacking, the training data consists of pairs of trajectories that stack blocks into a matching set of towers in the same order, but start from different start states. In this way, the imitation network learns to match the demonstratorâs ordering of blocks and size of towers without worrying about the relative location of theÂ towers.Block stackingThe task of creating color-coded stacks of blocks is simple enough that we were able to solve it with a scripted policy in simulation. We used the scripted policy to generate the training data for the imitation network. At test time, the imitation network was able to parse demonstrations produced by a human, even though it had never seen messy human dataÂ before.The imitation network usesÂ soft attentionÂ over the demonstration trajectory and the state vector which represents the locations of the blocks, allowing the system to work with demonstrations of variable length. It also performs attention over the locations of the different blocks, allowing it to imitate longer trajectories than itâs ever seen, and stack blocks into a configuration that has more blocks than any demonstration in its trainingÂ data.For the imitation network to learn a robust policy, we had to inject a modest amount of noise into the outputs of the scripted policy. This forced the scripted policy to demonstrate how to recover when things go wrong, which taught the imitation network to deal with the disturbances from an imperfect policy. Without injecting the noise, the policy learned by the imitation network would usually fail to complete the stackingÂ task.If youâd like to help us build this robot,Â join usÂ atÂ OpenAI.AuthorsPeter WelinderBob McGrewJonas SchneiderYan DuanJosh TobinRachel FongAlex RayFilip WolskiVikash KumarJonathan HoMarcin AndrychowiczBradly StadieAnkur HandaMatthias PlappertErika ReinhardtPieter AbbeelGreg BrockmanIlya SutskeverJack ClarkWojciech ZarembaResearchOverviewIndexProductOverviewChatGPTGPT-4DALLÂ·E 2Customer storiesSafety standardsAPI data privacyPricingSafetyOverviewCompanyAboutBlogCareersCharterSecurityOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top
