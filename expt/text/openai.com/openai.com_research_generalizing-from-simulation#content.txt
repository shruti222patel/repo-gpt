


Generalizing from simulation












CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexProductOverviewChatGPTGPT-4DALLÂ·E 2Customer storiesSafety standardsAPI data privacyPricingDevelopersOverviewDocumentationAPI referenceExamplesSafetyCompanyAboutBlogCareersCharterSecuritySearch Navigation quick links Log inSign upMenu Mobile Navigation CloseSite NavigationResearchProductDevelopersSafetyCompany Quick Links Log inSign upSearch Submit Generalizing from simulationOur latest robotics techniques allow robot controllers, trained entirely in simulation and deployed on physical robots, to react to unplanned changes in the environment as they solve simple tasks. That is, weâve used these techniques to build closed-loop systems rather than open-loop ones as before.October 19, 2017More resourcesRead paper (dynamics randomization)Read paper (image-based learning)Robotics,Â Sim-to-real,Â Domain randomization,Â Transfer learning,Â PublicationThe simulator need not match the real-world in appearance or dynamics; instead, we randomize relevant aspects of the environment, from friction to action delays to sensor noise. Our new results provide more evidence that general-purpose robots can be built by training entirely in simulation, followed by a small amount of self-calibration in the realÂ world.This robot was trained in simulation with dynamics randomization to push a puck to a goal. In the real world, we put the puck on a bag of chips, and the robot still achieves the goal despite the bag changing the sliding properties in an unfamiliar way.Dynamics randomizationWe developedÂ dynamics randomizationÂ to train a robot to adapt to unknown real-world dynamics. During training, we randomize a large set of ninety-five properties that determine the dynamics of the environment, such as altering the mass of each link in the robotâs body; the friction and damping of the object it is being trained on; the height of the table the object is on; the latency between actions; the noise in its observations; and soÂ on.We used this approach to train anÂ LSTM-based policy to push a hockey puck around a table. Our feed-forward networksÂ failÂ at this task, whereas LSTMs can use their past observations to analyze the dynamics of the world and adjust their behaviorÂ accordingly.From vision to actionWe also trained a robot end-to-end in simulation using reinforcement learning (RL), and deployed the resulting policy on a physical robot. The resulting system maps vision directly to action without special sensors, and can adapt to visualÂ feedback.View from the robotâs camera. The policy for picking up blocks is trained end-to-end from vision to action, on randomized visuals. Note that in simulation, the gripper moves up slightly as the block slides down in order to keep the block in the desired position. The gripper doesnât do this in the physical world as the block does not slide.The abundance of RL results with simulated robots can make it seem like RL easily solves most robotics tasks. But common RL algorithms work well only on tasks where small perturbations to your action can provide an incremental change to the reward. Some robotics tasks have simple rewards, like walking, where you can be scored on distance traveled. But most tasks doÂ notâto define a dense reward for block stacking, youâd need to encode that the arm is close to the block, that the arm approaches the block in the correct orientation, that the block is lifted off the ground, the distance of block to the desired position,Â etc.We spent a number of months unsuccessfully trying to get conventional RL algorithms working on pick-and-place tasks before ultimately developing a new reinforcement learning algorithm,Â Hindsight Experience ReplayÂ (HER), which allows agents to learn from a binary reward by pretending that a failure was what they wanted to do all along and learning from it accordingly. (By analogy, imagine looking for a gas station but ending up at a pizza shop. You still donât know where to get gas, but youâve now learned where to get pizza.) We also usedÂ domain randomizationÂ on the visual shapes to learn a vision system robust enough for the physicalÂ world.Our HER implementation uses the actor-critic technique with asymmetric information. (TheÂ actorÂ is the policy, and theÂ criticÂ is a network which receives action/state pairs and estimates their Q-value, or sum of future rewards, providing training signal to the actor.) While the critic has access to the full state of the simulator, the actor only has access to RGB and depth data. Thus the critic can provide fully accurate feedback, while the actor uses only data present in the realÂ world.CostsBoth techniques increase the computational requirements: dynamics randomization slows training down by a factor of 3x, while learning from images rather than states is about 5â10xÂ slower.We see three approaches to building general-purpose robots: training on huge fleets of physical robots, making simulators increasingly match the real world, and randomizing the simulator to allow the model to generalize to the real-world. We increasingly believe that the third will be the most important part of theÂ solution.If youâre interested in helping us push towards general-purpose robots, considerÂ joining our team atÂ OpenAI.A naive feed-forward policy is unable to adapt to the real-world puck environment despite solving the task in simulation.AuthorsXue Bin PengLerrel PintoAlex RayBob McGrewJonas SchneiderJosh TobinMarcin AndrychowiczPeter WelinderPieter AbbeelWojciech ZarembaResearchOverviewIndexProductOverviewChatGPTGPT-4DALLÂ·E 2Customer storiesSafety standardsAPI data privacyPricingSafetyOverviewCompanyAboutBlogCareersCharterSecurityOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top
