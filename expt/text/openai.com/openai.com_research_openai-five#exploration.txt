


OpenAI Five













CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexProductOverviewChatGPTGPT-4DALLÂ·E 2Customer storiesSafety standardsAPI data privacyPricingDevelopersOverviewDocumentationAPI referenceExamplesSafetyCompanyAboutBlogCareersCharterSecuritySearch Navigation quick links Log inSign upMenu Mobile Navigation CloseSite NavigationResearchProductDevelopersSafetyCompany Quick Links Log inSign upSearch Submit OpenAI FiveOur team of five neural networks, OpenAI Five, has started to defeat amateur human teams at DotaÂ 2.June 25, 2018Dota 2,Â Reinforcement learning,Â Self-play,Â Games,Â Software engineering,Â OpenAI FiveOur team of five neural networks, OpenAI Five, has started toÂ defeatÂ amateur human teams atÂ Dota 2. While today we play withÂ restrictions, we aim to beat a team of top professionals atÂ The InternationalÂ in August subject only to a limited set of heroes. We may not succeed: Dota 2 is one of the most popular andÂ complexÂ esports games in the world, with creative and motivated professionals whoÂ trainÂ year-round to earn part of Dotaâs annualÂ $40MÂ prize poolÂ (the largest of any esportsÂ game).OpenAI Five plays 180 years worth of games against itself every day, learning via self-play. It trains using a scaled-up version ofÂ Proximal Policy OptimizationÂ running on 256 GPUs and 128,000 CPU coresâa larger-scale version of the system we built to play the much-simplerÂ solo variantÂ of the game last year. Using a separateÂ LSTMÂ for each hero and no human data, it learns recognizable strategies. This indicates thatÂ reinforcement learningÂ can yield long-term planning with large but achievable scaleâwithout fundamental advances, contrary to our own expectations upon starting theÂ project.To benchmark our progress, weâll host a match versus top players on August 5th.Â FollowÂ us on Twitch to view the live broadcast, orÂ requestÂ an invite to attend inÂ person!Play videoThe problemOne AI milestone is to exceed human capabilities in a complex video game likeÂ StarCraftÂ or Dota. Relative to previous AI milestones likeÂ ChessÂ orÂ Go, complex video games start to capture the messiness and continuous nature of the real world. The hope is that systems which solve complex video games will be highly general, with applications outside ofÂ games.Dota 2 is a real-time strategy game played between two teams of five players, with each player controlling a character called a âheroâ. A Dota-playing AI must master theÂ following:Long time horizons.Â Dota games run at 30 frames per second for an average of 45 minutes, resulting in 80,000 ticks per game. Most actions (like ordering a hero toÂ moveÂ to a location) have minor impact individually, but some individual actions likeÂ town portalÂ usage can affect the game strategically; someÂ strategiesÂ can play out over an entire game. OpenAI Five observes every fourth frame, yielding 20,000 moves.Â ChessÂ usually ends before 40 moves,Â GoÂ before 150 moves, with almost every move beingÂ strategic.Partially-observed state.Â Units and buildings can only see the area around them. The rest of the map is covered in a fog hiding enemies and their strategies. Strong play requires making inferences based on incomplete data, as well as modeling what oneâs opponent might be up to. Both chess and Go are full-informationÂ games.High-dimensional, continuous action space.Â In Dota, each hero can take dozens of actions, and many actions target either another unit or a position on the ground. We discretize the space into 170,000 possible actions per hero (not all valid each tick, such as using a spell onÂ cooldown); not counting the continuous parts, there are an average of ~1,000 valid actions each tick. The averageÂ number of actionsÂ in chess is 35; in Go,Â 250.High-dimensional, continuous observation space.Â Dota is played on a large continuousÂ mapÂ containing ten heroes, dozens of buildings, dozens ofÂ NPCÂ units, and a long tail of game features such as runes, trees, and wards. Our model observes the state of a Dota game via ValveâsÂ Bot APIÂ as 20,000 (mostly floating-point) numbers representing all information a human is allowed to access. A chess board is naturally represented as about 70 enumeration values (a 8x8 board of 6 piece types and minorÂ historicalÂ info); a Go board as about 400 enumeration values (a 19x19 board of 2 piece types plusÂ Ko).The Dota rules are also very complex â the game has been actively developed for over a decade, with game logic implemented in hundreds of thousands of lines of code. This logic takes milliseconds per tick to execute, versus nanoseconds for Chess or Go engines. The game also gets an update about once every two weeks, constantly changing the environmentÂ semantics.Our approachOur system learns using a massively-scaled version ofÂ Proximal Policy Optimization. Both OpenAI Five and our earlierÂ 1v1 botÂ learn entirely from self-play. They start with random parameters and do not useÂ searchÂ or bootstrap from humanÂ replays.OpenAI 1v1 botOpenAI FiveCPUs60,000 CPU cores on Azure128,000 preemptible CPU cores on GCPGPUs256 K80 GPUs on Azure256 P100 GPUs on GCPExperience collected~300 years per day~180 years per day (~900 years per day counting each hero separately)Size of observation~3.3 kB~36.8 kBObservations per second of gameplay107.5Batch size8,388,608 observations1,048,576 observationsBatches per minute~20~60RL researchers (including ourselves) have generallyÂ believedÂ that long time horizons would require fundamentally new advances, such asÂ hierarchicalÂ reinforcementÂ learning. Our results suggest that we havenât been giving todayâs algorithms enough credit â at least when theyâre run at sufficient scale and with a reasonable way ofÂ exploring.Our agent is trained to maximize the exponentially decayed sum of future rewards, weighted by an exponential decay factor calledÂ Î³. During the latest training run of OpenAI Five, we annealedÂ Î³Â fromÂ 0.998Â (valuing future rewards with a half-life of 46 seconds) toÂ 0.9997Â (valuing future rewards with a half-life of five minutes). For comparison, the longest horizon in theÂ PPOÂ paper was a half-life of 0.5 seconds, the longest in theÂ RainbowÂ paper was a half-life of 4.4 seconds, and theÂ Observe and Look FurtherÂ paper used a half-life of 46Â seconds.While the current version of OpenAI Five is weak atÂ last-hittingÂ (observing our test matches, the professional Dota commentatorÂ BlitzÂ estimated it around median for Dota players), itsÂ objective prioritizationÂ matches a common professional strategy. Gaining long-term rewards such as strategic map control often requires sacrificing short-term rewards such as gold gained fromÂ farming, since grouping up to attack towers takes time. This observation reinforces our belief that the system is truly optimizing over a longÂ horizon.OpenAI Five: Dota Gamplay4:20Model structureEach ofÂ OpenAI Fiveâs networksÂ contain a single-layer, 1024-unitÂ LSTMÂ that sees the current game state (extracted from ValveâsÂ Bot API) and emits actions through several possible action heads. Each head has semantic meaning, for example, the number of ticks to delay this action, which action to select, the X or Y coordinate of this action in a grid around the unit, etc. Action heads are computedÂ independently.Interactive demonstration of the observation space and action space used by OpenAI Five. OpenAI Five views the world as a list of 20,000 numbers, and takes an action by emitting a list of 8 enumeration values. Select different actions and targets to understand how OpenAI Five encodes each action, and how it observes the world. The image shows the scene as a human would seeÂ it.OpenAI Five can react to missing pieces of state that correlate with what it does see. For example, until recently OpenAI Fiveâs observations did not includeÂ shrapnelÂ zones (areas where projectiles rain down on enemies), which humans see on screen. However, we observed OpenAI Five learning to walk out of (though not avoid entering) active shrapnel zones, since it could see its healthÂ decreasing.ExplorationGiven a learning algorithm capable of handling long horizons, we still need to explore the environment. Even with ourÂ restrictions, there are hundreds of items, dozens of buildings, spells, and unit types, and a long tail of game mechanics to learn aboutâmany of which yield powerful combinations. Itâs not easy to explore this combinatorially-vast spaceÂ efficiently.OpenAI Five learns from self-play (starting from random weights), which provides a natural curriculum for exploring the environment. To avoid âstrategy collapseâ, the agent trains 80% of its games against itself and the other 20% against its past selves. In the first games, the heroes walk aimlessly around the map. After several hours of training, concepts such asÂ laning,Â farming, or fighting overÂ midÂ emerge. After several days, they consistently adopt basic human strategies: attempt to stealÂ BountyÂ runes from their opponents, walk to theirÂ tier oneÂ towers to farm, and rotate heroes around the map to gain lane advantage. And with further training, they become proficient at high-level strategies likeÂ 5-heroÂ push.In March 2017, our firstÂ agentÂ defeated bots but got confused against humans. To force exploration in strategy space, during training (and only during training) we randomized the properties (health, speed, start level, etc.) of the units, and it began beating humans. Later on, when a test player was consistently beating our 1v1 bot, we increased our training randomizations and the test player started to lose. (Our robotics team concurrently applied similar randomization techniques toÂ physicalÂ robotsÂ to transfer from simulation to the realÂ world.)OpenAI Five uses the randomizations we wrote for our 1v1 bot. It also uses a new âlane assignmentâ one. At the beginning of each training game, we randomly âassignâ each hero to some subset ofÂ lanesÂ and penalize it for straying from those lanes until a randomly-chosen time in theÂ game.Exploration is also helped by a good reward.Â Our rewardÂ consists mostly of metrics humans track to decide how theyâre doing in the game: net worth, kills, deaths, assists, last hits, and the like. We postprocess each agentâs reward by subtracting the other teamâs average reward to prevent the agents from finding positive-sumÂ situations.We hardcode item and skill builds (originally written for ourÂ scriptedÂ baseline), and choose which of the builds to use at random.Â CourierÂ management is also imported from the scriptedÂ baseline.CoordinationOpenAI Five does not contain an explicit communication channel between the heroesâ neural networks. Teamwork is controlled by a hyperparameter we dubbed âteam spiritâ. Team spirit ranges from 0 to 1, putting a weight on how much each of OpenAI Fiveâs heroes should care about its individual reward function versus the average of the teamâs reward functions. We anneal its value from 0 to 1 overÂ training.RapidOur system is implemented as a general-purpose RL training system called Rapid, which can be applied to anyÂ GymÂ environment. Weâve used Rapid to solve other problems at OpenAI, includingÂ CompetitiveÂ Self-Play.The training system is separated intoÂ rolloutÂ workers, which run a copy of the game and an agent gathering experience, andÂ optimizerÂ nodes, which perform synchronous gradient descent across a fleet of GPUs. The rollout workers sync their experience through Redis to the optimizers. Each experiment also contains workers evaluating the trained agent versus reference agents, as well as monitoring software such asÂ TensorBoard,Â Sentry, andÂ Grafana.During synchronous gradient descent, each GPU computes a gradient on its part of the batch, and then the gradients are globally averaged. We originally usedÂ MPIâsÂ allreduceÂ for averaging, but now use our ownÂ NCCL2Â wrappers that parallelize GPU computations and network data transfer.The latencies for synchronizing 58MB of data (size of OpenAI Fiveâs parameters) across different numbers of GPUs are shown on the right. The latency is low enough to be largely masked by GPU computation which runs in parallel withÂ it.Weâve implemented Kubernetes, Azure, and GCP backends forÂ Rapid.The gamesThus far OpenAI Five has played (with ourÂ restrictions) versus each of theseÂ teams:Best OpenAI employee team: 2.5kÂ MMRÂ (46thÂ percentile)Best audience players watching OpenAI employee match (including Blitz, who commentated the first OpenAI employee match): 4â6k MMR (90th-99th percentile), though theyâd never played as aÂ team.Valve employee team: 2.5â4k MMR (46th-90thÂ percentile).Amateur team: 4.2k MMR (93rd percentile), trains as aÂ team.Semi-pro team: 5.5k MMR (99th percentile), trains as aÂ team.The April 23rd version of OpenAI Five was the first to beat our scripted baseline. The May 15th version of OpenAI Five was evenly matched versus team 1, winning one game and losing another. The June 6th version of OpenAI Five decisively won all its games versus teams 1â3. We set up informalÂ scrimsÂ with teams 4 & 5, expecting to lose soundly, but OpenAI Five won two of its first three games versusÂ both.The teamwork aspect of the bot was just overwhelming. It feels like five selfless players that know a good general strategy.BlitzWe observed that OpenAIÂ Five:Repeatedly sacrificed its ownÂ safe laneÂ (top lane for dire; bottom lane for radiant) in exchange for controlling the enemyâs safe lane, forcing the fight onto the side that is harder for their opponent to defend. This strategy emerged in the professional scene in the last few years, and is now considered to be the prevailing tactic. Blitz commented that he only learned this after eight years of play, whenÂ Team LiquidÂ told him aboutÂ it.Pushed theÂ transitionsÂ from early- to mid-game faster than its opponents. It did this by: (1) setting up successfulÂ ganksÂ (when players move around the map to ambush an enemy heroâsee animation) when players overextended in their lane, and (2) by grouping up to take towers before the opponents could organize aÂ counterplay.Deviated from currentÂ playstyleÂ in a few areas, such as givingÂ supportÂ heroes (which usually do not take priority for resources) lots of early experience and gold. OpenAI Fiveâs prioritization allows for its damage to peak sooner and push its advantage harder, winning team fights and capitalizing on mistakes to ensure a fastÂ win.Trophies awarded after the match between the best players at OpenAI and our bot team. One trophy for the humans, one trophy for the bots (represented by Susan Zhang from our team!)Differences versus humansOpenAI Five is given access to the same information as humans, but instantly sees data like positions, healths, and item inventories that humans have to check manually. Our method isnât fundamentally tied to observing state, but just rendering pixels from the game would require thousands ofÂ GPUs.OpenAI Five averages around 150-170 actions per minute (and has a theoretical maximum of 450 due to observing every 4th frame). Frame-perfect timing, whileÂ possibleÂ for skilled players, is trivial for OpenAI Five. OpenAI Five has an average reaction time of 80ms, which is faster thanÂ humans.These differences matter most in 1v1 (where our bot had a reaction time of 67ms), but the playing field is relatively equitable as weâve seen humans learn from and adapt to the bot. Dozens ofÂ professionalsÂ usedÂ our 1v1 bot forÂ trainingÂ in the months after last yearâsÂ TI. According to Blitz, the 1v1 bot has changed the way people think about 1v1s (the bot adopted a fast-paced playstyle, and everyone has now adapted to keepÂ up).Surprising findingsBinary rewards can give good performance.Â Our 1v1 model had a shaped reward, including rewards for last hits, kills, and the like. We ran an experiment where we only rewarded the agent for winning or losing, and it trained an order of magnitude slower and somewhat plateaued in the middle, in contrast to the smooth learning curves we usually see. The experiment ran on 4,500 cores and 16 k80 GPUs, training to the level of semi-pros (70Â TrueSkill) rather than 90 TrueSkill of our best 1v1Â bot).Creep blocking can be learned from scratch.Â For 1v1, we learnedÂ creep blockingÂ using traditional RL with a âcreep blockâ reward. One of our team members left a 2v2 model training when he went on vacation (proposing to his now wife!), intending to see how much longer training would boost performance. To his surprise, the model hadÂ learned to creep blockÂ without any special guidance orÂ reward.Weâre still fixing bugs.Â The chart shows a training run of the code that defeated amateur players, compared to a version where we simply fixed a number of bugs, such as rare crashes during training, or a bug which resulted in a large negative reward for reaching level 25. It turns out itâs possible to beat good humans while still hiding seriousÂ bugs!A subset of the OpenAI Dota team, holding the laptop that defeated the worldâs top professionals at Dota 1v1 at The International last year.*Whatâs nextOur team is focused on making our August goal. We donât know if it will be achievable, but we believe that with hard work (and some luck) we have a realÂ shot.This post described a snapshot of our system as of June 6th. Weâll release updates along the way to surpassing human performance and write a report on our final system once we complete the project. Please join us on August 5thÂ virtuallyÂ orÂ in person, when weâll play a team of topÂ players!Our underlying motivation reaches beyond Dota. Real-world AI deployments will need to deal with theÂ challengesÂ raised by Dota which are not reflected in Chess, Go, Atari games, or Mujoco benchmark tasks. Ultimately, we will measure the success of our Dota system in its application to real-world tasks. If youâd like to be part of what comes next, weâreÂ hiring!AuthorsAuthorsGreg BrockmanChristy DennisonSusan ZhangJakub PachockiMichael PetrovHenrique PondÃ©PrzemysÅaw DÄbiakDavid FarhiFilip WolskiJonathan RaimanJie TangSzymon SidorBrooke ChanContributorsQuirin FischerChristopher HesseShariq HashmeIlya SutskeverAlec RadfordScott GrayJack ClarkPaul ChristianoDavid LuanChristopher BernerEric SiglerJonas SchneiderLarissa SchiavoDiane YoonJohn SchulmanAcknowledgmentsCurrent set of restrictionsMirror match of Necrophos, Sniper, Viper, Crystal Maiden, and LichNo wardingNo RoshanNo invisibility (consumables and relevant items)No summons/illusionsNo Divine Rapier, Bottle, Quelling Blade, Boots of Travel, Tome of Knowledge, Infused Raindrop5 invulnerable couriers, no exploiting them by scouting or tankingNo ScanThe hero set restriction makes the game very different from how Dota is played at world-elite level (i.e. Captains Mode drafting from all 100+ heroes). However, the difference from regular âpublicâ games (All Pick / Random Draft) is smaller.Most of the restrictions come from remaining aspects of the game we havenât integrated yet. Some restrictions, in particular wards and Roshan, are central components of professional-level play. Weâre working to add these as soon as possible.Draft feedbackThanks to the following for feedback on drafts of this post: Alexander Lavin, Andrew Gibiansky, Anna Goldie, Azalia Mirhoseini, Catherine Olsson, David Dohan, David Ha, Denny Britz, Erich Elsen, James Bradbury, John Miller, Luke Metz, Maddie Hall, Miles Brundage, Nelson Elhage, Ofir Nachum, Pieter Abbeel, Rumen Hristov, Shubho Sengupta, Solomon Boulos, Stephen Merity, Tom Brown, Zak StoneResearchOverviewIndexProductOverviewChatGPTGPT-4DALLÂ·E 2Customer storiesSafety standardsAPI data privacyPricingSafetyOverviewCompanyAboutBlogCareersCharterSecurityOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top
