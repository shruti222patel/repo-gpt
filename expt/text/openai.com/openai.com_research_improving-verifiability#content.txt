


Improving verifiability in AI development












CloseSearch Submit Skip to main contentSite NavigationResearchOverviewIndexProductOverviewChatGPTGPT-4DALLÂ·E 2Customer storiesSafety standardsAPI data privacyPricingDevelopersOverviewDocumentationAPI referenceExamplesSafetyCompanyAboutBlogCareersCharterSecuritySearch Navigation quick links Log inSign upMenu Mobile Navigation CloseSite NavigationResearchProductDevelopersSafetyCompany Quick Links Log inSign upSearch Submit Improving verifiability in AI developmentIllustration: Justin Jay WangWeâve contributed to a multi-stakeholder report byÂ 58 co-authorsÂ at 30 organizations, including theÂ Centre for the Future of Intelligence,Â Mila,Â Schwartz Reisman Institute for Technology and Society,Â Center for Advanced Study in the Behavioral Sciences, andÂ Center for Security and Emerging Technologies. This report describes 10 mechanisms to improve the verifiability of claims made about AI systems. Developers can use these tools to provide evidence that AI systems are safe, secure, fair, or privacy-preserving. Users, policymakers, and civil society can use these tools to evaluate AI developmentÂ processes.April 16, 2020More resourcesRead paperResponsible AI,Â PublicationWhile a growing number of organizations have articulated ethics principles to guide their AI development process, it can be difficult for those outside of an organization to verify whether the organizationâs AI systems reflect those principles in practice. This ambiguity makes it harder for stakeholders such as users, policymakers, and civil society to scrutinize AI developersâ claims about properties of AI systems and could fuel competitive corner-cutting, increasing social risks and harms. The report describes existing and potential mechanisms that can help stakeholders grapple with questionsÂ like:Can I (as a user) verify the claims made about the level of privacy protection guaranteed by a new AI system Iâd like to use for machine translation of sensitiveÂ documents?Can I (as a regulator) trace the steps that led to an accident caused by an autonomous vehicle? Against what standards should an autonomous vehicle companyâs safety claims beÂ compared?Can I (as an academic) conduct impartial research on the risks associated with large-scale AI systems when I lack the computing resources ofÂ industry?Can I (as an AI developer) verify that my competitors in a given area of AI development will follow best practices rather than cut corners to gain anÂ advantage?The 10 mechanisms highlighted in the report are listed below, along with recommendations aimed at advancing each one. (See theÂ reportÂ for discussion of how these mechanisms support verifiable claims as well as relevant caveats about ourÂ findings.)Institutional Mechanisms and RecommendationsThird party auditing. A coalition of stakeholders should create a task force to research options for conducting and funding third party auditing of AIÂ systems.Red teaming exercises. Organizations developing AI should run red teaming exercises to explore risks associated with systems they develop, and should share best practices andÂ tools.Bias and safety bounties. AI developers should pilot bias and safety bounties for AI systems to strengthen incentives and processes for broad-based scrutiny of AIÂ systems.Sharing of AI incidents. AI developers should share more information about AI incidents, including through collaborativeÂ channels.Software Mechanisms and RecommendationsAudit trails. Standard setting bodies should work with academia and industry to develop audit trail requirements for safety-critical applications of AIÂ systems.Interpretability. Organizations developing AI and funding bodies should support research into the interpretability of AI systems, with a focus on supporting risk assessment andÂ auditing.Privacy-preserving machine learning. AI developers should develop, share, and use suites of tools for privacy-preserving machine learning that include measures of performance against commonÂ standards.Hardware Mechanisms and RecommendationsSecure hardware for machine learning. Industry and academia should work together to develop hardware security features for AI accelerators or otherwise establish best practices for the use of secure hardware (including secure enclaves on commodity hardware) in machine learningÂ contexts.High-precision compute measurement. One or more AI labs should estimate the computing power involved in a single project in great detail and report on lessons learned regarding the potential for wider adoption of suchÂ methods.Compute support for academia. Government funding bodies should substantially increase funding for computing power resources for researchers in academia, in order to improve the ability of those researchers to verify claims made byÂ industry.We and our co-authors will be doing further research on these mechanisms and OpenAI will be looking to adopt several of these mechanisms in the future. We hope that this report inspires meaningful dialogue, and we are eager to discuss additional institutional, software, and hardware mechanisms that could be useful in enabling trustworthy AI development. We encourage anyone interested in collaborating on these issues to connect with the corresponding authors and visit theÂ reportÂ website.AcknowledgmentsReport authors, equal contributionMiles Brundage (OpenAI)Shahar Avin (Centre for the Study of Existential Risk, Leverhulme Centre for the Future of Intelligence)Jasmine Wang (Mila, University of Montreal)Haydn Belfield (Centre for the Study of Existential Risk, Leverhulme Centre for the Future of Intelligence)Gretchen Krueger (OpenAI)Report authors, descending contributionGillian Hadfield (OpenAI, University of Toronto, Schwartz Reisman Institute for Technology and Society)Heidy Khlaaf (Adelard)Jingying Yang (Partnership on AI)Helen Toner (Center for Security and Emerging Technology)Ruth Fong (University of Oxford)Tegan Maharaj (Mila, Montreal Polytechnic)Pang Wei Koh (Stanford University)Sara Hooker (Google Brain)Jade Leung (Future of Humanity Institute)Andrew Trask (University of Oxford)Emma Bluemke (University of Oxford)Jonathan Lebensold (Mila, McGill University)Cullen OâKeefe (OpenAI)Mark Koren (Stanford Centre for AI Safety)ThÃ©o Ryffel (Ãcole Normale SupÃ©rieure [Paris])JB Rubinovitz (Remedy.AI)Tamay Besiroglu (University of Cambridge)Federica Carugati (Center for Advanced Study in the Behavioral Sciences)Jack Clark (OpenAI)Peter Eckersley (Partnership on AI)Sarah de Haas (Google Research)Maritza Johnson (Google Research)Ben Laurie (Google Research)Alex Ingerman (Google Research)Igor Krawczuk (Ãcole Polytechnique FÃ©dÃ©rale de Lausanne)Amanda Askell (OpenAI)Rosario Cammarota (Intel)Andrew Lohn (RAND Corporation)David Krueger (Mila, Montreal Polytechnic)Charlotte Stix (Eindhoven University of Technology)Peter Henderson (Stanford University)Logan Graham (University of Oxford)Carina Prunkl (Future of Humanity Institute)Bianca Martin (OpenAI)Elizabeth Seger (University of Cambridge)Noa Zilberman (University of Oxford)SeÃ¡n Ã hÃigeartaigh (Leverhulme Centre for the Future of Intelligence, Centre for the Study of Existential Risk)Frens Kroeger (Coventry University)Girish Sastry (OpenAI)Rebecca Kagan (Center for Security and Emerging Technology)Adrian Weller (University of Cambridge, Alan Turing Institute)Brian Tse (Future of Humanity Institute, Partnership on AI)Elizabeth Barnes (OpenAI)Allan Dafoe (Future of Humanity Institute)Paul Scharre (Center for a New American Security)Ariel Herbert-Voss (OpenAI)Martijn Rasser (Center for a New American Security)Shagun Sodhani (Mila, University of Montreal)Carrick Flynn (Center for Security and Emerging Technology)Thomas Gilbert (University of California, Berkeley)Lisa Dyer (Partnership on AI)Saif Khan (Center for Security and Emerging Technology)Yoshua Bengio (Mila, University of Montreal)Markus Anderljung (Future of Humanity Institute)ResearchOverviewIndexProductOverviewChatGPTGPT-4DALLÂ·E 2Customer storiesSafety standardsAPI data privacyPricingSafetyOverviewCompanyAboutBlogCareersCharterSecurityOpenAI Â© 2015âââ2023Terms & policiesPrivacy policyBrand guidelinesSocialTwitterYouTubeGitHubSoundCloudLinkedInBack to top
