{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3e67f200",
   "metadata": {},
   "source": [
    "# How to call functions with chat models\n",
    "\n",
    "This notebook covers how to use the Chat Completions API in combination with external functions to extend the capabilities of GPT models.\n",
    "\n",
    "`functions` is an optional parameter in the Chat Completion API which can be used to provide function specifications. The purpose of this is to enable models to generate function arguments which adhere to the provided specifications. Note that the API will not actually execute any function calls. It is up to developers to execute function calls using model outputs.\n",
    "\n",
    "If the `functions` parameter is provided then by default the model will decide when it is appropriate to use one of the functions. The API can be forced to use a specific function by setting the `function_call` parameter to `{\"name\": \"<insert-function-name>\"}`. The API can also be forced to not use any function by setting the `function_call` parameter to `\"none\"`. If a function is used, the output will contain `\"finish_reason\": \"function_call\"` in the response, as well as a `function_call` object that has the name of the function and the generated function arguments.\n",
    "\n",
    "### Overview\n",
    "\n",
    "This notebook contains the following 2 sections:\n",
    "\n",
    "- **How to generate function arguments:** Specify a set of functions and use the API to generate function arguments.\n",
    "- **How to call functions with model generated arguments:** Close the loop by actually executing functions with model generated arguments."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "64c85e26",
   "metadata": {},
   "source": [
    "## How to generate function arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80e71f33",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in /Users/shrutipatel/projects/work/repo-gpt/.venv/lib/python3.11/site-packages (1.11.1)\n",
      "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /Users/shrutipatel/projects/work/repo-gpt/.venv/lib/python3.11/site-packages (from scipy) (1.26.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: tenacity in /Users/shrutipatel/projects/work/repo-gpt/.venv/lib/python3.11/site-packages (8.2.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: tiktoken in /Users/shrutipatel/projects/work/repo-gpt/.venv/lib/python3.11/site-packages (0.4.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/shrutipatel/projects/work/repo-gpt/.venv/lib/python3.11/site-packages (from tiktoken) (2023.8.8)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/shrutipatel/projects/work/repo-gpt/.venv/lib/python3.11/site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/shrutipatel/projects/work/repo-gpt/.venv/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/shrutipatel/projects/work/repo-gpt/.venv/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/shrutipatel/projects/work/repo-gpt/.venv/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/shrutipatel/projects/work/repo-gpt/.venv/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting termcolor\n",
      "  Using cached termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Installing collected packages: termcolor\n",
      "Successfully installed termcolor-2.3.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: openai in /Users/shrutipatel/projects/work/repo-gpt/.venv/lib/python3.11/site-packages (0.27.10)\n",
      "Requirement already satisfied: requests>=2.20 in /Users/shrutipatel/projects/work/repo-gpt/.venv/lib/python3.11/site-packages (from openai) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /Users/shrutipatel/projects/work/repo-gpt/.venv/lib/python3.11/site-packages (from openai) (4.66.1)\n",
      "Requirement already satisfied: aiohttp in /Users/shrutipatel/projects/work/repo-gpt/.venv/lib/python3.11/site-packages (from openai) (3.8.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/shrutipatel/projects/work/repo-gpt/.venv/lib/python3.11/site-packages (from requests>=2.20->openai) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/shrutipatel/projects/work/repo-gpt/.venv/lib/python3.11/site-packages (from requests>=2.20->openai) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/shrutipatel/projects/work/repo-gpt/.venv/lib/python3.11/site-packages (from requests>=2.20->openai) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/shrutipatel/projects/work/repo-gpt/.venv/lib/python3.11/site-packages (from requests>=2.20->openai) (2023.7.22)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/shrutipatel/projects/work/repo-gpt/.venv/lib/python3.11/site-packages (from aiohttp->openai) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/shrutipatel/projects/work/repo-gpt/.venv/lib/python3.11/site-packages (from aiohttp->openai) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/shrutipatel/projects/work/repo-gpt/.venv/lib/python3.11/site-packages (from aiohttp->openai) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/shrutipatel/projects/work/repo-gpt/.venv/lib/python3.11/site-packages (from aiohttp->openai) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/shrutipatel/projects/work/repo-gpt/.venv/lib/python3.11/site-packages (from aiohttp->openai) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/shrutipatel/projects/work/repo-gpt/.venv/lib/python3.11/site-packages (from aiohttp->openai) (1.3.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: requests in /Users/shrutipatel/projects/work/repo-gpt/.venv/lib/python3.11/site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/shrutipatel/projects/work/repo-gpt/.venv/lib/python3.11/site-packages (from requests) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/shrutipatel/projects/work/repo-gpt/.venv/lib/python3.11/site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/shrutipatel/projects/work/repo-gpt/.venv/lib/python3.11/site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/shrutipatel/projects/work/repo-gpt/.venv/lib/python3.11/site-packages (from requests) (2023.7.22)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install scipy\n",
    "!pip install tenacity\n",
    "!pip install tiktoken\n",
    "!pip install termcolor \n",
    "!pip install openai\n",
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dab872c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import openai\n",
    "import requests\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt\n",
    "from termcolor import colored\n",
    "\n",
    "GPT_MODEL = \"gpt-3.5-turbo-0613\"\n",
    "openai.api_key = \"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\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "69ee6a93",
   "metadata": {},
   "source": [
    "### Utilities\n",
    "\n",
    "First let's define a few utilities for making calls to the Chat Completions API and for maintaining and keeping track of the conversation state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c4d1c99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_conversation(messages):\n",
    "    role_to_color = {\n",
    "        \"system\": \"red\",\n",
    "        \"user\": \"green\",\n",
    "        \"assistant\": \"blue\",\n",
    "        \"function\": \"magenta\",\n",
    "    }\n",
    "    \n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"system\":\n",
    "            print(colored(f\"system: {message['content']}\\n\", role_to_color[message[\"role\"]]))\n",
    "        elif message[\"role\"] == \"user\":\n",
    "            print(colored(f\"user: {message['content']}\\n\", role_to_color[message[\"role\"]]))\n",
    "        elif message[\"role\"] == \"assistant\" and message.get(\"function_call\"):\n",
    "            print(colored(f\"assistant: {message['function_call']}\\n\", role_to_color[message[\"role\"]]))\n",
    "        elif message[\"role\"] == \"assistant\" and not message.get(\"function_call\"):\n",
    "            print(colored(f\"assistant: {message['content']}\\n\", role_to_color[message[\"role\"]]))\n",
    "        elif message[\"role\"] == \"function\":\n",
    "            print(colored(f\"function ({message['name']}): {message['content']}\\n\", role_to_color[message[\"role\"]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2e25069",
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [\n",
    "    {\n",
    "        \"name\": \"get_current_weather\",\n",
    "        \"description\": \"Get the current weather\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                },\n",
    "                \"format\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                    \"description\": \"The temperature unit to use. Infer this from the users location.\",\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"location\", \"format\"],\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"get_n_day_weather_forecast\",\n",
    "        \"description\": \"Get an N-day weather forecast\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                },\n",
    "                \"format\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                    \"description\": \"The temperature unit to use. Infer this from the users location.\",\n",
    "                },\n",
    "                \"num_days\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"description\": \"The number of days to forecast\",\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"location\", \"format\", \"num_days\"]\n",
    "        },\n",
    "    },\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bfc39899",
   "metadata": {},
   "source": [
    "If we prompt the model about the current weather, it will respond with some clarifying questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "518d6827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'assistant',\n",
       " 'content': 'In which city and state would you like to know the current weather?'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = []\n",
    "messages.append({\"role\": \"system\", \"content\": \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"})\n",
    "messages.append({\"role\": \"user\", \"content\": \"What's the weather like today\"})\n",
    "chat_response = chat_completion_request(\n",
    "    messages, functions=functions\n",
    ")\n",
    "assistant_message = chat_response.json()[\"choices\"][0][\"message\"]\n",
    "messages.append(assistant_message)\n",
    "assistant_message\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4c999375",
   "metadata": {},
   "source": [
    "Once we provide the missing information, it will generate the appropriate function arguments for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23c42a6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'assistant',\n",
       " 'content': None,\n",
       " 'function_call': {'name': 'get_current_weather',\n",
       "  'arguments': '{\\n  \"location\": \"Glasgow, Scotland\",\\n  \"format\": \"celsius\"\\n}'}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.append({\"role\": \"user\", \"content\": \"I'm in Glasgow, Scotland.\"})\n",
    "chat_response = chat_completion_request(\n",
    "    messages, functions=functions\n",
    ")\n",
    "assistant_message = chat_response.json()[\"choices\"][0][\"message\"]\n",
    "messages.append(assistant_message)\n",
    "assistant_message\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c14d4762",
   "metadata": {},
   "source": [
    "By prompting it differently, we can get it to target the other function we've told it about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa232e54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'assistant',\n",
       " 'content': 'Sure, I can help you with that. Please provide me with the number of days you want to forecast for.'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = []\n",
    "messages.append({\"role\": \"system\", \"content\": \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"})\n",
    "messages.append({\"role\": \"user\", \"content\": \"what is the weather going to be like in Glasgow, Scotland over the next x days\"})\n",
    "chat_response = chat_completion_request(\n",
    "    messages, functions=functions\n",
    ")\n",
    "assistant_message = chat_response.json()[\"choices\"][0][\"message\"]\n",
    "messages.append(assistant_message)\n",
    "assistant_message\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6172ddac",
   "metadata": {},
   "source": [
    "Once again, the model is asking us for clarification because it doesn't have enough information yet. In this case it already knows the location for the forecast, but it needs to know how many days are required in the forecast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7d8a543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'index': 0,\n",
       " 'message': {'role': 'assistant',\n",
       "  'content': None,\n",
       "  'function_call': {'name': 'get_n_day_weather_forecast',\n",
       "   'arguments': '{\\n  \"location\": \"Glasgow, Scotland\",\\n  \"format\": \"celsius\",\\n  \"num_days\": 5\\n}'}},\n",
       " 'finish_reason': 'function_call'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.append({\"role\": \"user\", \"content\": \"5 days\"})\n",
    "chat_response = chat_completion_request(\n",
    "    messages, functions=functions\n",
    ")\n",
    "chat_response.json()[\"choices\"][0]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4b758a0a",
   "metadata": {},
   "source": [
    "#### Forcing the use of specific functions or no function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "412f79ba",
   "metadata": {},
   "source": [
    "We can force the model to use a specific function, for example get_n_day_weather_forecast by using the function_call argument. By doing so, we force the model to make assumptions about how to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "559371b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'assistant',\n",
       " 'content': None,\n",
       " 'function_call': {'name': 'get_n_day_weather_forecast',\n",
       "  'arguments': '{\\n  \"location\": \"Toronto, Canada\",\\n  \"format\": \"celsius\",\\n  \"num_days\": 1\\n}'}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in this cell we force the model to use get_n_day_weather_forecast\n",
    "messages = []\n",
    "messages.append({\"role\": \"system\", \"content\": \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"})\n",
    "messages.append({\"role\": \"user\", \"content\": \"Give me a weather report for Toronto, Canada.\"})\n",
    "chat_response = chat_completion_request(\n",
    "    messages, functions=functions, function_call={\"name\": \"get_n_day_weather_forecast\"}\n",
    ")\n",
    "chat_response.json()[\"choices\"][0][\"message\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7ab0f58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'assistant',\n",
       " 'content': None,\n",
       " 'function_call': {'name': 'get_current_weather',\n",
       "  'arguments': '{\\n  \"location\": \"Toronto, Canada\",\\n  \"format\": \"celsius\"\\n}'}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if we don't force the model to use get_n_day_weather_forecast it may not\n",
    "messages = []\n",
    "messages.append({\"role\": \"system\", \"content\": \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"})\n",
    "messages.append({\"role\": \"user\", \"content\": \"Give me a weather report for Toronto, Canada.\"})\n",
    "chat_response = chat_completion_request(\n",
    "    messages, functions=functions\n",
    ")\n",
    "chat_response.json()[\"choices\"][0][\"message\"]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3bd70e48",
   "metadata": {},
   "source": [
    "We can also force the model to not use a function at all. By doing so we prevent it from producing a proper function call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acfe54e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'assistant', 'content': 'Sure, let me get that information for you.'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = []\n",
    "messages.append({\"role\": \"system\", \"content\": \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"})\n",
    "messages.append({\"role\": \"user\", \"content\": \"Give me the current weather (use Celcius) for Toronto, Canada.\"})\n",
    "chat_response = chat_completion_request(\n",
    "    messages, functions=functions, function_call=\"none\"\n",
    ")\n",
    "chat_response.json()[\"choices\"][0][\"message\"]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b4482aee",
   "metadata": {},
   "source": [
    "## How to call functions with model generated arguments\n",
    "\n",
    "In our next example, we'll demonstrate how to execute functions whose inputs are model-generated, and use this to implement an agent that can answer questions for us about a database. For simplicity we'll use the [Chinook sample database](https://www.sqlitetutorial.net/sqlite-sample-database/).\n",
    "\n",
    "*Note:* SQL generation can be high-risk in a production environment since models are not perfectly reliable at generating correct SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d567ddf6-98b8-4270-b602-d2ca86faffbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "edc1bd9d-9957-4ef5-83b0-1571f890a8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./LICENSE\n",
      "./README.md\n",
      "./poetry.lock\n",
      "./pyproject.toml\n",
      "./test.py\n",
      "test/__init__.py\n",
      "test/code_manager/__init__.py\n",
      "test/code_manager/code_extractor.py\n",
      "test/file_handler/__init__.py\n",
      "test/file_handler/test_elixir_extract_code.py\n",
      "test/file_handler/test_elixir_extract_vscode_ext_codelens.py\n",
      "test/file_handler/test_php_extract_code.py\n",
      "test/file_handler/test_php_extract_vscode_ext_codelens.py\n",
      "test/file_handler/test_python_extract_code.py\n",
      "test/file_handler/test_python_extract_vscode_ext_codelens.py\n",
      "test/file_handler/test_sql_extract_vscode_ext_codelens.py\n",
      "test/file_handler/test_sql_file_handler_extract_code.py\n",
      "test/file_handler/test_typescript_extract_code.py\n",
      "test/file_handler/test_typescript_extract_vscode_ext_codelens.py\n",
      "imgs/example_output.png\n",
      "expt/call_functions_with_chat_models.ipynb\n",
      "expt/web-qa.ipynb\n",
      "src/repo_gpt/__init__.py\n",
      "src/repo_gpt/add_tests.py\n",
      "src/repo_gpt/cli.py\n",
      "src/repo_gpt/console.py\n",
      "src/repo_gpt/incorrect.py\n",
      "src/repo_gpt/openai_service.py\n",
      "src/repo_gpt/prompt_service.py\n",
      "src/repo_gpt/search_service.py\n",
      "src/repo_gpt/test_generator.py\n",
      "src/repo_gpt/utils.py\n",
      "src/repo_gpt/vscode_prompt_service.py\n",
      "src/repo_gpt/code_manager/__init__.py\n",
      "src/repo_gpt/code_manager/abstract_extractor.py\n",
      "src/repo_gpt/code_manager/code_dir_extractor.py\n",
      "src/repo_gpt/code_manager/code_manager.py\n",
      "src/repo_gpt/code_manager/code_processor.py\n",
      "src/repo_gpt/code_manager/code_vscode_file_extractor.py\n",
      "src/repo_gpt/file_handler/__init__.py\n",
      "src/repo_gpt/file_handler/abstract_handler.py\n",
      "src/repo_gpt/file_handler/generic_code_file_handler.py\n",
      "src/repo_gpt/file_handler/generic_sql_file_handler.py\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathspec import PathSpec\n",
    "from pathspec.patterns import GitWildMatchPattern\n",
    "\n",
    "def get_gitignore_spec(root_directory):\n",
    "    gitignore_file = os.path.join(root_directory, '.gitignore')\n",
    "    if not os.path.exists(gitignore_file):\n",
    "        return None\n",
    "    with open(gitignore_file, 'r') as f:\n",
    "        spec = PathSpec.from_lines(GitWildMatchPattern, f)\n",
    "    return spec\n",
    "\n",
    "def is_hidden(path):\n",
    "    # Check if a file or directory is hidden by checking if its name starts with a dot\n",
    "    return os.path.basename(path).startswith('.')\n",
    "\n",
    "def get_indented_directory_structure(root_directory):\n",
    "    structured_output = []\n",
    "    gitignore_spec = get_gitignore_spec(root_directory)\n",
    "    \n",
    "    for current_path, directories, files in os.walk(root_directory):\n",
    "        \n",
    "        # Filter out hidden directories and those in gitignore\n",
    "        directories[:] = [\n",
    "            d for d in directories \n",
    "            if not is_hidden(d) \n",
    "            and (not gitignore_spec or not gitignore_spec.match_file(os.path.join(current_path, d)))\n",
    "        ]\n",
    "\n",
    "        # Skip hidden directories in the main loop\n",
    "        if is_hidden(current_path):\n",
    "            continue\n",
    "        \n",
    "        depth = current_path.replace(root_directory, \"\").count(os.sep)\n",
    "        indent = \"    \" * depth\n",
    "        structured_output.append(f\"{indent}/{os.path.basename(current_path)}\")\n",
    "        sub_indent = \"    \" * (depth + 1)\n",
    "        \n",
    "        for file in sorted(files):\n",
    "            # Skip hidden files or those in gitignore\n",
    "            if not is_hidden(file) and (not gitignore_spec or not gitignore_spec.match_file(os.path.join(current_path, file))):\n",
    "                structured_output.append(f\"{sub_indent}{file}\")\n",
    "\n",
    "    return \"\\n\".join(structured_output)\n",
    "\n",
    "def get_relative_path_directory_structure(root_directory):\n",
    "    structured_output = []\n",
    "    gitignore_spec = get_gitignore_spec(root_directory)\n",
    "    \n",
    "    for current_path, directories, files in os.walk(root_directory):\n",
    "        \n",
    "        # Filter out hidden directories and those in gitignore\n",
    "        directories[:] = [\n",
    "            d for d in directories \n",
    "            if not is_hidden(d) \n",
    "            and (not gitignore_spec or not gitignore_spec.match_file(os.path.join(current_path, d)))\n",
    "        ]\n",
    "\n",
    "        # Skip hidden directories in the main loop\n",
    "        if is_hidden(current_path):\n",
    "            continue\n",
    "        \n",
    "        # # Convert the current directory path to a relative path from the root directory\n",
    "        rel_dir = os.path.relpath(current_path, root_directory)\n",
    "        \n",
    "        # # Append the relative directory path to structured_output\n",
    "        # structured_output.append(rel_dir if rel_dir != \".\" else \"\")\n",
    "        \n",
    "        for file in sorted(files):\n",
    "            # Skip hidden files or those in gitignore\n",
    "            if not is_hidden(file) and (not gitignore_spec or not gitignore_spec.match_file(os.path.join(current_path, file))):\n",
    "                # Combine the relative directory path with the file name to get the relative file path\n",
    "                rel_file_path = os.path.join(rel_dir, file)\n",
    "                structured_output.append(rel_file_path)\n",
    "                \n",
    "    return structured_output\n",
    "\n",
    "def get_relative_path_directory_structure_string(root_directory):\n",
    "    return \"\\n\".join(get_relative_path_directory_structure(root_directory))\n",
    "\n",
    "\n",
    "print(get_relative_path_directory_structure_string(\"/Users/shrutipatel/projects/work/repo-gpt\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ae73c9ee",
   "metadata": {},
   "source": [
    "As before, we'll define a function specification for the function we'd like the API to generate arguments for. Notice that we are inserting the database schema into the function specification. This will be important for the model to know about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "b96929e7-ebcc-43dc-8723-8e82ef7ae977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement pprint (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for pprint\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "8e6a4540-b556-4820-8199-6dcf4899bb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tiktoken\n",
    "import inspect\n",
    "import pprint\n",
    "from io import StringIO\n",
    "\n",
    "class ParentAgent:\n",
    "\n",
    "    summary_prompt = \"\"\"*Briefly* summarize this partial conversation about programming.\n",
    "    Include less detail about older parts and more detail about the most recent messages.\n",
    "    Start a new paragraph every time the topic changes!\n",
    "    \n",
    "    This is only part of a longer conversation so *DO NOT* conclude the summary with language like \"Finally, ...\". Because the conversation continues after the summary.\n",
    "    The summary *MUST* include the function names, libraries, packages that are being discussed.\n",
    "    The summary *MUST* include the filenames that are being referenced by the assistant inside the ```...``` fenced code blocks!\n",
    "    The summaries *MUST NOT* include ```...``` fenced code blocks!\n",
    "    \n",
    "    Phrase the summary with the USER in first person, telling the ASSISTANT about the conversation.\n",
    "    Write *as* the user.\n",
    "    The user should refer to the assistant as *you*.\n",
    "    Start the summary with \"I asked you...\".\n",
    "    \"\"\"\n",
    "    GPT_MODEL = \"gpt-3.5-turbo-0613\" # gpt-4-0613\n",
    "    SUMMARY_MODEL = \"gpt-4\"\n",
    "    \n",
    "    def __init__(self, user_task, terminating_function_call_name, system_prompt, threshold=10, debug = False):\n",
    "        self.terminating_function_call_name = terminating_function_call_name\n",
    "        self.messages = []\n",
    "        self.user_task = user_task\n",
    "        self.system_prompt = system_prompt\n",
    "        self._initialize_messages()\n",
    "        self.functions = self._initialize_functions()\n",
    "        # self.raw_messages =[]\n",
    "        # self.summary_messages = []\n",
    "        self.functions = []\n",
    "        self.threshold = threshold\n",
    "        self.debug = debug\n",
    "        # self.initial_messages= [] \n",
    "        # self.chat_messages = []\n",
    "    \n",
    "    def _initialize_messages(self):\n",
    "        initial_messages = [\n",
    "            {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "            {\"role\": \"user\", \"content\": self.user_task}\n",
    "        ]\n",
    "        self.messages = initial_messages\n",
    "\n",
    "    def _initialize_functions(self):# -> List[Dict]:\n",
    "        raise NotImplementedException(\"Implment this function\")\n",
    "        \n",
    "    def _parse_arguments(self, function_call):\n",
    "        return json.loads(function_call[\"arguments\"])\n",
    "\n",
    "    def _count_messages_tokens(self):\n",
    "        enc = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "        return len(enc.encode(json.dumps(self.messages))) + len(enc.encode(json.dumps(self.functions)))\n",
    "\n",
    "    def _append_message(self, message):\n",
    "        self.messages.append(message)\n",
    "        if self._count_messages_tokens() >= 4000:\n",
    "            self.compress_messages()\n",
    "\n",
    "    @retry(wait=wait_random_exponential(multiplier=1, max=40), stop=stop_after_attempt(3))\n",
    "    def compress_messages(self):\n",
    "        # TODO: use something intelligent like semantic search possibly to select relevant messages\n",
    "        output = StringIO()\n",
    "        pprint.pprint(self.messages, stream=output)\n",
    "        formatted_messages = output.getvalue()\n",
    "\n",
    "        summary_messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are an expert technical writer.\",\n",
    "                },\n",
    "                {\"role\": \"user\", \"content\": self.summary_prompt+formatted_messages},\n",
    "            ],\n",
    "        try:\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=self.SUMMARY_MODEL,\n",
    "                messages=summary_messages\n",
    "            )\n",
    "            print(response)\n",
    "            assistant_message = chat_response[\"choices\"][0][\"message\"]\n",
    "            print(assistant_message)\n",
    "            self._initialize_messages()\n",
    "            self._append_message(assistant_message)\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            print(\"Unable to generate ChatCompletion response\")\n",
    "            print(f\"Exception: {e}\")\n",
    "            raise e\n",
    "\n",
    "\n",
    "    def execute_function_call(self, message):\n",
    "        function_name = message[\"function_call\"][\"name\"]\n",
    "        args = self._parse_arguments(message[\"function_call\"])\n",
    "    \n",
    "        func = getattr(self, function_name, None)\n",
    "        if not func:\n",
    "            return f\"Error: function {function_name} does not exist\"\n",
    "    \n",
    "        # Filter out args to only pass those that the function accepts\n",
    "        accepted_args = inspect.signature(func).parameters.keys()\n",
    "        filtered_args = {key: value for key, value in args.items() if key in accepted_args}\n",
    "    \n",
    "        return func(**filtered_args)\n",
    "\n",
    "\n",
    "    @retry(wait=wait_random_exponential(multiplier=1, max=40), stop=stop_after_attempt(3))\n",
    "    def chat_completion_request(self, function_call=None, model=GPT_MODEL):\n",
    "        try:\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=model,\n",
    "                messages=self.messages,\n",
    "                functions=self.functions,\n",
    "                function_call=function_call if function_call else 'auto'\n",
    "            )\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            print(\"Unable to generate ChatCompletion response\")\n",
    "            print(f\"Exception: {e}\")\n",
    "            raise e\n",
    "    \n",
    "    def process_messages(self):\n",
    "        # TODO: make ending function name settable OR move this into the childclass\n",
    "        iter_count = 0\n",
    "        function_call_name = \"\"\n",
    "        \n",
    "        results = \"\"\n",
    "\n",
    "        while iter_count < self.threshold and function_call_name != self.terminating_function_call_name:\n",
    "            chat_response = self.chat_completion_request()\n",
    "            assistant_message = chat_response[\"choices\"][0][\"message\"]\n",
    "            self._append_message(assistant_message)\n",
    "            if self.debug: print(assistant_message)\n",
    "            if 'function_call' in assistant_message:\n",
    "                results = self.execute_function_call(assistant_message)\n",
    "                function_call_name = assistant_message[\"function_call\"][\"name\"]\n",
    "                self._append_message({\"role\": \"function\", \"content\": results, \"name\": function_call_name})\n",
    "            else:\n",
    "                self._append_message({\"role\": \"user\", \"content\": \"Continue\"})\n",
    "            iter_count += 1\n",
    "\n",
    "        if function_call_name == self.terminating_function_call_name:\n",
    "            return results\n",
    "        raise Exception(\"I had to stop the search loop before plan for formulated because I reached the end of my allotted function calls\")\n",
    "\n",
    "# Refactored RepoUnderstandingAgent using the ParentAgent\n",
    "from tqdm import tqdm\n",
    "from repo_gpt.openai_service import OpenAIService\n",
    "from repo_gpt.search_service import SearchService\n",
    "from repo_gpt.file_handler.generic_code_file_handler import PythonFileHandler\n",
    "from pathlib import Path\n",
    "\n",
    "# Initialize the tqdm integration with pandas\n",
    "tqdm.pandas()\n",
    "\n",
    "class RepoUnderstandingAgent(ParentAgent):\n",
    "    \n",
    "    system_prompt = \"You are an expert software engineer on a specific code repository. Users ask you how they can implement something in their codebase. You first use your tools to search and understand the codebase and then figure out how to implement the users' task in the repository.\"\n",
    "    \n",
    "    def __init__(self, user_task, system_prompt = system_prompt, threshold=10, debug=False):\n",
    "        self.system_prompt = system_prompt\n",
    "        super().__init__(user_task, \"create_plan_to_complete_user_task\", system_prompt, threshold, debug)  # Call ParentAgent constructor\n",
    "        self.root_path = Path(\"/Users/shrutipatel/projects/work/repo-gpt/\")\n",
    "        self.embedding_path = self.root_path / \".repo_gpt/code_embeddings.pkl\"\n",
    "        self.openai_service = OpenAIService()\n",
    "        self.search_service = SearchService(self.openai_service, self.embedding_path)\n",
    "        self.pythonfilehandler = PythonFileHandler() # TODO: update to handle more than python files (all except SQL)\n",
    "        \n",
    "        self.functions = self._initialize_functions()\n",
    "        \n",
    "    \n",
    "    def _initialize_functions(self):\n",
    "        # Define function details\n",
    "        return [\n",
    "        {\n",
    "        \"name\": \"semantic_search\",\n",
    "        \"description\": \"Use this function to search the entire codebase semantically. The input should be the search query string.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"query\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": f\"\"\"\n",
    "                            The semantic search query to use to search the code base.\n",
    "                            \"\"\",\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"query\"],\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"view_function_code\",\n",
    "        \"description\": \"Use this function to search for and view a function's code in the user's codebase. Input should be the name of the function you want to search for. An empty response means the given files don't exist.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"function_name\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": f\"\"\"\n",
    "                            The name of the function or its description.\n",
    "                            \"\"\",\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"function_name\"],\n",
    "        },\n",
    "    },\n",
    "{\n",
    "    \"name\": \"view_file_functions_and_classes\",\n",
    "    \"description\": \"Use this function to retrieve a list of the functions and classes in a file from the user's codebase. An empty response means the given files don't exist.\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"file_paths\": {\n",
    "                \"type\": \"array\",\n",
    "                \"items\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"An array of one or more file paths of a file you want to retrieve functions and classes from. If a file doesn't exist, the function will return a string saying so.\"\n",
    "                },\n",
    "                \"description\": f\"\"\"\n",
    "                        The file paths of the files you want to retrieve functions and classes for to better understand the user's task. Below are the files within the user's repository:\n",
    "                        {get_relative_path_directory_structure(\"/Users/shrutipatel/projects/work/repo-gpt\")}\n",
    "                        \"\"\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"file_paths\"],\n",
    "    }\n",
    "},\n",
    "    {\n",
    "        \"name\": \"create_plan_to_complete_user_task\",\n",
    "        \"description\": \"Use this function when you understand the user's task and have a detailed plan ready for completing the user's task. The input should be a step-by-step plan on how to complete the user's task. It can include things like 'Create a new file with a given file path', 'Add the given code to the file', etc.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"plan\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": f\"\"\"\n",
    "                            A step-by-step plan on how to complete the user's task. It can include things like \"Create a new file with a given file path\", \"Add the given code to the file\", etc.\n",
    "                            \"\"\",\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"plan\"],\n",
    "        },\n",
    "    },\n",
    "    \n",
    "]\n",
    "        \n",
    "    def view_function_code(self, function_name):\n",
    "        functions_df, classes_df = self.search_service.find_function_match(function_name)\n",
    "        \n",
    "        if (classes_df is None or classes_df.empty) and (functions_df is None or functions_df.empty):\n",
    "            return ''\n",
    "        elif functions_df is None or functions_df.empty:\n",
    "            return classes_df.to_csv(index=False, path_or_buf=None)\n",
    "        elif classes_df is None or classes_df.empty:\n",
    "            return functions_df.to_csv(index=False, path_or_buf=None)\n",
    "        else:\n",
    "            return functions_df.append(classes_df).to_csv(index=False, path_or_buf=None)\n",
    "\n",
    "    def semantic_search(self, query):\n",
    "        return self.search_service.semantic_search(query).to_csv(index=False, path_or_buf=None)\n",
    "\n",
    "    def view_file_functions_and_classes(self, file_paths):\n",
    "        results = []\n",
    "        for file_path in file_paths:\n",
    "            full_path = self.root_path / Path(file_path)\n",
    "            \n",
    "            if not full_path.exists():\n",
    "                results.append(f\"File not found: {file_path}\")\n",
    "                continue  # Skip to the next iteration\n",
    "            elif full_path.is_dir():\n",
    "                results.append(f\"This is not a file, but a directory, pass a filepath instead: {file_path}\")\n",
    "                continue  # Skip to the next iteration\n",
    "                \n",
    "            results.append(self.pythonfilehandler.summarize_file(full_path))\n",
    "            \n",
    "        return \"\\n\".join(results)\n",
    "\n",
    "    def create_plan_to_complete_user_task(self, plan):\n",
    "        return plan\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "class CodeWritingAgent(ParentAgent):\n",
    "    \n",
    "    def __init__(self, user_task, threshold=10, debug=False):\n",
    "        system_prompt = \"You are an expert software engineer writing code in a repository. The user gives you a plan detailing how the code needs to be updated. You implement the code changes using functions. Ask clarifying questions.\"\n",
    "        super().__init__(user_task, \"completed_all_code_updates\", system_prompt, threshold, debug)  # Call ParentAgent constructor\n",
    "        self.root_path = Path(\"/Users/shrutipatel/projects/work/repo-gpt/\")\n",
    "        self.embedding_path = self.root_path / \".repo_gpt/code_embeddings.pkl\"\n",
    "        self.openai_service = OpenAIService()\n",
    "        self.search_service = SearchService(self.openai_service, self.embedding_path)\n",
    "        self.pythonfilehandler = PythonFileHandler() # TODO: update to handle more than python files (all except sql)\n",
    "        \n",
    "        self.functions = self._initialize_functions()\n",
    "\n",
    "    def _initialize_functions(self):\n",
    "        return [\n",
    "            {\n",
    "                \"name\": \"create_file\",\n",
    "                \"description\": \"Create a new file with the provided content.\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"file_path\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"Path to the new file to be created.\"\n",
    "                        },\n",
    "                        \"content\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"Content to write in the new file.\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"file_path\", \"content\"]\n",
    "                },\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"append_to_file\",\n",
    "                \"description\": \"Append content to an existing file.\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"file_path\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"Path to the file to be updated.\"\n",
    "                        },\n",
    "                        \"content\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"Content to append to the file.\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"file_path\", \"content\"]\n",
    "                },\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"completed_all_code_updates\",\n",
    "                \"description\": \"Call this function when all the code updates are completed.\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"code_changes\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"Enumeration of all the changes that were made to the code.\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"code_changes\"]\n",
    "                },\n",
    "            },\n",
    "            \n",
    "        ]\n",
    "\n",
    "    def completed_all_code_updates(self, code_changes):\n",
    "        return code_changes\n",
    "    \n",
    "\n",
    "    def create_file(self, file_path, content):\n",
    "        \"\"\"\n",
    "        Create a new file with the provided content.\n",
    "        \n",
    "        Args:\n",
    "        - file_path (str): Path to the new file to be created.\n",
    "        - content (str): Content to write in the new file.\n",
    "\n",
    "        Returns:\n",
    "        - str: Success or error message.\n",
    "        \"\"\"\n",
    "        full_path = self.root_path / Path(file_path)\n",
    "        \n",
    "        # Check if file already exists\n",
    "        if full_path.exists():\n",
    "            return f\"File {file_path} already exists. To update it, use append_to_file().\"\n",
    "        \n",
    "        with open(full_path, 'w') as f:\n",
    "            f.write(content)\n",
    "            \n",
    "        return f\"File {file_path} has been created successfully.\"\n",
    "\n",
    "    def append_to_file(self, file_path, content):\n",
    "        \"\"\"\n",
    "        Append content to an existing file.\n",
    "        \n",
    "        Args:\n",
    "        - file_path (str): Path to the file to be updated.\n",
    "        - content (str): Content to append in the file.\n",
    "\n",
    "        Returns:\n",
    "        - str: Success or error message.\n",
    "        \"\"\"\n",
    "        full_path = self.root_path / Path(file_path)\n",
    "        \n",
    "        # Check if file exists\n",
    "        if not full_path.exists():\n",
    "            return f\"File {file_path} does not exist. To create it, use create_file().\"\n",
    "        \n",
    "        with open(full_path, 'a') as f:\n",
    "            f.write(content)\n",
    "            \n",
    "        return f\"Content has been appended to {file_path} successfully.\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "8f75fcf7-ba08-4c4b-8602-6042122b302a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 144/144 [00:00<00:00, 72324.25it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">66</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m \u001b[0m\u001b[1;31m66\u001b[0m\u001b[92m \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">src/repo_gpt/code_manager/abstract_extractor.py:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">66</span>  <span style=\"color: #808000; text-decoration-color: #808000\">distance</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.755</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "src/repo_gpt/code_manager/abstract_extractor.py:\u001b[1;36m66\u001b[0m  \u001b[33mdistance\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.755\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #66d9ef; text-decoration-color: #66d9ef; background-color: #272822\">def</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> </span><span style=\"color: #a6e22e; text-decoration-color: #a6e22e; background-color: #272822\">get_handler</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">(self, filepath: str) </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">-&gt;</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> Type[FileHandler]:</span><span style=\"background-color: #272822\">                                                         </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">        _, file_extension </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> os</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">path</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">splitext(filepath)</span><span style=\"background-color: #272822\">                                                             </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">        handler_class </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> self</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">HANDLER_MAPPING</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">get(file_extension)</span><span style=\"background-color: #272822\">                                                   </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">        </span><span style=\"color: #66d9ef; text-decoration-color: #66d9ef; background-color: #272822\">if</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> handler_class </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">is</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> </span><span style=\"color: #66d9ef; text-decoration-color: #66d9ef; background-color: #272822\">None</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">:</span><span style=\"background-color: #272822\">                                                                                  </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">            print(</span><span style=\"background-color: #272822\">                                                                                                 </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">                </span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">f\"No handler for files with extension {</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">file_extension</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">}. Skipping file {</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">filepath</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">}\"</span><span style=\"background-color: #272822\">                  </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">            )</span><span style=\"background-color: #272822\">                                                                                                      </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;102;217;239;48;2;39;40;34mdef\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;166;226;46;48;2;39;40;34mget_handler\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mself\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mfilepath\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m:\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mstr\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m-\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m>\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mType\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m[\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mFileHandler\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m]\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m:\u001b[0m\u001b[48;2;39;40;34m                                                         \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m        \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m_\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mfile_extension\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mos\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mpath\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34msplitext\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mfilepath\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                             \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m        \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mhandler_class\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mself\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mHANDLER_MAPPING\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mget\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mfile_extension\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                   \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m        \u001b[0m\u001b[38;2;102;217;239;48;2;39;40;34mif\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mhandler_class\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34mis\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;102;217;239;48;2;39;40;34mNone\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m:\u001b[0m\u001b[48;2;39;40;34m                                                                                  \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m            \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[48;2;39;40;34m                                                                                                 \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m                \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mf\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mNo handler for files with extension \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m{\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mfile_extension\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m}\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m. Skipping file \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m{\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mfilepath\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m}\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[48;2;39;40;34m                  \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m            \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                                      \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">83</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m \u001b[0m\u001b[1;31m83\u001b[0m\u001b[92m \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">src/repo_gpt/file_handler/generic_code_file_handler.py:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">83</span>  <span style=\"color: #808000; text-decoration-color: #808000\">distance</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.736</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "src/repo_gpt/file_handler/generic_code_file_handler.py:\u001b[1;36m83\u001b[0m  \u001b[33mdistance\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.736\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #66d9ef; text-decoration-color: #66d9ef; background-color: #272822\">class</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> </span><span style=\"color: #a6e22e; text-decoration-color: #a6e22e; background-color: #272822\">TypeScriptFileHandler</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">(GenericCodeFileHandler):</span><span style=\"background-color: #272822\">                                                               </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    </span><span style=\"color: #66d9ef; text-decoration-color: #66d9ef; background-color: #272822\">def</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> </span><span style=\"color: #a6e22e; text-decoration-color: #a6e22e; background-color: #272822\">__init__</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">(self):</span><span style=\"background-color: #272822\">                                                                                            </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">        super()</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #a6e22e; text-decoration-color: #a6e22e; background-color: #272822\">__init__</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">(</span><span style=\"background-color: #272822\">                                                                                          </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">            lang</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"typescript\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">,</span><span style=\"background-color: #272822\">                                                                                     </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">            function_name_node_type</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"identifier\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">,</span><span style=\"background-color: #272822\">                                                                  </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">            class_name_node_type</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"type_identifier\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">,</span><span style=\"background-color: #272822\">                                                                </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">            parent_class_name_node_type</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"identifier\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">,</span><span style=\"background-color: #272822\">                                                              </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;102;217;239;48;2;39;40;34mclass\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;166;226;46;48;2;39;40;34mTypeScriptFileHandler\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mGenericCodeFileHandler\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m:\u001b[0m\u001b[48;2;39;40;34m                                                               \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m    \u001b[0m\u001b[38;2;102;217;239;48;2;39;40;34mdef\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;166;226;46;48;2;39;40;34m__init__\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mself\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m:\u001b[0m\u001b[48;2;39;40;34m                                                                                            \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m        \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34msuper\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;166;226;46;48;2;39;40;34m__init__\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[48;2;39;40;34m                                                                                          \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m            \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mlang\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mtypescript\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[48;2;39;40;34m                                                                                     \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m            \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mfunction_name_node_type\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34midentifier\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[48;2;39;40;34m                                                                  \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m            \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mclass_name_node_type\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mtype_identifier\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[48;2;39;40;34m                                                                \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m            \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mparent_class_name_node_type\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34midentifier\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[48;2;39;40;34m                                                              \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">82</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m \u001b[0m\u001b[1;31m82\u001b[0m\u001b[92m \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">src/repo_gpt/file_handler/generic_code_file_handler.py:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">82</span>  <span style=\"color: #808000; text-decoration-color: #808000\">distance</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.734</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "src/repo_gpt/file_handler/generic_code_file_handler.py:\u001b[1;36m82\u001b[0m  \u001b[33mdistance\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.734\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #66d9ef; text-decoration-color: #66d9ef; background-color: #272822\">class</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> </span><span style=\"color: #a6e22e; text-decoration-color: #a6e22e; background-color: #272822\">PythonFileHandler</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">(GenericCodeFileHandler):</span><span style=\"background-color: #272822\">                                                                   </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    </span><span style=\"color: #66d9ef; text-decoration-color: #66d9ef; background-color: #272822\">def</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> </span><span style=\"color: #a6e22e; text-decoration-color: #a6e22e; background-color: #272822\">__init__</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">(self):</span><span style=\"background-color: #272822\">                                                                                            </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">        super()</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #a6e22e; text-decoration-color: #a6e22e; background-color: #272822\">__init__</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">(</span><span style=\"background-color: #272822\">                                                                                          </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">            lang</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"python\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">,</span><span style=\"background-color: #272822\">                                                                                         </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">            function_name_node_type</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"identifier\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">,</span><span style=\"background-color: #272822\">                                                                  </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">            class_name_node_type</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"identifier\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">,</span><span style=\"background-color: #272822\">                                                                     </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">            function_node_type</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"function_definition\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">,</span><span style=\"background-color: #272822\">                                                              </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;102;217;239;48;2;39;40;34mclass\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;166;226;46;48;2;39;40;34mPythonFileHandler\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mGenericCodeFileHandler\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m:\u001b[0m\u001b[48;2;39;40;34m                                                                   \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m    \u001b[0m\u001b[38;2;102;217;239;48;2;39;40;34mdef\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;166;226;46;48;2;39;40;34m__init__\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mself\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m:\u001b[0m\u001b[48;2;39;40;34m                                                                                            \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m        \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34msuper\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;166;226;46;48;2;39;40;34m__init__\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[48;2;39;40;34m                                                                                          \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m            \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mlang\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mpython\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[48;2;39;40;34m                                                                                         \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m            \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mfunction_name_node_type\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34midentifier\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[48;2;39;40;34m                                                                  \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m            \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mclass_name_node_type\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34midentifier\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[48;2;39;40;34m                                                                     \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m            \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mfunction_node_type\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mfunction_definition\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[48;2;39;40;34m                                                              \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'1. Create a new file with the test file path: `test/file_handler/test_my_file_handler.py`.\\n\\n2. Import the necessary modules and classes for testing, including your new file handler class.\\n\\n3. Write test cases to validate the behavior of your file handler. Consider testing various scenarios, such as handling different file types, parsing file contents correctly, and returning the expected output.\\n\\n4. Run the test file to verify that all the test cases pass successfully.'"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_agent = RepoUnderstandingAgent(\"Where should I add tests for the new file handler I'm writing? Do I need to create a new test file? If so, where?\")\n",
    "plan = repo_agent.process_messages()\n",
    "plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "ced7daba-8ba0-4fd3-8845-5c7122608ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# repo_agent.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "ef0b9161-405a-4235-911e-9179791e8cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"role\": \"assistant\",\n",
      "  \"content\": null,\n",
      "  \"function_call\": {\n",
      "    \"name\": \"create_file\",\n",
      "    \"arguments\": \"{\\n  \\\"file_path\\\": \\\"src/repo_gpt/file_handler/test_{file_handler_name}.py\\\",\\n  \\\"content\\\": \\\"\\\"\\n}\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"role\": \"assistant\",\n",
      "  \"content\": null,\n",
      "  \"function_call\": {\n",
      "    \"name\": \"append_to_file\",\n",
      "    \"arguments\": \"{\\n  \\\"file_path\\\": \\\"src/repo_gpt/file_handler/test_{file_handler_name}.py\\\",\\n  \\\"content\\\": \\\"import unittest\\\\n\\\\n\\\\nclass Test{file_handler_name}(unittest.TestCase):\\\\n    def test_scenario_1(self):\\\\n        # Test case for scenario 1\\\\n        pass\\\\n\\\\n    def test_scenario_2(self):\\\\n        # Test case for scenario 2\\\\n        pass\\\\n\\\\n    def test_scenario_3(self):\\\\n        # Test case for scenario 3\\\\n        pass\\\\n\\\\n    # Add more test cases as needed\\\\n\\\\nif __name__ == '__main__':\\\\n    unittest.main()\\\\n\\\"\\n}\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"role\": \"assistant\",\n",
      "  \"content\": null,\n",
      "  \"function_call\": {\n",
      "    \"name\": \"completed_all_code_updates\",\n",
      "    \"arguments\": \"{\\n  \\\"code_changes\\\": \\\"Created test_{file_handler_name}.py in src/repo_gpt/file_handler directory and added test cases for the file handler.\\\"\\n}\"\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Created test_{file_handler_name}.py in src/repo_gpt/file_handler directory and added test cases for the file handler.'"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writer_agent = CodeWritingAgent(plan, debug=True)\n",
    "result = writer_agent.process_messages()\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6be1f70a-3419-4432-be55-56dbe035baab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## TODO\n",
    "# - Update initial user query so we include results from semantic search\n",
    "# - Update system prompt and or the function json to include the output schema of the returned code_embedding csv rows\n",
    "# - Update sep for CSV output to a more uncommon character since commas are frequently used in code? (This may not be necessary because converting to csv usually ex\n",
    "# - Add a function where the writeragent can ask the repounderstandingagent\n",
    "# - Update function so you search for files -- this will decrease the token size because we won't send the all the files every time and this will handle larger codebases with greater ease"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
